{
    "contents": {
        "0": [
            "Competitive Programmer\u2019s Handbook\nAntti Laaksonen\nDraft August 19, 2019",
            "ii",
            "Contents\nPreface ix\nI Basic techniques 1\n1 Introduction 3\n1.1 Programming languages . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Input and output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Working with numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4 Shortening code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.5 Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.6 Contests and resources . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2 Time complexity 17\n2.1 Calculation rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.2 Complexity classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3 Estimating ef\ufb01ciency . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4 Maximum subarray sum . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3 Sorting 25\n3.1 Sorting theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.2 Sorting in C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3 Binary search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4 Data structures 35\n4.1 Dynamic arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.2 Set structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n4.3 Map structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n4.4 Iterators and ranges . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.5 Other structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n4.6 Comparison to sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5 Complete search 47\n5.1 Generating subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n5.2 Generating permutations . . . . . . . . . . . . . . . . . . . . . . . . . 49\n5.3 Backtracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n5.4 Pruning the search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n5.5 Meet in the middle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\niii",
            "6 Greedy algorithms 57\n6.1 Coin problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n6.2 Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n6.3 Tasks and deadlines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n6.4 Minimizing sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n6.5 Data compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7 Dynamic programming 65\n7.1 Coin problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n7.2 Longest increasing subsequence . . . . . . . . . . . . . . . . . . . . . 70\n7.3 Paths in a grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n7.4 Knapsack problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n7.5 Edit distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n7.6 Counting tilings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n8 Amortized analysis 77\n8.1 Two pointers method . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n8.2 Nearest smaller elements . . . . . . . . . . . . . . . . . . . . . . . . . 79\n8.3 Sliding window minimum . . . . . . . . . . . . . . . . . . . . . . . . . 81\n9 Range queries 83\n9.1 Static array queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n9.2 Binary indexed tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n9.3 Segment tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n9.4 Additional techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n10 Bit manipulation 95\n10.1 Bit representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n10.2 Bit operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n10.3 Representing sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n10.4 Bit optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n10.5 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nII Graph algorithms 107\n11 Basics of graphs 109\n11.1 Graph terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n11.2 Graph representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n12 Graph traversal 117\n12.1 Depth-\ufb01rst search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n12.2 Breadth-\ufb01rst search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n12.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\niv",
            "13 Shortest paths 123\n13.1 Bellman\u2013Ford algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 123\n13.2 Dijkstra\u2019s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n13.3 Floyd\u2013Warshall algorithm . . . . . . . . . . . . . . . . . . . . . . . . 129\n14 Tree algorithms 133\n14.1 Tree traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n14.2 Diameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n14.3 All longest paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n14.4 Binary trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n15 Spanning trees 141\n15.1 Kruskal\u2019s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n15.2 Union-\ufb01nd structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n15.3 Prim\u2019s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n16 Directed graphs 149\n16.1 Topological sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n16.2 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n16.3 Successor paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n16.4 Cycle detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n17 Strong connectivity 157\n17.1 Kosaraju\u2019s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n17.2 2SAT problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n18 Tree queries 163\n18.1 Finding ancestors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n18.2 Subtrees and paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n18.3 Lowest common ancestor . . . . . . . . . . . . . . . . . . . . . . . . . 167\n18.4 Of\ufb02ine algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\n19 Paths and circuits 173\n19.1 Eulerian paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n19.2 Hamiltonian paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n19.3 De Bruijn sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n19.4 Knight\u2019s tours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n20 Flows and cuts 181\n20.1 Ford\u2013Fulkerson algorithm . . . . . . . . . . . . . . . . . . . . . . . . 182\n20.2 Disjoint paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n20.3 Maximum matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n20.4 Path covers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\nv",
            "III Advanced topics 195\n21 Number theory 197\n21.1 Primes and factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n21.2 Modular arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n21.3 Solving equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\n21.4 Other results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n22 Combinatorics 207\n22.1 Binomial coef\ufb01cients . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n22.2 Catalan numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n22.3 Inclusion-exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n22.4 Burnside\u2019s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n22.5 Cayley\u2019s formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n23 Matrices 217\n23.1 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n23.2 Linear recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n23.3 Graphs and matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n24 Probability 225\n24.1 Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n24.2 Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n24.3 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n24.4 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230\n24.5 Randomized algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n25 Game theory 235\n25.1 Game states . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n25.2 Nim game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n25.3 Sprague\u2013Grundy theorem . . . . . . . . . . . . . . . . . . . . . . . . 238\n26 String algorithms 243\n26.1 String terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n26.2 Trie structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n26.3 String hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n26.4 Z-algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n27 Square root algorithms 251\n27.1 Combining algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n27.2 Integer partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n27.3 Mo\u2019s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n28 Segment trees revisited 257\n28.1 Lazy propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n28.2 Dynamic trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261\n28.3 Data structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n28.4 Two-dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\nvi",
            "29 Geometry 265\n29.1 Complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266\n29.2 Points and lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n29.3 Polygon area . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n29.4 Distance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n30 Sweep line algorithms 275\n30.1 Intersection points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n30.2 Closest pair problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n30.3 Convex hull problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\nBibliography 281\nvii",
            "viii",
            "Preface\nThe purpose of this book is to give you a thorough introduction to competitive\nprogramming. It is assumed that you already know the basics of programming,\nbut no previous background in competitive programming is needed.\nThe book is especially intended for students who want to learn algorithms\nand possibly participate in the International Olympiad in Informatics (IOI) or in\nthe International Collegiate Programming Contest (ICPC). Of course, the book is\nalso suitable for anybody else interested in competitive programming.\nIt takes a long time to become a good competitive programmer, but it is also\nan opportunity to learn a lot. You can be sure that you will get a good general\nunderstanding of algorithms if you spend time reading the book, solving problems\nand taking part in contests.\nThe book is under continuous development. You can always send feedback on\nthe book to ahslaaks@cs.helsinki.fi.\nHelsinki, August 2019\nAntti Laaksonen\nix",
            "x",
            "Part I\nBasic techniques\n1",
            "",
            "Chapter 1\nIntroduction\nCompetitive programming combines two topics: (1) the design of algorithms and\n(2) the implementation of algorithms.\nThe design of algorithms consists of problem solving and mathematical\nthinking. Skills for analyzing problems and solving them creatively are needed.\nAn algorithm for solving a problem has to be both correct and ef\ufb01cient, and the\ncore of the problem is often about inventing an ef\ufb01cient algorithm.\nTheoretical knowledge of algorithms is important to competitive programmers.\nTypically, a solution to a problem is a combination of well-known techniques and\nnew insights. The techniques that appear in competitive programming also form\nthe basis for the scienti\ufb01c research of algorithms.\nThe implementation of algorithms requires good programming skills. In\ncompetitive programming, the solutions are graded by testing an implemented\nalgorithm using a set of test cases. Thus, it is not enough that the idea of the\nalgorithm is correct, but the implementation also has to be correct.\nA good coding style in contests is straightforward and concise. Programs\nshould be written quickly, because there is not much time available. Unlike in\ntraditional software engineering, the programs are short (usually at most a few\nhundred lines of code), and they do not need to be maintained after the contest.\n1.1 Programming languages\nAt the moment, the most popular programming languages used in contests are\nC++, Python and Java. For example, in Google Code Jam 2017, among the best\n3,000 participants, 79 % used C++, 16 % used Python and 8 % used Java [ 29].\nSome participants also used several languages.\nMany people think that C++ is the best choice for a competitive programmer,\nand C++ is nearly always available in contest systems. The bene\ufb01ts of using C++\nare that it is a very ef\ufb01cient language and its standard library contains a large\ncollection of data structures and algorithms.\nOn the other hand, it is good to master several languages and understand\ntheir strengths. For example, if large integers are needed in the problem, Python\ncan be a good choice, because it contains built-in operations for calculating with\n3",
            "large integers. Still, most problems in programming contests are set so that using\na speci\ufb01c programming language is not an unfair advantage.\nAll example programs in this book are written in C++, and the standard\nlibrary\u2019s data structures and algorithms are often used. The programs follow the\nC++11 standard, which can be used in most contests nowadays. If you cannot\nprogram in C++ yet, now is a good time to start learning.\nC++ code template\nA typical C++ code template for competitive programming looks like this:\n#include <bits/stdc++.h>\nusing namespace std;\nint main() {\n// solution comes here\n}\nThe #include line at the beginning of the code is a feature of the g++ compiler\nthat allows us to include the entire standard library. Thus, it is not needed to\nseparately include libraries such as iostream, vector and algorithm, but rather\nthey are available automatically.\nThe using line declares that the classes and functions of the standard library\ncan be used directly in the code. Without the using line we would have to write,\nfor example, std::cout, but now it suf\ufb01ces to write cout.\nThe code can be compiled using the following command:\ng++ -std=c++11 -O2 -Wall test.cpp -o test\nThis command produces a binary \ufb01le test from the source code test.cpp. The\ncompiler follows the C++11 standard (-std=c++11), optimizes the code (-O2) and\nshows warnings about possible errors (-Wall).\n1.2 Input and output\nIn most contests, standard streams are used for reading input and writing output.\nIn C++, the standard streams are cin for input and cout for output. In addition,\nthe C functions scanf and printf can be used.\nThe input for the program usually consists of numbers and strings that are\nseparated with spaces and newlines. They can be read from the cin stream as\nfollows:\nint a, b;\nstring x;\ncin >> a >> b >> x;\n4",
            "This kind of code always works, assuming that there is at least one space or\nnewline between each element in the input. For example, the above code can\nread both of the following inputs:\n123 456 monkey\n123 456\nmonkey\nThe cout stream is used for output as follows:\nint a = 123, b = 456;\nstring x = \"monkey\";\ncout << a << \" \" << b << \" \" << x << \"\\n\";\nInput and output is sometimes a bottleneck in the program. The following\nlines at the beginning of the code make input and output more ef\ufb01cient:\nios::sync_with_stdio(0);\ncin.tie(0);\nNote that the newline \"\\n\" works faster than endl, because endl always\ncauses a \ufb02ush operation.\nThe C functions scanf and printf are an alternative to the C++ standard\nstreams. They are usually a bit faster, but they are also more dif\ufb01cult to use. The\nfollowing code reads two integers from the input:\nint a, b;\nscanf(\"%d %d\", &a, &b);\nThe following code prints two integers:\nint a = 123, b = 456;\nprintf(\"%d %d\\n\", a, b);\nSometimes the program should read a whole line from the input, possibly\ncontaining spaces. This can be accomplished by using the getline function:\nstring s;\ngetline(cin, s);\nIf the amount of data is unknown, the following loop is useful:\nwhile (cin >> x) {\n// code\n}\nThis loop reads elements from the input one after another, until there is no more\ndata available in the input.\n5",
            "In some contest systems, \ufb01les are used for input and output. An easy solution\nfor this is to write the code as usual using standard streams, but add the following\nlines to the beginning of the code:\nfreopen(\"input.txt\", \"r\", stdin);\nfreopen(\"output.txt\", \"w\", stdout);\nAfter this, the program reads the input from the \ufb01le \u201dinput.txt\u201d and writes the\noutput to the \ufb01le \u201doutput.txt\u201d.\n1.3 Working with numbers\nIntegers\nThe most used integer type in competitive programming is int, which is a 32-bit\ntype with a value range of \u2212231 ... 231 \u22121 or about \u22122\u00b7109 ... 2\u00b7109. If the type\nint is not enough, the 64-bit type long long can be used. It has a value range of\n\u2212263 ... 263 \u22121 or about \u22129\u00b71018 ... 9\u00b71018.\nThe following code de\ufb01nes a long long variable:\nlong long x = 123456789123456789LL;\nThe suf\ufb01x LL means that the type of the number is long long.\nA common mistake when using the type long long is that the type int is still\nused somewhere in the code. For example, the following code contains a subtle\nerror:\nint a = 123456789;\nlong long b = a*a;\ncout << b << \"\\n\"; // -1757895751\nEven though the variable b is of type long long, both numbers in the expres-\nsion a*a are of type int and the result is also of type int. Because of this, the\nvariable b will contain a wrong result. The problem can be solved by changing\nthe type of a to long long or by changing the expression to (long long)a*a.\nUsually contest problems are set so that the type long long is enough. Still,\nit is good to know that the g++ compiler also provides a 128-bit type __int128_t\nwith a value range of \u22122127 ... 2127 \u22121 or about \u22121038 ... 1038. However, this type\nis not available in all contest systems.\nModular arithmetic\nWe denote by x mod m the remainder when x is divided by m. For example,\n17 mod 5 = 2, because 17 = 3\u00b75+2.\nSometimes, the answer to a problem is a very large number but it is enough\nto output it \u201dmodulo m\u201d, i.e., the remainder when the answer is divided by m (for\n6",
            "example, \u201dmodulo 109 +7\u201d). The idea is that even if the actual answer is very\nlarge, it suf\ufb01ces to use the types int and long long.\nAn important property of the remainder is that in addition, subtraction and\nmultiplication, the remainder can be taken before the operation:\n(a +b) mod m = (a mod m +b mod m) mod m\n(a \u2212b) mod m = (a mod m \u2212b mod m) mod m\n(a \u00b7b) mod m = (a mod m \u00b7b mod m) mod m\nThus, we can take the remainder after every operation and the numbers will\nnever become too large.\nFor example, the following code calculates n!, the factorial of n, modulo m:\nlong long x = 1;\nfor (int i = 2; i <= n; i++) {\nx = (x*i)%m;\n}\ncout << x%m << \"\\n\";\nUsually we want the remainder to always be between 0... m \u22121. However, in\nC++ and other languages, the remainder of a negative number is either zero or\nnegative. An easy way to make sure there are no negative remainders is to \ufb01rst\ncalculate the remainder as usual and then add m if the result is negative:\nx = x%m;\nif (x < 0) x += m;\nHowever, this is only needed when there are subtractions in the code and the\nremainder may become negative.\nFloating point numbers\nThe usual \ufb02oating point types in competitive programming are the 64-bit double\nand, as an extension in the g++ compiler, the 80-bit long double. In most cases,\ndouble is enough, but long double is more accurate.\nThe required precision of the answer is usually given in the problem statement.\nAn easy way to output the answer is to use the printf function and give the\nnumber of decimal places in the formatting string. For example, the following\ncode prints the value of x with 9 decimal places:\nprintf(\"%.9f\\n\", x);\nA dif\ufb01culty when using \ufb02oating point numbers is that some numbers cannot\nbe represented accurately as \ufb02oating point numbers, and there will be rounding\nerrors. For example, the result of the following code is surprising:\ndouble x = 0.3*3+0.1;\nprintf(\"%.20f\\n\", x); // 0.99999999999999988898\n7",
            "Due to a rounding error, the value ofx is a bit smaller than 1, while the correct\nvalue would be 1.\nIt is risky to compare \ufb02oating point numbers with the == operator, because it\nis possible that the values should be equal but they are not because of precision\nerrors. A better way to compare \ufb02oating point numbers is to assume that two\nnumbers are equal if the difference between them is less than \u03b5, where \u03b5 is a\nsmall number.\nIn practice, the numbers can be compared as follows (\u03b5 = 10\u22129):\nif (abs(a-b) < 1e-9) {\n// a and b are equal\n}\nNote that while \ufb02oating point numbers are inaccurate, integers up to a certain\nlimit can still be represented accurately. For example, using double, it is possible\nto accurately represent all integers whose absolute value is at most 253.\n1.4 Shortening code\nShort code is ideal in competitive programming, because programs should be\nwritten as fast as possible. Because of this, competitive programmers often de\ufb01ne\nshorter names for datatypes and other parts of code.\nType names\nUsing the command typedef it is possible to give a shorter name to a datatype.\nFor example, the name long long is long, so we can de\ufb01ne a shorter name ll:\ntypedef long long ll;\nAfter this, the code\nlong long a = 123456789;\nlong long b = 987654321;\ncout << a*b << \"\\n\";\ncan be shortened as follows:\nll a = 123456789;\nll b = 987654321;\ncout << a*b << \"\\n\";\nThe command typedef can also be used with more complex types. For example,\nthe following code gives the name vi for a vector of integers and the name pi for\na pair that contains two integers.\ntypedef vector<int> vi;\ntypedef pair<int,int> pi;\n8",
            "Macros\nAnother way to shorten code is to de\ufb01ne macros. A macro means that certain\nstrings in the code will be changed before the compilation. In C++, macros are\nde\ufb01ned using the #define keyword.\nFor example, we can de\ufb01ne the following macros:\n#define F first\n#define S second\n#define PB push_back\n#define MP make_pair\nAfter this, the code\nv.push_back(make_pair(y1,x1));\nv.push_back(make_pair(y2,x2));\nint d = v[i].first+v[i].second;\ncan be shortened as follows:\nv.PB(MP(y1,x1));\nv.PB(MP(y2,x2));\nint d = v[i].F+v[i].S;\nA macro can also have parameters which makes it possible to shorten loops\nand other structures. For example, we can de\ufb01ne the following macro:\n#define REP(i,a,b) for (int i = a; i <= b; i++)\nAfter this, the code\nfor (int i = 1; i <= n; i++) {\nsearch(i);\n}\ncan be shortened as follows:\nREP(i,1,n) {\nsearch(i);\n}\nSometimes macros cause bugs that may be dif\ufb01cult to detect. For example,\nconsider the following macro that calculates the square of a number:\n#define SQ(a) a*a\nThis macro does not always work as expected. For example, the code\ncout << SQ(3+3) << \"\\n\";\n9",
            "corresponds to the code\ncout << 3+3*3+3 << \"\\n\"; // 15\nA better version of the macro is as follows:\n#define SQ(a) (a)*(a)\nNow the code\ncout << SQ(3+3) << \"\\n\";\ncorresponds to the code\ncout << (3+3)*(3+3) << \"\\n\"; // 36\n1.5 Mathematics\nMathematics plays an important role in competitive programming, and it is\nnot possible to become a successful competitive programmer without having\ngood mathematical skills. This section discusses some important mathematical\nconcepts and formulas that are needed later in the book.\nSum formulas\nEach sum of the form\nn\u2211\nx=1\nxk = 1k +2k +3k +... +nk,\nwhere k is a positive integer, has a closed-form formula that is a polynomial of\ndegree k +1. For example1,\nn\u2211\nx=1\nx = 1+2+3+... +n = n(n +1)\n2\nand\nn\u2211\nx=1\nx2 = 12 +22 +32 +... +n2 = n(n +1)(2n +1)\n6 .\nAn arithmetic progression is a sequence of numbers where the difference\nbetween any two consecutive numbers is constant. For example,\n3,7,11,15\n1 There is even a general formula for such sums, called Faulhaber\u2019s formula, but it is too\ncomplex to be presented here.\n10",
            "is an arithmetic progression with constant 4. The sum of an arithmetic progres-\nsion can be calculated using the formula\na +\u00b7\u00b7\u00b7+ b\ued19 \ued18\ued17 \ued1a\nn numbers\n= n(a +b)\n2\nwhere a is the \ufb01rst number, b is the last number and n is the amount of numbers.\nFor example,\n3+7+11+15 = 4\u00b7(3+15)\n2 = 36.\nThe formula is based on the fact that the sum consists of n numbers and the\nvalue of each number is (a +b)/2 on average.\nA geometric progression is a sequence of numbers where the ratio between\nany two consecutive numbers is constant. For example,\n3,6,12,24\nis a geometric progression with constant 2. The sum of a geometric progression\ncan be calculated using the formula\na +ak +ak2 +\u00b7\u00b7\u00b7+ b = bk \u2212a\nk \u22121\nwhere a is the \ufb01rst number, b is the last number and the ratio between consecu-\ntive numbers is k. For example,\n3+6+12+24 = 24\u00b72\u22123\n2\u22121 = 45.\nThis formula can be derived as follows. Let\nS = a +ak +ak2 +\u00b7\u00b7\u00b7+ b.\nBy multiplying both sides by k, we get\nkS = ak +ak2 +ak3 +\u00b7\u00b7\u00b7+ bk,\nand solving the equation\nkS \u2212S = bk \u2212a\nyields the formula.\nA special case of a sum of a geometric progression is the formula\n1+2+4+8+... +2n\u22121 = 2n \u22121.\nA harmonic sum is a sum of the form\nn\u2211\nx=1\n1\nx = 1+ 1\n2 + 1\n3 +... + 1\nn.\nAn upper bound for a harmonic sum is log2(n) +1. Namely, we can modify\neach term 1/k so that k becomes the nearest power of two that does not exceed k.\nFor example, when n = 6, we can estimate the sum as follows:\n1+ 1\n2 + 1\n3 + 1\n4 + 1\n5 + 1\n6 \u2264 1+ 1\n2 + 1\n2 + 1\n4 + 1\n4 + 1\n4.\nThis upper bound consists of log2(n)+1 parts (1, 2\u00b71/2, 4\u00b71/4, etc.), and the value\nof each part is at most 1.\n11",
            "Set theory\nA set is a collection of elements. For example, the set\nX = {2,4,7}\ncontains elements 2, 4 and 7. The symbol \u2208 denotes an empty set, and |S| denotes\nthe size of a set S, i.e., the number of elements in the set. For example, in the\nabove set, |X| =3.\nIf a set S contains an element x, we write x \u2208 S, and otherwise we write x \u2209 S.\nFor example, in the above set\n4 \u2208 X and 5 \u2209 X.\nNew sets can be constructed using set operations:\n\u2022 The intersection A \u2229B consists of elements that are in both A and B. For\nexample, if A = {1,2,5} and B = {2,4}, then A \u2229B = {2}.\n\u2022 The union A \u222a B consists of elements that are in A or B or both. For\nexample, if A = {3,7} and B = {2,3,8}, then A \u222aB = {2,3,7,8}.\n\u2022 The complement \u00afA consists of elements that are not in A. The interpre-\ntation of a complement depends on the universal set, which contains all\npossible elements. For example, if A = {1,2,5,7} and the universal set is\n{1,2,..., 10}, then \u00afA = {3,4,6,8,9,10}.\n\u2022 The difference A \\ B = A \u2229 \u00afB consists of elements that are in A but not\nin B. Note that B can contain elements that are not in A. For example, if\nA = {2,3,7,8} and B = {3,5,8}, then A \\ B = {2,7}.\nIf each element of A also belongs to S, we say that A is a subset of S, denoted\nby A \u2282 S. A set S always has 2|S| subsets, including the empty set. For example,\nthe subsets of the set {2,4,7} are\n\u2208, {2}, {4}, {7}, {2,4}, {2,7}, {4,7} and {2,4,7}.\nSome often used sets are N (natural numbers), Z (integers), Q (rational\nnumbers) and R (real numbers). The set N can be de\ufb01ned in two ways, depending\non the situation: either N = {0,1,2,... } or N = {1,2,3,...}.\nWe can also construct a set using a rule of the form\n{f (n) :n \u2208 S},\nwhere f (n) is some function. This set contains all elements of the form f (n),\nwhere n is an element in S. For example, the set\nX = {2n : n \u2208 Z}\ncontains all even integers.\n12",
            "Logic\nThe value of a logical expression is either true (1) or false (0). The most impor-\ntant logical operators are \u00ac (negation), \u2227 (conjunction), \u2228 (disjunction), \u21d2\n(implication) and \u21d4 (equivalence). The following table shows the meanings\nof these operators:\nA B \u00acA \u00acB A \u2227B A \u2228B A \u21d2 B A \u21d4 B\n0 0 1 1 0 0 1 1\n0 1 1 0 0 1 1 0\n1 0 0 1 0 1 0 0\n1 1 0 0 1 1 1 1\nThe expression \u00acA has the opposite value of A. The expression A \u2227B is true\nif both A and B are true, and the expression A \u2228B is true if A or B or both are\ntrue. The expression A \u21d2 B is true if whenever A is true, also B is true. The\nexpression A \u21d4 B is true if A and B are both true or both false.\nA predicate is an expression that is true or false depending on its parameters.\nPredicates are usually denoted by capital letters. For example, we can de\ufb01ne\na predicate P(x) that is true exactly when x is a prime number. Using this\nde\ufb01nition, P(7) is true but P(8) is false.\nA quanti\ufb01er connects a logical expression to the elements of a set. The most\nimportant quanti\ufb01ers are \u2200 (for all) and \u2203 (there is). For example,\n\u2200x(\u2203y(y < x))\nmeans that for each element x in the set, there is an element y in the set such\nthat y is smaller than x. This is true in the set of integers, but false in the set of\nnatural numbers.\nUsing the notation described above, we can express many kinds of logical\npropositions. For example,\n\u2200x((x > 1\u2227\u00acP(x)) \u21d2 (\u2203a(\u2203b(a > 1\u2227b > 1\u2227 x = ab))))\nmeans that if a number x is larger than 1 and not a prime number, then there are\nnumbers a and b that are larger than 1 and whose product is x. This proposition\nis true in the set of integers.\nFunctions\nThe function \u230ax\u230b rounds the number x down to an integer, and the function \u2308x\u2309\nrounds the number x up to an integer. For example,\n\u230a3/2\u230b =1 and \u23083/2\u2309 =2.\nThe functions min(x1, x2,..., xn) and max(x1, x2,..., xn) give the smallest and\nlargest of values x1, x2,..., xn. For example,\nmin(1,2,3) = 1 and max(1 ,2,3) = 3.\n13",
            "The factorial n! can be de\ufb01ned\nn\u220f\nx=1\nx = 1\u00b72\u00b73\u00b7... \u00b7n\nor recursively\n0! = 1\nn! = n \u00b7(n \u22121)!\nThe Fibonacci numbers arise in many situations. They can be de\ufb01ned\nrecursively as follows:\nf (0) = 0\nf (1) = 1\nf (n) = f (n \u22121)+ f (n \u22122)\nThe \ufb01rst Fibonacci numbers are\n0,1,1,2,3,5,8,13,21,34,55,...\nThere is also a closed-form formula for calculating Fibonacci numbers, which is\nsometimes called Binet\u2019s formula:\nf (n) = (1+\n\u2282\n5)n \u2212(1\u2212\n\u2282\n5)n\n2n\u2282\n5\n.\nLogarithms\nThe logarithm of a number x is denoted logk(x), where k is the base of the\nlogarithm. According to the de\ufb01nition, log k(x) = a exactly when ka = x.\nA useful property of logarithms is that logk(x) equals the number of times we\nhave to divide x by k before we reach the number 1. For example, log2(32) = 5\nbecause 5 divisions by 2 are needed:\n32 \u2192 16 \u2192 8 \u2192 4 \u2192 2 \u2192 1\nLogarithms are often used in the analysis of algorithms, because many ef-\n\ufb01cient algorithms halve something at each step. Hence, we can estimate the\nef\ufb01ciency of such algorithms using logarithms.\nThe logarithm of a product is\nlogk(ab) = logk(a)+logk(b),\nand consequently,\nlogk(xn) = n \u00b7logk(x).\nIn addition, the logarithm of a quotient is\nlogk\n(a\nb\n)\n= logk(a)\u2212logk(b).\nAnother useful formula is\nlogu(x) = logk(x)\nlogk(u),\n14",
            "and using this, it is possible to calculate logarithms to any base if there is a way\nto calculate logarithms to some \ufb01xed base.\nThe natural logarithm ln(x) of a number x is a logarithm whose base is\ne \u2248 2.71828. Another property of logarithms is that the number of digits of an\ninteger x in base b is \u230alogb(x)+1\u230b. For example, the representation of 123 in base\n2 is 1111011 and \u230alog2(123)+1\u230b =7.\n1.6 Contests and resources\nIOI\nThe International Olympiad in Informatics (IOI) is an annual programming\ncontest for secondary school students. Each country is allowed to send a team of\nfour students to the contest. There are usually about 300 participants from 80\ncountries.\nThe IOI consists of two \ufb01ve-hour long contests. In both contests, the partic-\nipants are asked to solve three algorithm tasks of various dif\ufb01culty. The tasks\nare divided into subtasks, each of which has an assigned score. Even if the\ncontestants are divided into teams, they compete as individuals.\nThe IOI syllabus [ 41] regulates the topics that may appear in IOI tasks.\nAlmost all the topics in the IOI syllabus are covered by this book.\nParticipants for the IOI are selected through national contests. Before the IOI,\nmany regional contests are organized, such as the Baltic Olympiad in Informatics\n(BOI), the Central European Olympiad in Informatics (CEOI) and the Asia-Paci\ufb01c\nInformatics Olympiad (APIO).\nSome countries organize online practice contests for future IOI participants,\nsuch as the Croatian Open Competition in Informatics [11] and the USA Comput-\ning Olympiad [68]. In addition, a large collection of problems from Polish contests\nis available online [60].\nICPC\nThe International Collegiate Programming Contest (ICPC) is an annual program-\nming contest for university students. Each team in the contest consists of three\nstudents, and unlike in the IOI, the students work together; there is only one\ncomputer available for each team.\nThe ICPC consists of several stages, and \ufb01nally the best teams are invited to\nthe World Finals. While there are tens of thousands of participants in the contest,\nthere are only a small number2 of \ufb01nal slots available, so even advancing to the\n\ufb01nals is a great achievement in some regions.\nIn each ICPC contest, the teams have \ufb01ve hours of time to solve about ten\nalgorithm problems. A solution to a problem is accepted only if it solves all test\ncases ef\ufb01ciently. During the contest, competitors may view the results of other\n2The exact number of \ufb01nal slots varies from year to year; in 2017, there were 133 \ufb01nal slots.\n15",
            "teams, but for the last hour the scoreboard is frozen and it is not possible to see\nthe results of the last submissions.\nThe topics that may appear at the ICPC are not so well speci\ufb01ed as those\nat the IOI. In any case, it is clear that more knowledge is needed at the ICPC,\nespecially more mathematical skills.\nOnline contests\nThere are also many online contests that are open for everybody. At the moment,\nthe most active contest site is Codeforces, which organizes contests about weekly.\nIn Codeforces, participants are divided into two divisions: beginners compete in\nDiv2 and more experienced programmers in Div1. Other contest sites include\nAtCoder, CS Academy, HackerRank and Topcoder.\nSome companies organize online contests with onsite \ufb01nals. Examples of such\ncontests are Facebook Hacker Cup, Google Code Jam and Yandex.Algorithm. Of\ncourse, companies also use those contests for recruiting: performing well in a\ncontest is a good way to prove one\u2019s skills.\nBooks\nThere are already some books (besides this book) that focus on competitive\nprogramming and algorithmic problem solving:\n\u2022 S. S. Skiena and M. A. Revilla:Programming Challenges: The Programming\nContest Training Manual [59]\n\u2022 S. Halim and F. Halim:Competitive Programming 3: The New Lower Bound\nof Programming Contests [33]\n\u2022 K. Diks et al.: Looking for a Challenge? The Ultimate Problem Set from the\nUniversity of Warsaw Programming Competitions [15]\nThe \ufb01rst two books are intended for beginners, whereas the last book contains\nadvanced material.\nOf course, general algorithm books are also suitable for competitive program-\nmers. Some popular books are:\n\u2022 T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein: Introduction to\nAlgorithms [13]\n\u2022 J. Kleinberg and \u00c9. Tardos: Algorithm Design [45]\n\u2022 S. S. Skiena: The Algorithm Design Manual [58]\n16",
            "Chapter 2\nTime complexity\nThe ef\ufb01ciency of algorithms is important in competitive programming. Usually,\nit is easy to design an algorithm that solves the problem slowly, but the real\nchallenge is to invent a fast algorithm. If the algorithm is too slow, it will get only\npartial points or no points at all.\nThe time complexity of an algorithm estimates how much time the algo-\nrithm will use for some input. The idea is to represent the ef\ufb01ciency as a function\nwhose parameter is the size of the input. By calculating the time complexity, we\ncan \ufb01nd out whether the algorithm is fast enough without implementing it.\n2.1 Calculation rules\nThe time complexity of an algorithm is denoted O(\u00b7\u00b7\u00b7 ) where the three dots\nrepresent some function. Usually, the variable n denotes the input size. For\nexample, if the input is an array of numbers, n will be the size of the array, and if\nthe input is a string, n will be the length of the string.\nLoops\nA common reason why an algorithm is slow is that it contains many loops that go\nthrough the input. The more nested loops the algorithm contains, the slower it is.\nIf there are k nested loops, the time complexity is O(nk).\nFor example, the time complexity of the following code is O(n):\nfor (int i = 1; i <= n; i++) {\n// code\n}\nAnd the time complexity of the following code is O(n2):\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= n; j++) {\n// code\n}\n}\n17",
            "Order of magnitude\nA time complexity does not tell us the exact number of times the code inside\na loop is executed, but it only shows the order of magnitude. In the following\nexamples, the code inside the loop is executed 3n, n +5 and \u2308n/2\u2309 times, but the\ntime complexity of each code is O(n).\nfor (int i = 1; i <= 3*n; i++) {\n// code\n}\nfor (int i = 1; i <= n+5; i++) {\n// code\n}\nfor (int i = 1; i <= n; i += 2) {\n// code\n}\nAs another example, the time complexity of the following code is O(n2):\nfor (int i = 1; i <= n; i++) {\nfor (int j = i+1; j <= n; j++) {\n// code\n}\n}\nPhases\nIf the algorithm consists of consecutive phases, the total time complexity is the\nlargest time complexity of a single phase. The reason for this is that the slowest\nphase is usually the bottleneck of the code.\nFor example, the following code consists of three phases with time complexities\nO(n), O(n2) and O(n). Thus, the total time complexity is O(n2).\nfor (int i = 1; i <= n; i++) {\n// code\n}\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= n; j++) {\n// code\n}\n}\nfor (int i = 1; i <= n; i++) {\n// code\n}\n18",
            "Several variables\nSometimes the time complexity depends on several factors. In this case, the time\ncomplexity formula contains several variables.\nFor example, the time complexity of the following code is O(nm):\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= m; j++) {\n// code\n}\n}\nRecursion\nThe time complexity of a recursive function depends on the number of times\nthe function is called and the time complexity of a single call. The total time\ncomplexity is the product of these values.\nFor example, consider the following function:\nvoid f(int n) {\nif (n == 1) return;\nf(n-1);\n}\nThe call f(n) causes n function calls, and the time complexity of each call is O(1).\nThus, the total time complexity is O(n).\nAs another example, consider the following function:\nvoid g(int n) {\nif (n == 1) return;\ng(n-1);\ng(n-1);\n}\nIn this case each function call generates two other calls, except for n = 1. Let us\nsee what happens when g is called with parameter n. The following table shows\nthe function calls produced by this single call:\nfunction call number of calls\ng(n) 1\ng(n \u22121) 2\ng(n \u22122) 4\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\ng(1) 2 n\u22121\nBased on this, the time complexity is\n1+2+4+\u00b7\u00b7\u00b7+ 2n\u22121 = 2n \u22121 = O(2n).\n19",
            "2.2 Complexity classes\nThe following list contains common time complexities of algorithms:\nO(1) The running time of a constant-time algorithm does not depend on the\ninput size. A typical constant-time algorithm is a direct formula that\ncalculates the answer.\nO(logn) A logarithmic algorithm often halves the input size at each step. The\nrunning time of such an algorithm is logarithmic, because log2 n equals the\nnumber of times n must be divided by 2 to get 1.\nO(\u2282n) A square root algorithm is slower than O(logn) but faster than O(n).\nA special property of square roots is that \u2282n = n/\u2282n, so the square root \u2282n\nlies, in some sense, in the middle of the input.\nO(n) A linear algorithm goes through the input a constant number of times. This\nis often the best possible time complexity, because it is usually necessary to\naccess each input element at least once before reporting the answer.\nO(nlogn) This time complexity often indicates that the algorithm sorts the input,\nbecause the time complexity of ef\ufb01cient sorting algorithms is O(nlogn).\nAnother possibility is that the algorithm uses a data structure where each\noperation takes O(logn) time.\nO(n2) A quadratic algorithm often contains two nested loops. It is possible to\ngo through all pairs of the input elements in O(n2) time.\nO(n3) A cubic algorithm often contains three nested loops. It is possible to go\nthrough all triplets of the input elements in O(n3) time.\nO(2n) This time complexity often indicates that the algorithm iterates through\nall subsets of the input elements. For example, the subsets of {1,2,3} are \u2208,\n{1}, {2}, {3}, {1,2}, {1,3}, {2,3} and {1,2,3}.\nO(n!) This time complexity often indicates that the algorithm iterates through\nall permutations of the input elements. For example, the permutations of\n{1,2,3} are (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2) and (3,2,1).\nAn algorithm is polynomial if its time complexity is at mostO(nk) where k is\na constant. All the above time complexities exceptO(2n) and O(n!) are polynomial.\nIn practice, the constant k is usually small, and therefore a polynomial time\ncomplexity roughly means that the algorithm is ef\ufb01cient.\nMost algorithms in this book are polynomial. Still, there are many important\nproblems for which no polynomial algorithm is known, i.e., nobody knows how to\nsolve them ef\ufb01ciently. NP-hard problems are an important set of problems, for\nwhich no polynomial algorithm is known1.\n1A classic book on the topic is M. R. Garey\u2019s and D. S. Johnson\u2019sComputers and Intractability:\nA Guide to the Theory of NP-Completeness [28].\n20",
            "2.3 Estimating ef\ufb01ciency\nBy calculating the time complexity of an algorithm, it is possible to check, before\nimplementing the algorithm, that it is ef\ufb01cient enough for the problem. The\nstarting point for estimations is the fact that a modern computer can perform\nsome hundreds of millions of operations in a second.\nFor example, assume that the time limit for a problem is one second and the\ninput size is n = 105. If the time complexity is O(n2), the algorithm will perform\nabout (105)2 = 1010 operations. This should take at least some tens of seconds, so\nthe algorithm seems to be too slow for solving the problem.\nOn the other hand, given the input size, we can try to guess the required time\ncomplexity of the algorithm that solves the problem. The following table contains\nsome useful estimates assuming a time limit of one second.\ninput size required time complexity\nn \u2264 10 O(n!)\nn \u2264 20 O(2n)\nn \u2264 500 O(n3)\nn \u2264 5000 O(n2)\nn \u2264 106 O(nlogn) or O(n)\nn is large O(1) or O(logn)\nFor example, if the input size is n = 105, it is probably expected that the\ntime complexity of the algorithm is O(n) or O(nlogn). This information makes it\neasier to design the algorithm, because it rules out approaches that would yield\nan algorithm with a worse time complexity.\nStill, it is important to remember that a time complexity is only an estimate\nof ef\ufb01ciency, because it hides the constant factors. For example, an algorithm\nthat runs in O(n) time may perform n/2 or 5n operations. This has an important\neffect on the actual running time of the algorithm.\n2.4 Maximum subarray sum\nThere are often several possible algorithms for solving a problem such that their\ntime complexities are different. This section discusses a classic problem that has\na straightforward O(n3) solution. However, by designing a better algorithm, it is\npossible to solve the problem in O(n2) time and even in O(n) time.\nGiven an array of n numbers, our task is to calculate the maximum subar-\nray sum, i.e., the largest possible sum of a sequence of consecutive values in the\narray2. The problem is interesting when there may be negative values in the\narray. For example, in the array\n\u22121 2 4 \u22123 5 2 \u22125 2\n2J. Bentley\u2019s bookProgramming Pearls [8] made the problem popular.\n21",
            "the following subarray produces the maximum sum 10:\n\u22121 2 4 \u22123 5 2 \u22125 2\nWe assume that an empty subarray is allowed, so the maximum subarray\nsum is always at least 0.\nAlgorithm 1\nA straightforward way to solve the problem is to go through all possible subarrays,\ncalculate the sum of values in each subarray and maintain the maximum sum.\nThe following code implements this algorithm:\nint best = 0;\nfor (int a = 0; a < n; a++) {\nfor (int b = a; b < n; b++) {\nint sum = 0;\nfor (int k = a; k <= b; k++) {\nsum += array[k];\n}\nbest = max(best,sum);\n}\n}\ncout << best << \"\\n\";\nThe variables a and b \ufb01x the \ufb01rst and last index of the subarray, and the\nsum of values is calculated to the variable sum. The variable best contains the\nmaximum sum found during the search.\nThe time complexity of the algorithm is O(n3), because it consists of three\nnested loops that go through the input.\nAlgorithm 2\nIt is easy to make Algorithm 1 more ef\ufb01cient by removing one loop from it. This\nis possible by calculating the sum at the same time when the right end of the\nsubarray moves. The result is the following code:\nint best = 0;\nfor (int a = 0; a < n; a++) {\nint sum = 0;\nfor (int b = a; b < n; b++) {\nsum += array[b];\nbest = max(best,sum);\n}\n}\ncout << best << \"\\n\";\nAfter this change, the time complexity is O(n2).\n22",
            "Algorithm 3\nSurprisingly, it is possible to solve the problem in O(n) time3, which means that\njust one loop is enough. The idea is to calculate, for each array position, the\nmaximum sum of a subarray that ends at that position. After this, the answer\nfor the problem is the maximum of those sums.\nConsider the subproblem of \ufb01nding the maximum-sum subarray that ends at\nposition k. There are two possibilities:\n1. The subarray only contains the element at position k.\n2. The subarray consists of a subarray that ends at position k \u22121, followed by\nthe element at position k.\nIn the latter case, since we want to \ufb01nd a subarray with maximum sum, the\nsubarray that ends at position k \u22121 should also have the maximum sum. Thus,\nwe can solve the problem ef\ufb01ciently by calculating the maximum subarray sum\nfor each ending position from left to right.\nThe following code implements the algorithm:\nint best = 0, sum = 0;\nfor (int k = 0; k < n; k++) {\nsum = max(array[k],sum+array[k]);\nbest = max(best,sum);\n}\ncout << best << \"\\n\";\nThe algorithm only contains one loop that goes through the input, so the time\ncomplexity is O(n). This is also the best possible time complexity, because any\nalgorithm for the problem has to examine all array elements at least once.\nE\ufb03ciency comparison\nIt is interesting to study how ef\ufb01cient algorithms are in practice. The following\ntable shows the running times of the above algorithms for different values of n\non a modern computer.\nIn each test, the input was generated randomly. The time needed for reading\nthe input was not measured.\narray size n Algorithm 1 Algorithm 2 Algorithm 3\n102 0.0 s 0 .0 s 0 .0 s\n103 0.1 s 0 .0 s 0 .0 s\n104 > 10.0 s 0 .1 s 0 .0 s\n105 > 10.0 s 5 .3 s 0 .0 s\n106 > 10.0 s > 10 .0 s 0 .0 s\n107 > 10.0 s > 10 .0 s 0 .0 s\n3In [8], this linear-time algorithm is attributed to J. B. Kadane, and the algorithm is sometimes\ncalled Kadane\u2019s algorithm.\n23",
            "The comparison shows that all algorithms are ef\ufb01cient when the input size is\nsmall, but larger inputs bring out remarkable differences in the running times\nof the algorithms. Algorithm 1 becomes slow when n = 104, and Algorithm 2\nbecomes slow when n = 105. Only Algorithm 3 is able to process even the largest\ninputs instantly.\n24",
            "Chapter 3\nSorting\nSorting is a fundamental algorithm design problem. Many ef\ufb01cient algorithms\nuse sorting as a subroutine, because it is often easier to process data if the\nelements are in a sorted order.\nFor example, the problem \u201ddoes an array contain two equal elements?\u201d is easy\nto solve using sorting. If the array contains two equal elements, they will be next\nto each other after sorting, so it is easy to \ufb01nd them. Also, the problem \u201dwhat is\nthe most frequent element in an array?\u201d can be solved similarly.\nThere are many algorithms for sorting, and they are also good examples of\nhow to apply different algorithm design techniques. The ef\ufb01cient general sorting\nalgorithms work in O(nlogn) time, and many algorithms that use sorting as a\nsubroutine also have this time complexity.\n3.1 Sorting theory\nThe basic problem in sorting is as follows:\nGiven an array that contains n elements, your task is to sort the elements in\nincreasing order.\nFor example, the array\n1 3 8 2 9 2 5 6\nwill be as follows after sorting:\n1 2 2 3 5 6 8 9\nO(n2) algorithms\nSimple algorithms for sorting an array work in O(n2) time. Such algorithms\nare short and usually consist of two nested loops. A famous O(n2) time sorting\n25",
            "algorithm is bubble sort where the elements \u201dbubble\u201d in the array according to\ntheir values.\nBubble sort consists of n rounds. On each round, the algorithm iterates\nthrough the elements of the array. Whenever two consecutive elements are found\nthat are not in correct order, the algorithm swaps them. The algorithm can be\nimplemented as follows:\nfor (int i = 0; i < n; i++) {\nfor (int j = 0; j < n-1; j++) {\nif (array[j] > array[j+1]) {\nswap(array[j],array[j+1]);\n}\n}\n}\nAfter the \ufb01rst round of the algorithm, the largest element will be in the correct\nposition, and in general, after k rounds, the k largest elements will be in the\ncorrect positions. Thus, after n rounds, the whole array will be sorted.\nFor example, in the array\n1 3 8 2 9 2 5 6\nthe \ufb01rst round of bubble sort swaps elements as follows:\n1 3 2 8 9 2 5 6\n1 3 2 8 2 9 5 6\n1 3 2 8 2 5 9 6\n1 3 2 8 2 5 6 9\nInversions\nBubble sort is an example of a sorting algorithm that always swaps consecutive\nelements in the array. It turns out that the time complexity of such an algorithm\nis always at least O(n2), because in the worst case, O(n2) swaps are required for\nsorting the array.\nA useful concept when analyzing sorting algorithms is an inversion: a pair\nof array elements (array[a],array[b]) such that a < b and array[a] > array[b], i.e.,\nthe elements are in the wrong order. For example, the array\n26",
            "1 2 2 6 3 5 9 8\nhas three inversions: (6,3), (6,5) and (9,8). The number of inversions indicates\nhow much work is needed to sort the array. An array is completely sorted when\nthere are no inversions. On the other hand, if the array elements are in the\nreverse order, the number of inversions is the largest possible:\n1+2+\u00b7\u00b7\u00b7+ (n \u22121) = n(n \u22121)\n2 = O(n2)\nSwapping a pair of consecutive elements that are in the wrong order removes\nexactly one inversion from the array. Hence, if a sorting algorithm can only swap\nconsecutive elements, each swap removes at most one inversion, and the time\ncomplexity of the algorithm is at least O(n2).\nO(nlogn) algorithms\nIt is possible to sort an array ef\ufb01ciently in O(nlogn) time using algorithms that\nare not limited to swapping consecutive elements. One such algorithm is merge\nsort1, which is based on recursion.\nMerge sort sorts a subarray array[a... b] as follows:\n1. If a = b, do not do anything, because the subarray is already sorted.\n2. Calculate the position of the middle element: k = \u230a(a +b)/2\u230b.\n3. Recursively sort the subarray array[a... k].\n4. Recursively sort the subarray array[k +1... b].\n5. Merge the sorted subarrays array[a... k] and array[k +1... b] into a sorted\nsubarray array[a... b].\nMerge sort is an ef\ufb01cient algorithm, because it halves the size of the subarray\nat each step. The recursion consists of O(logn) levels, and processing each level\ntakes O(n) time. Merging the subarrays array[a... k] and array[k +1... b] is\npossible in linear time, because they are already sorted.\nFor example, consider sorting the following array:\n1 3 6 2 8 2 5 9\nThe array will be divided into two subarrays as follows:\n1 3 6 2 8 2 5 9\nThen, the subarrays will be sorted recursively as follows:\n1 2 3 6 2 5 8 9\n1According to [47], merge sort was invented by J. von Neumann in 1945.\n27",
            "Finally, the algorithm merges the sorted subarrays and creates the \ufb01nal\nsorted array:\n1 2 2 3 5 6 8 9\nSorting lower bound\nIs it possible to sort an array faster than in O(nlogn) time? It turns out that this\nis not possible when we restrict ourselves to sorting algorithms that are based on\ncomparing array elements.\nThe lower bound for the time complexity can be proved by considering sorting\nas a process where each comparison of two elements gives more information\nabout the contents of the array. The process creates the following tree:\nx < y?\nx < y? x < y?\nx < y? x < y? x < y? x < y?\nHere \u201dx < y?\u201d means that some elementsx and y are compared. If x < y, the\nprocess continues to the left, and otherwise to the right. The results of the process\nare the possible ways to sort the array, a total of n! ways. For this reason, the\nheight of the tree must be at least\nlog2(n!) = log2(1)+log2(2)+\u00b7\u00b7\u00b7+ log2(n).\nWe get a lower bound for this sum by choosing the lastn/2 elements and changing\nthe value of each element to log2(n/2). This yields an estimate\nlog2(n!) \u2265 (n/2)\u00b7log2(n/2),\nso the height of the tree and the minimum possible number of steps in a sorting\nalgorithm in the worst case is at least nlogn.\nCounting sort\nThe lower bound nlogn does not apply to algorithms that do not compare array\nelements but use some other information. An example of such an algorithm is\ncounting sort that sorts an array in O(n) time assuming that every element in\nthe array is an integer between 0... c and c = O(n).\nThe algorithm creates a bookkeeping array, whose indices are elements of the\noriginal array. The algorithm iterates through the original array and calculates\nhow many times each element appears in the array.\n28",
            "For example, the array\n1 3 6 9 9 3 5 9\ncorresponds to the following bookkeeping array:\n1 0 2 0 1 1 0 0 3\n1 2 3 4 5 6 7 8 9\nFor example, the value at position 3 in the bookkeeping array is 2, because\nthe element 3 appears 2 times in the original array.\nConstruction of the bookkeeping array takes O(n) time. After this, the sorted\narray can be created in O(n) time because the number of occurrences of each\nelement can be retrieved from the bookkeeping array. Thus, the total time\ncomplexity of counting sort is O(n).\nCounting sort is a very ef\ufb01cient algorithm but it can only be used when the\nconstant c is small enough, so that the array elements can be used as indices in\nthe bookkeeping array.\n3.2 Sorting in C++\nIt is almost never a good idea to use a home-made sorting algorithm in a contest,\nbecause there are good implementations available in programming languages.\nFor example, the C++ standard library contains the function sort that can be\neasily used for sorting arrays and other data structures.\nThere are many bene\ufb01ts in using a library function. First, it saves time\nbecause there is no need to implement the function. Second, the library imple-\nmentation is certainly correct and ef\ufb01cient: it is not probable that a home-made\nsorting function would be better.\nIn this section we will see how to use the C++ sort function. The following\ncode sorts a vector in increasing order:\nvector<int> v = {4,2,5,3,5,8,3};\nsort(v.begin(),v.end());\nAfter the sorting, the contents of the vector will be [2,3,3,4,5,5,8]. The default\nsorting order is increasing, but a reverse order is possible as follows:\nsort(v.rbegin(),v.rend());\nAn ordinary array can be sorted as follows:\nint n = 7; // array size\nint a[] = {4,2,5,3,5,8,3};\nsort(a,a+n);\n29",
            "The following code sorts the string s:\nstring s = \"monkey\";\nsort(s.begin(), s.end());\nSorting a string means that the characters of the string are sorted. For example,\nthe string \u201dmonkey\u201d becomes \u201dekmnoy\u201d.\nComparison operators\nThe function sort requires that a comparison operator is de\ufb01ned for the data\ntype of the elements to be sorted. When sorting, this operator will be used\nwhenever it is necessary to \ufb01nd out the order of two elements.\nMost C++ data types have a built-in comparison operator, and elements\nof those types can be sorted automatically. For example, numbers are sorted\naccording to their values and strings are sorted in alphabetical order.\nPairs (pair) are sorted primarily according to their \ufb01rst elements ( first).\nHowever, if the \ufb01rst elements of two pairs are equal, they are sorted according to\ntheir second elements (second):\nvector<pair<int,int>> v;\nv.push_back({1,5});\nv.push_back({2,3});\nv.push_back({1,2});\nsort(v.begin(), v.end());\nAfter this, the order of the pairs is (1,2), (1,5) and (2,3).\nIn a similar way, tuples ( tuple) are sorted primarily by the \ufb01rst element,\nsecondarily by the second element, etc.2:\nvector<tuple<int,int,int>> v;\nv.push_back({2,1,4});\nv.push_back({1,5,3});\nv.push_back({2,1,3});\nsort(v.begin(), v.end());\nAfter this, the order of the tuples is (1,5,3), (2,1,3) and (2,1,4).\nUser-de\ufb01ned structs\nUser-de\ufb01ned structs do not have a comparison operator automatically. The\noperator should be de\ufb01ned inside the struct as a function operator<, whose\nparameter is another element of the same type. The operator should return true\nif the element is smaller than the parameter, and false otherwise.\nFor example, the following struct P contains the x and y coordinates of a point.\nThe comparison operator is de\ufb01ned so that the points are sorted primarily by the\n2Note that in some older compilers, the function make_tuple has to be used to create a tuple\ninstead of braces (for example, make_tuple(2,1,4) instead of {2,1,4}).\n30",
            "x coordinate and secondarily by the y coordinate.\nstruct P {\nint x, y;\nbool operator<(const P &p) {\nif (x != p.x) return x < p.x;\nelse return y < p.y;\n}\n};\nComparison functions\nIt is also possible to give an external comparison function to the sort function\nas a callback function. For example, the following comparison function comp sorts\nstrings primarily by length and secondarily by alphabetical order:\nbool comp(string a, string b) {\nif (a.size() != b.size()) return a.size() < b.size();\nreturn a < b;\n}\nNow a vector of strings can be sorted as follows:\nsort(v.begin(), v.end(), comp);\n3.3 Binary search\nA general method for searching for an element in an array is to use a for loop\nthat iterates through the elements of the array. For example, the following code\nsearches for an element x in an array:\nfor (int i = 0; i < n; i++) {\nif (array[i] == x) {\n// x found at index i\n}\n}\nThe time complexity of this approach is O(n), because in the worst case, it\nis necessary to check all elements of the array. If the order of the elements is\narbitrary, this is also the best possible approach, because there is no additional\ninformation available where in the array we should search for the element x.\nHowever, if the array is sorted, the situation is different. In this case it is\npossible to perform the search much faster, because the order of the elements in\nthe array guides the search. The following binary search algorithm ef\ufb01ciently\nsearches for an element in a sorted array in O(logn) time.\n31",
            "Method 1\nThe usual way to implement binary search resembles looking for a word in a\ndictionary. The search maintains an active region in the array, which initially\ncontains all array elements. Then, a number of steps is performed, each of which\nhalves the size of the region.\nAt each step, the search checks the middle element of the active region. If\nthe middle element is the target element, the search terminates. Otherwise, the\nsearch recursively continues to the left or right half of the region, depending on\nthe value of the middle element.\nThe above idea can be implemented as follows:\nint a = 0, b = n-1;\nwhile (a <= b) {\nint k = (a+b)/2;\nif (array[k] == x) {\n// x found at index k\n}\nif (array[k] > x) b = k-1;\nelse a = k+1;\n}\nIn this implementation, the active region is a... b, and initially the region is\n0... n \u22121. The algorithm halves the size of the region at each step, so the time\ncomplexity is O(logn).\nMethod 2\nAn alternative method to implement binary search is based on an ef\ufb01cient way to\niterate through the elements of the array. The idea is to make jumps and slow\nthe speed when we get closer to the target element.\nThe search goes through the array from left to right, and the initial jump\nlength is n/2. At each step, the jump length will be halved: \ufb01rst n/4, then n/8,\nn/16, etc., until \ufb01nally the length is 1. After the jumps, either the target element\nhas been found or we know that it does not appear in the array.\nThe following code implements the above idea:\nint k = 0;\nfor (int b = n/2; b >= 1; b /= 2) {\nwhile (k+b < n && array[k+b] <= x) k += b;\n}\nif (array[k] == x) {\n// x found at index k\n}\nDuring the search, the variable b contains the current jump length. The\ntime complexity of the algorithm is O(logn), because the code in the while loop is\nperformed at most twice for each jump length.\n32",
            "C++ functions\nThe C++ standard library contains the following functions that are based on\nbinary search and work in logarithmic time:\n\u2022 lower_bound returns a pointer to the \ufb01rst array element whose value is at\nleast x.\n\u2022 upper_bound returns a pointer to the \ufb01rst array element whose value is\nlarger than x.\n\u2022 equal_range returns both above pointers.\nThe functions assume that the array is sorted. If there is no such element,\nthe pointer points to the element after the last array element. For example, the\nfollowing code \ufb01nds out whether an array contains an element with value x:\nauto k = lower_bound(array,array+n,x)-array;\nif (k < n && array[k] == x) {\n// x found at index k\n}\nThen, the following code counts the number of elements whose value is x:\nauto a = lower_bound(array, array+n, x);\nauto b = upper_bound(array, array+n, x);\ncout << b-a << \"\\n\";\nUsing equal_range, the code becomes shorter:\nauto r = equal_range(array, array+n, x);\ncout << r.second-r.first << \"\\n\";\nFinding the smallest solution\nAn important use for binary search is to \ufb01nd the position where the value of a\nfunction changes. Suppose that we wish to \ufb01nd the smallest value k that is a\nvalid solution for a problem. We are given a function ok(x) that returns true if x\nis a valid solution and false otherwise. In addition, we know that ok(x) is false\nwhen x < k and true when x \u2265 k. The situation looks as follows:\nx 0 1 \u00b7\u00b7\u00b7 k \u22121 k k +1 \u00b7\u00b7\u00b7\nok(x) false false \u00b7\u00b7\u00b7 false true true \u00b7\u00b7\u00b7\nNow, the value of k can be found using binary search:\nint x = -1;\nfor (int b = z; b >= 1; b /= 2) {\nwhile (!ok(x+b)) x += b;\n}\nint k = x+1;\n33",
            "The search \ufb01nds the largest value of x for which ok(x) is false. Thus, the next\nvalue k = x +1 is the smallest possible value for which ok(k) is true. The initial\njump length z has to be large enough, for example some value for which we know\nbeforehand that ok(z) is true.\nThe algorithm calls the function ok O(log z) times, so the total time complexity\ndepends on the function ok. For example, if the function works in O(n) time, the\ntotal time complexity is O(nlog z).\nFinding the maximum value\nBinary search can also be used to \ufb01nd the maximum value for a function that is\n\ufb01rst increasing and then decreasing. Our task is to \ufb01nd a position k such that\n\u2022 f (x) < f (x +1) when x < k, and\n\u2022 f (x) > f (x +1) when x \u2265 k.\nThe idea is to use binary search for \ufb01nding the largest value of x for which\nf (x) < f (x+1). This implies that k = x+1 because f (x+1) > f (x+2). The following\ncode implements the search:\nint x = -1;\nfor (int b = z; b >= 1; b /= 2) {\nwhile (f(x+b) < f(x+b+1)) x += b;\n}\nint k = x+1;\nNote that unlike in the ordinary binary search, here it is not allowed that\nconsecutive values of the function are equal. In this case it would not be possible\nto know how to continue the search.\n34",
            "Chapter 4\nData structures\nA data structure is a way to store data in the memory of a computer. It is\nimportant to choose an appropriate data structure for a problem, because each\ndata structure has its own advantages and disadvantages. The crucial question\nis: which operations are ef\ufb01cient in the chosen data structure?\nThis chapter introduces the most important data structures in the C++ stan-\ndard library. It is a good idea to use the standard library whenever possible,\nbecause it will save a lot of time. Later in the book we will learn about more\nsophisticated data structures that are not available in the standard library.\n4.1 Dynamic arrays\nA dynamic array is an array whose size can be changed during the execution of\nthe program. The most popular dynamic array in C++ is the vector structure,\nwhich can be used almost like an ordinary array.\nThe following code creates an empty vector and adds three elements to it:\nvector<int> v;\nv.push_back(3); // [3]\nv.push_back(2); // [3,2]\nv.push_back(5); // [3,2,5]\nAfter this, the elements can be accessed like in an ordinary array:\ncout << v[0] << \"\\n\"; // 3\ncout << v[1] << \"\\n\"; // 2\ncout << v[2] << \"\\n\"; // 5\nThe function size returns the number of elements in the vector. The following\ncode iterates through the vector and prints all elements in it:\nfor (int i = 0; i < v.size(); i++) {\ncout << v[i] << \"\\n\";\n}\n35",
            "A shorter way to iterate through a vector is as follows:\nfor (auto x : v) {\ncout << x << \"\\n\";\n}\nThe function back returns the last element in the vector, and the function\npop_back removes the last element:\nvector<int> v;\nv.push_back(5);\nv.push_back(2);\ncout << v.back() << \"\\n\"; // 2\nv.pop_back();\ncout << v.back() << \"\\n\"; // 5\nThe following code creates a vector with \ufb01ve elements:\nvector<int> v = {2,4,2,5,1};\nAnother way to create a vector is to give the number of elements and the\ninitial value for each element:\n// size 10, initial value 0\nvector<int> v(10);\n// size 10, initial value 5\nvector<int> v(10, 5);\nThe internal implementation of a vector uses an ordinary array. If the size of\nthe vector increases and the array becomes too small, a new array is allocated\nand all the elements are moved to the new array. However, this does not happen\noften and the average time complexity of push_back is O(1).\nThe string structure is also a dynamic array that can be used almost like\na vector. In addition, there is special syntax for strings that is not available in\nother data structures. Strings can be combined using the + symbol. The function\nsubstr(k, x) returns the substring that begins at position k and has length x, and\nthe function find(t) \ufb01nds the position of the \ufb01rst occurrence of a substring t.\nThe following code presents some string operations:\nstring a = \"hatti\";\nstring b = a+a;\ncout << b << \"\\n\"; // hattihatti\nb[5] = \u2019v\u2019;\ncout << b << \"\\n\"; // hattivatti\nstring c = b.substr(3,4);\ncout << c << \"\\n\"; // tiva\n36",
            "4.2 Set structures\nA set is a data structure that maintains a collection of elements. The basic\noperations of sets are element insertion, search and removal.\nThe C++ standard library contains two set implementations: The structure\nset is based on a balanced binary tree and its operations work in O(logn) time.\nThe structure unordered_set uses hashing, and its operations work in O(1) time\non average.\nThe choice of which set implementation to use is often a matter of taste. The\nbene\ufb01t of the set structure is that it maintains the order of the elements and\nprovides functions that are not available in unordered_set. On the other hand,\nunordered_set can be more ef\ufb01cient.\nThe following code creates a set that contains integers, and shows some of the\noperations. The function insert adds an element to the set, the function count\nreturns the number of occurrences of an element in the set, and the function\nerase removes an element from the set.\nset<int> s;\ns.insert(3);\ns.insert(2);\ns.insert(5);\ncout << s.count(3) << \"\\n\"; // 1\ncout << s.count(4) << \"\\n\"; // 0\ns.erase(3);\ns.insert(4);\ncout << s.count(3) << \"\\n\"; // 0\ncout << s.count(4) << \"\\n\"; // 1\nA set can be used mostly like a vector, but it is not possible to access the\nelements using the [] notation. The following code creates a set, prints the\nnumber of elements in it, and then iterates through all the elements:\nset<int> s = {2,5,6,8};\ncout << s.size() << \"\\n\"; // 4\nfor (auto x : s) {\ncout << x << \"\\n\";\n}\nAn important property of sets is that all their elements are distinct. Thus,\nthe function count always returns either 0 (the element is not in the set) or 1 (the\nelement is in the set), and the function insert never adds an element to the set if\nit is already there. The following code illustrates this:\nset<int> s;\ns.insert(5);\ns.insert(5);\ns.insert(5);\ncout << s.count(5) << \"\\n\"; // 1\n37",
            "C++ also contains the structuresmultiset and unordered_multiset that other-\nwise work like set and unordered_set but they can contain multiple instances of\nan element. For example, in the following code all three instances of the number\n5 are added to a multiset:\nmultiset<int> s;\ns.insert(5);\ns.insert(5);\ns.insert(5);\ncout << s.count(5) << \"\\n\"; // 3\nThe function erase removes all instances of an element from a multiset:\ns.erase(5);\ncout << s.count(5) << \"\\n\"; // 0\nOften, only one instance should be removed, which can be done as follows:\ns.erase(s.find(5));\ncout << s.count(5) << \"\\n\"; // 2\n4.3 Map structures\nA map is a generalized array that consists of key-value-pairs. While the keys in\nan ordinary array are always the consecutive integers 0,1,..., n \u22121, where n is\nthe size of the array, the keys in a map can be of any data type and they do not\nhave to be consecutive values.\nThe C++ standard library contains two map implementations that correspond\nto the set implementations: the structure map is based on a balanced binary tree\nand accessing elements takes O(logn) time, while the structure unordered_map\nuses hashing and accessing elements takes O(1) time on average.\nThe following code creates a map where the keys are strings and the values\nare integers:\nmap<string,int> m;\nm[\"monkey\"] = 4;\nm[\"banana\"] = 3;\nm[\"harpsichord\"] = 9;\ncout << m[\"banana\"] << \"\\n\"; // 3\nIf the value of a key is requested but the map does not contain it, the key\nis automatically added to the map with a default value. For example, in the\nfollowing code, the key \u201daybabtu\u201d with value 0 is added to the map.\nmap<string,int> m;\ncout << m[\"aybabtu\"] << \"\\n\"; // 0\n38",
            "The function count checks if a key exists in a map:\nif (m.count(\"aybabtu\")) {\n// key exists\n}\nThe following code prints all the keys and values in a map:\nfor (auto x : m) {\ncout << x.first << \" \" << x.second << \"\\n\";\n}\n4.4 Iterators and ranges\nMany functions in the C++ standard library operate with iterators. An iterator\nis a variable that points to an element in a data structure.\nThe often used iterators begin and end de\ufb01ne a range that contains all ele-\nments in a data structure. The iterator begin points to the \ufb01rst element in the\ndata structure, and the iterator end points to the position after the last element.\nThe situation looks as follows:\n{ 3, 4, 6, 8, 12, 13, 14, 17 }\n\u2191 \u2191\ns.begin() s.end()\nNote the asymmetry in the iterators: s.begin() points to an element in the\ndata structure, while s.end() points outside the data structure. Thus, the range\nde\ufb01ned by the iterators is half-open.\nWorking with ranges\nIterators are used in C++ standard library functions that are given a range of\nelements in a data structure. Usually, we want to process all elements in a data\nstructure, so the iterators begin and end are given for the function.\nFor example, the following code sorts a vector using the function sort, then\nreverses the order of the elements using the functionreverse, and \ufb01nally shuf\ufb02es\nthe order of the elements using the function random_shuffle.\nsort(v.begin(), v.end());\nreverse(v.begin(), v.end());\nrandom_shuffle(v.begin(), v.end());\nThese functions can also be used with an ordinary array. In this case, the\nfunctions are given pointers to the array instead of iterators:\n39",
            "sort(a, a+n);\nreverse(a, a+n);\nrandom_shuffle(a, a+n);\nSet iterators\nIterators are often used to access elements of a set. The following code creates an\niterator it that points to the smallest element in a set:\nset<int>::iterator it = s.begin();\nA shorter way to write the code is as follows:\nauto it = s.begin();\nThe element to which an iterator points can be accessed using the * symbol. For\nexample, the following code prints the \ufb01rst element in the set:\nauto it = s.begin();\ncout << *it << \"\\n\";\nIterators can be moved using the operators ++ (forward) and -- (backward),\nmeaning that the iterator moves to the next or previous element in the set.\nThe following code prints all the elements in increasing order:\nfor (auto it = s.begin(); it != s.end(); it++) {\ncout << *it << \"\\n\";\n}\nThe following code prints the largest element in the set:\nauto it = s.end(); it--;\ncout << *it << \"\\n\";\nThe function find(x) returns an iterator that points to an element whose\nvalue is x. However, if the set does not contain x, the iterator will be end.\nauto it = s.find(x);\nif (it == s.end()) {\n// x is not found\n}\nThe function lower_bound(x) returns an iterator to the smallest element in the\nset whose value is at least x, and the function upper_bound(x) returns an iterator\nto the smallest element in the set whose value is larger than x. In both functions,\nif such an element does not exist, the return value is end. These functions are\nnot supported by the unordered_set structure which does not maintain the order\nof the elements.\n40",
            "For example, the following code \ufb01nds the element nearest to x:\nauto it = s.lower_bound(x);\nif (it == s.begin()) {\ncout << *it << \"\\n\";\n} else if (it == s.end()) {\nit--;\ncout << *it << \"\\n\";\n} else {\nint a = *it; it--;\nint b = *it;\nif (x-b < a-x) cout << b << \"\\n\";\nelse cout << a << \"\\n\";\n}\nThe code assumes that the set is not empty, and goes through all possible\ncases using an iterator it. First, the iterator points to the smallest element\nwhose value is at least x. If it equals begin, the corresponding element is nearest\nto x. If it equals end, the largest element in the set is nearest to x. If none\nof the previous cases hold, the element nearest to x is either the element that\ncorresponds to it or the previous element.\n4.5 Other structures\nBitset\nA bitset is an array whose each value is either 0 or 1. For example, the following\ncode creates a bitset that contains 10 elements:\nbitset<10> s;\ns[1] = 1;\ns[3] = 1;\ns[4] = 1;\ns[7] = 1;\ncout << s[4] << \"\\n\"; // 1\ncout << s[5] << \"\\n\"; // 0\nThe bene\ufb01t of using bitsets is that they require less memory than ordinary\narrays, because each element in a bitset only uses one bit of memory. For\nexample, if n bits are stored in an int array, 32n bits of memory will be used, but\na corresponding bitset only requires n bits of memory. In addition, the values of a\nbitset can be ef\ufb01ciently manipulated using bit operators, which makes it possible\nto optimize algorithms using bit sets.\nThe following code shows another way to create the above bitset:\nbitset<10> s(string(\"0010011010\")); // from right to left\ncout << s[4] << \"\\n\"; // 1\ncout << s[5] << \"\\n\"; // 0\n41",
            "The function count returns the number of ones in the bitset:\nbitset<10> s(string(\"0010011010\"));\ncout << s.count() << \"\\n\"; // 4\nThe following code shows examples of using bit operations:\nbitset<10> a(string(\"0010110110\"));\nbitset<10> b(string(\"1011011000\"));\ncout << (a&b) << \"\\n\"; // 0010010000\ncout << (a|b) << \"\\n\"; // 1011111110\ncout << (a^b) << \"\\n\"; // 1001101110\nDeque\nA deque is a dynamic array whose size can be ef\ufb01ciently changed at both ends of\nthe array. Like a vector, a deque provides the functionspush_back and pop_back,\nbut it also includes the functions push_front and pop_front which are not avail-\nable in a vector.\nA deque can be used as follows:\ndeque<int> d;\nd.push_back(5); // [5]\nd.push_back(2); // [5,2]\nd.push_front(3); // [3,5,2]\nd.pop_back(); // [3,5]\nd.pop_front(); // [5]\nThe internal implementation of a deque is more complex than that of a vector,\nand for this reason, a deque is slower than a vector. Still, both adding and\nremoving elements take O(1) time on average at both ends.\nStack\nA stack is a data structure that provides two O(1) time operations: adding an\nelement to the top, and removing an element from the top. It is only possible to\naccess the top element of a stack.\nThe following code shows how a stack can be used:\nstack<int> s;\ns.push(3);\ns.push(2);\ns.push(5);\ncout << s.top(); // 5\ns.pop();\ncout << s.top(); // 2\n42",
            "Queue\nA queue also provides two O(1) time operations: adding an element to the end\nof the queue, and removing the \ufb01rst element in the queue. It is only possible to\naccess the \ufb01rst and last element of a queue.\nThe following code shows how a queue can be used:\nqueue<int> q;\nq.push(3);\nq.push(2);\nq.push(5);\ncout << q.front(); // 3\nq.pop();\ncout << q.front(); // 2\nPriority queue\nA priority queue maintains a set of elements. The supported operations are\ninsertion and, depending on the type of the queue, retrieval and removal of either\nthe minimum or maximum element. Insertion and removal take O(logn) time,\nand retrieval takes O(1) time.\nWhile an ordered set ef\ufb01ciently supports all the operations of a priority queue,\nthe bene\ufb01t of using a priority queue is that it has smaller constant factors. A\npriority queue is usually implemented using a heap structure that is much\nsimpler than a balanced binary tree used in an ordered set.\nBy default, the elements in a C++ priority queue are sorted in decreasing\norder, and it is possible to \ufb01nd and remove the largest element in the queue. The\nfollowing code illustrates this:\npriority_queue<int> q;\nq.push(3);\nq.push(5);\nq.push(7);\nq.push(2);\ncout << q.top() << \"\\n\"; // 7\nq.pop();\ncout << q.top() << \"\\n\"; // 5\nq.pop();\nq.push(6);\ncout << q.top() << \"\\n\"; // 6\nq.pop();\nIf we want to create a priority queue that supports \ufb01nding and removing the\nsmallest element, we can do it as follows:\npriority_queue<int,vector<int>,greater<int>> q;\n43",
            "Policy-based data structures\nThe g++ compiler also supports some data structures that are not part of the C++\nstandard library. Such structures are called policy-based data structures. To use\nthese structures, the following lines must be added to the code:\n#include <ext/pb_ds/assoc_container.hpp>\nusing namespace __gnu_pbds;\nAfter this, we can de\ufb01ne a data structure indexed_set that is like set but can be\nindexed like an array. The de\ufb01nition for int values is as follows:\ntypedef tree<int,null_type,less<int>,rb_tree_tag,\ntree_order_statistics_node_update> indexed_set;\nNow we can create a set as follows:\nindexed_set s;\ns.insert(2);\ns.insert(3);\ns.insert(7);\ns.insert(9);\nThe speciality of this set is that we have access to the indices that the elements\nwould have in a sorted array. The function find_by_order returns an iterator to\nthe element at a given position:\nauto x = s.find_by_order(2);\ncout << *x << \"\\n\"; // 7\nAnd the function order_of_key returns the position of a given element:\ncout << s.order_of_key(7) << \"\\n\"; // 2\nIf the element does not appear in the set, we get the position that the element\nwould have in the set:\ncout << s.order_of_key(6) << \"\\n\"; // 2\ncout << s.order_of_key(8) << \"\\n\"; // 3\nBoth the functions work in logarithmic time.\n4.6 Comparison to sorting\nIt is often possible to solve a problem using either data structures or sorting.\nSometimes there are remarkable differences in the actual ef\ufb01ciency of these\napproaches, which may be hidden in their time complexities.\nLet us consider a problem where we are given two lists A and B that both\ncontain n elements. Our task is to calculate the number of elements that belong\n44",
            "to both of the lists. For example, for the lists\nA = [5,2,8,9] and B = [3,2,9,5],\nthe answer is 3 because the numbers 2, 5 and 9 belong to both of the lists.\nA straightforward solution to the problem is to go through all pairs of elements\nin O(n2) time, but next we will focus on more ef\ufb01cient algorithms.\nAlgorithm 1\nWe construct a set of the elements that appear in A, and after this, we iterate\nthrough the elements of B and check for each elements if it also belongs to A.\nThis is ef\ufb01cient because the elements of A are in a set. Using the set structure,\nthe time complexity of the algorithm is O(nlogn).\nAlgorithm 2\nIt is not necessary to maintain an ordered set, so instead of the set structure\nwe can also use the unordered_set structure. This is an easy way to make the\nalgorithm more ef\ufb01cient, because we only have to change the underlying data\nstructure. The time complexity of the new algorithm is O(n).\nAlgorithm 3\nInstead of data structures, we can use sorting. First, we sort both lists A and\nB. After this, we iterate through both the lists at the same time and \ufb01nd the\ncommon elements. The time complexity of sorting is O(nlogn), and the rest of\nthe algorithm works in O(n) time, so the total time complexity is O(nlogn).\nE\ufb03ciency comparison\nThe following table shows how ef\ufb01cient the above algorithms are when n varies\nand the elements of the lists are random integers between 1... 109:\nn Algorithm 1 Algorithm 2 Algorithm 3\n106 1.5 s 0 .3 s 0 .2 s\n2\u00b7106 3.7 s 0 .8 s 0 .3 s\n3\u00b7106 5.7 s 1 .3 s 0 .5 s\n4\u00b7106 7.7 s 1 .7 s 0 .7 s\n5\u00b7106 10.0 s 2 .3 s 0 .9 s\nAlgorithms 1 and 2 are equal except that they use different set structures. In\nthis problem, this choice has an important effect on the running time, because\nAlgorithm 2 is 4\u20135 times faster than Algorithm 1.\nHowever, the most ef\ufb01cient algorithm is Algorithm 3 which uses sorting.\nIt only uses half the time compared to Algorithm 2. Interestingly, the time\ncomplexity of both Algorithm 1 and Algorithm 3 is O(nlogn), but despite this,\nAlgorithm 3 is ten times faster. This can be explained by the fact that sorting is a\n45",
            "simple procedure and it is done only once at the beginning of Algorithm 3, and\nthe rest of the algorithm works in linear time. On the other hand, Algorithm 1\nmaintains a complex balanced binary tree during the whole algorithm.\n46",
            "Chapter 5\nComplete search\nComplete search is a general method that can be used to solve almost any\nalgorithm problem. The idea is to generate all possible solutions to the problem\nusing brute force, and then select the best solution or count the number of\nsolutions, depending on the problem.\nComplete search is a good technique if there is enough time to go through\nall the solutions, because the search is usually easy to implement and it always\ngives the correct answer. If complete search is too slow, other techniques, such as\ngreedy algorithms or dynamic programming, may be needed.\n5.1 Generating subsets\nWe \ufb01rst consider the problem of generating all subsets of a set of n elements. For\nexample, the subsets of {0,1,2} are \u2208, {0}, {1}, {2}, {0,1}, {0,2}, {1,2} and {0,1,2}.\nThere are two common methods to generate subsets: we can either perform a\nrecursive search or exploit the bit representation of integers.\nMethod 1\nAn elegant way to go through all subsets of a set is to use recursion. The\nfollowing function search generates the subsets of the set {0,1,..., n \u22121}. The\nfunction maintains a vector subset that will contain the elements of each subset.\nThe search begins when the function is called with parameter 0.\nvoid search(int k) {\nif (k == n) {\n// process subset\n} else {\nsearch(k+1);\nsubset.push_back(k);\nsearch(k+1);\nsubset.pop_back();\n}\n}\n47",
            "When the function search is called with parameter k, it decides whether to\ninclude the element k in the subset or not, and in both cases, then calls itself\nwith parameter k +1 However, if k = n, the function notices that all elements\nhave been processed and a subset has been generated.\nThe following tree illustrates the function calls when n = 3. We can always\nchoose either the left branch (k is not included in the subset) or the right branch\n(k is included in the subset).\nsearch(0)\nsearch(1) search(1)\nsearch(2) search(2) search(2) search(2)\nsearch(3) search(3) search(3) search(3) search(3) search(3) search(3) search(3)\n\u2208 {2} {1} {1,2} {0} {0,2} {0,1} {0,1,2}\nMethod 2\nAnother way to generate subsets is based on the bit representation of integers.\nEach subset of a set of n elements can be represented as a sequence of n bits,\nwhich corresponds to an integer between 0... 2n \u22121. The ones in the bit sequence\nindicate which elements are included in the subset.\nThe usual convention is that the last bit corresponds to element 0, the second\nlast bit corresponds to element 1, and so on. For example, the bit representation\nof 25 is 11001, which corresponds to the subset {0,3,4}.\nThe following code goes through the subsets of a set of n elements\nfor (int b = 0; b < (1<<n); b++) {\n// process subset\n}\nThe following code shows how we can \ufb01nd the elements of a subset that\ncorresponds to a bit sequence. When processing each subset, the code builds a\nvector that contains the elements in the subset.\nfor (int b = 0; b < (1<<n); b++) {\nvector<int> subset;\nfor (int i = 0; i < n; i++) {\nif (b&(1<<i)) subset.push_back(i);\n}\n}\n48",
            "5.2 Generating permutations\nNext we consider the problem of generating all permutations of a set ofn elements.\nFor example, the permutations of {0,1,2} are (0,1,2), (0,2,1), (1,0,2), (1,2,0),\n(2,0,1) and (2,1,0). Again, there are two approaches: we can either use recursion\nor go through the permutations iteratively.\nMethod 1\nLike subsets, permutations can be generated using recursion. The following\nfunction search goes through the permutations of the set {0,1,..., n \u22121}. The\nfunction builds a vector permutation that contains the permutation, and the\nsearch begins when the function is called without parameters.\nvoid search() {\nif (permutation.size() == n) {\n// process permutation\n} else {\nfor (int i = 0; i < n; i++) {\nif (chosen[i]) continue;\nchosen[i] = true;\npermutation.push_back(i);\nsearch();\nchosen[i] = false;\npermutation.pop_back();\n}\n}\n}\nEach function call adds a new element to permutation. The array chosen\nindicates which elements are already included in the permutation. If the size of\npermutation equals the size of the set, a permutation has been generated.\nMethod 2\nAnother method for generating permutations is to begin with the permutation\n{0,1,..., n \u22121} and repeatedly use a function that constructs the next permu-\ntation in increasing order. The C++ standard library contains the function\nnext_permutation that can be used for this:\nvector<int> permutation;\nfor (int i = 0; i < n; i++) {\npermutation.push_back(i);\n}\ndo {\n// process permutation\n} while (next_permutation(permutation.begin(),permutation.end()));\n49",
            "5.3 Backtracking\nA backtracking algorithm begins with an empty solution and extends the\nsolution step by step. The search recursively goes through all different ways how\na solution can be constructed.\nAs an example, consider the problem of calculating the number of ways n\nqueens can be placed on an n \u00d7n chessboard so that no two queens attack each\nother. For example, when n = 4, there are two possible solutions:\nQ\nQ\nQ\nQ\nQ\nQ\nQ\nQ\nThe problem can be solved using backtracking by placing queens to the board\nrow by row. More precisely, exactly one queen will be placed on each row so that\nno queen attacks any of the queens placed before. A solution has been found\nwhen all n queens have been placed on the board.\nFor example, when n = 4, some partial solutions generated by the backtrack-\ning algorithm are as follows:\nQ Q Q Q\nQ Q Q Q\nQ Q Q Q\nillegal illegal illegal valid\nAt the bottom level, the three \ufb01rst con\ufb01gurations are illegal, because the\nqueens attack each other. However, the fourth con\ufb01guration is valid and it can be\nextended to a complete solution by placing two more queens to the board. There\nis only one way to place the two remaining queens.\nThe algorithm can be implemented as follows:\n50",
            "void search(int y) {\nif (y == n) {\ncount++;\nreturn;\n}\nfor (int x = 0; x < n; x++) {\nif (column[x] || diag1[x+y] || diag2[x-y+n-1]) continue;\ncolumn[x] = diag1[x+y] = diag2[x-y+n-1] = 1;\nsearch(y+1);\ncolumn[x] = diag1[x+y] = diag2[x-y+n-1] = 0;\n}\n}\nThe search begins by calling search(0). The size of the board is n \u00d7n, and the\ncode calculates the number of solutions to count.\nThe code assumes that the rows and columns of the board are numbered from\n0 to n\u22121. When the function search is called with parameter y, it places a queen\non row y and then calls itself with parameter y+1. Then, if y = n, a solution has\nbeen found and the variable count is increased by one.\nThe array column keeps track of columns that contain a queen, and the arrays\ndiag1 and diag2 keep track of diagonals. It is not allowed to add another queen\nto a column or diagonal that already contains a queen. For example, the columns\nand diagonals of the 4\u00d74 board are numbered as follows:\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0 1 2 3\n1 2 3 4\n2 3 4 5\n3 4 5 6\n3 4 5 6\n2 3 4 5\n1 2 3 4\n0 1 2 3\ncolumn diag1 diag2\nLet q(n) denote the number of ways to place n queens on an n \u00d7n chessboard.\nThe above backtracking algorithm tells us that, for example, q(8) = 92. When\nn increases, the search quickly becomes slow, because the number of solutions\nincreases exponentially. For example, calculating q(16) = 14772512 using the\nabove algorithm already takes about a minute on a modern computer1.\n5.4 Pruning the search\nWe can often optimize backtracking by pruning the search tree. The idea is to\nadd \u201dintelligence\u201d to the algorithm so that it will notice as soon as possible if a\npartial solution cannot be extended to a complete solution. Such optimizations\ncan have a tremendous effect on the ef\ufb01ciency of the search.\n1There is no known way to ef\ufb01ciently calculate larger values of q(n). The current record is\nq(27) = 234907967154122528, calculated in 2016 [55].\n51",
            "Let us consider the problem of calculating the number of paths in an n \u00d7n\ngrid from the upper-left corner to the lower-right corner such that the path visits\neach square exactly once. For example, in a 7 \u00d77 grid, there are 111712 such\npaths. One of the paths is as follows:\nWe focus on the 7 \u00d77 case, because its level of dif\ufb01culty is appropriate to\nour needs. We begin with a straightforward backtracking algorithm, and then\noptimize it step by step using observations of how the search can be pruned.\nAfter each optimization, we measure the running time of the algorithm and the\nnumber of recursive calls, so that we clearly see the effect of each optimization\non the ef\ufb01ciency of the search.\nBasic algorithm\nThe \ufb01rst version of the algorithm does not contain any optimizations. We simply\nuse backtracking to generate all possible paths from the upper-left corner to the\nlower-right corner and count the number of such paths.\n\u2022 running time: 483 seconds\n\u2022 number of recursive calls: 76 billion\nOptimization 1\nIn any solution, we \ufb01rst move one step down or right. There are always two\npaths that are symmetric about the diagonal of the grid after the \ufb01rst step. For\nexample, the following paths are symmetric:\nHence, we can decide that we always \ufb01rst move one step down (or right), and\n\ufb01nally multiply the number of solutions by two.\n\u2022 running time: 244 seconds\n\u2022 number of recursive calls: 38 billion\n52",
            "Optimization 2\nIf the path reaches the lower-right square before it has visited all other squares\nof the grid, it is clear that it will not be possible to complete the solution. An\nexample of this is the following path:\nUsing this observation, we can terminate the search immediately if we reach the\nlower-right square too early.\n\u2022 running time: 119 seconds\n\u2022 number of recursive calls: 20 billion\nOptimization 3\nIf the path touches a wall and can turn either left or right, the grid splits into\ntwo parts that contain unvisited squares. For example, in the following situation,\nthe path can turn either left or right:\nIn this case, we cannot visit all squares anymore, so we can terminate the search.\nThis optimization is very useful:\n\u2022 running time: 1.8 seconds\n\u2022 number of recursive calls: 221 million\nOptimization 4\nThe idea of Optimization 3 can be generalized: if the path cannot continue\nforward but can turn either left or right, the grid splits into two parts that both\ncontain unvisited squares. For example, consider the following path:\n53",
            "It is clear that we cannot visit all squares anymore, so we can terminate the\nsearch. After this optimization, the search is very ef\ufb01cient:\n\u2022 running time: 0.6 seconds\n\u2022 number of recursive calls: 69 million\nNow is a good moment to stop optimizing the algorithm and see what we have\nachieved. The running time of the original algorithm was 483 seconds, and now\nafter the optimizations, the running time is only 0.6 seconds. Thus, the algorithm\nbecame nearly 1000 times faster after the optimizations.\nThis is a usual phenomenon in backtracking, because the search tree is usually\nlarge and even simple observations can effectively prune the search. Especially\nuseful are optimizations that occur during the \ufb01rst steps of the algorithm, i.e., at\nthe top of the search tree.\n5.5 Meet in the middle\nMeet in the middle is a technique where the search space is divided into two\nparts of about equal size. A separate search is performed for both of the parts,\nand \ufb01nally the results of the searches are combined.\nThe technique can be used if there is an ef\ufb01cient way to combine the results\nof the searches. In such a situation, the two searches may require less time than\none large search. Typically, we can turn a factor of 2n into a factor of 2n/2 using\nthe meet in the middle technique.\nAs an example, consider a problem where we are given a list of n numbers\nand a number x, and we want to \ufb01nd out if it is possible to choose some numbers\nfrom the list so that their sum is x. For example, given the list [2 ,4,5,9] and\nx = 15, we can choose the numbers [2,4,9] to get 2 +4+9 = 15. However, if x = 10\nfor the same list, it is not possible to form the sum.\nA simple algorithm to the problem is to go through all subsets of the elements\nand check if the sum of any of the subsets is x. The running time of such an\nalgorithm is O(2n), because there are 2n subsets. However, using the meet in the\nmiddle technique, we can achieve a more ef\ufb01cient O(2n/2) time algorithm2. Note\nthat O(2n) and O(2n/2) are different complexities because 2n/2 equals\n\u2282\n2n.\n2This idea was introduced in 1974 by E. Horowitz and S. Sahni [39].\n54",
            "The idea is to divide the list into two lists A and B such that both lists contain\nabout half of the numbers. The \ufb01rst search generates all subsets of A and stores\ntheir sums to a list SA. Correspondingly, the second search creates a list SB from\nB. After this, it suf\ufb01ces to check if it is possible to choose one element from SA\nand another element from SB such that their sum is x. This is possible exactly\nwhen there is a way to form the sum x using the numbers of the original list.\nFor example, suppose that the list is [2 ,4,5,9] and x = 15. First, we divide\nthe list into A = [2,4] and B = [5,9]. After this, we create lists SA = [0,2,4,6]\nand SB = [0,5,9,14]. In this case, the sum x = 15 is possible to form, because SA\ncontains the sum 6, SB contains the sum 9, and 6+9 = 15. This corresponds to\nthe solution [2,4,9].\nWe can implement the algorithm so that its time complexity is O(2n/2). First,\nwe generate sorted lists SA and SB, which can be done in O(2n/2) time using a\nmerge-like technique. After this, since the lists are sorted, we can check inO(2n/2)\ntime if the sum x can be created from SA and SB.\n55",
            "56",
            "Chapter 6\nGreedy algorithms\nA greedy algorithm constructs a solution to the problem by always making a\nchoice that looks the best at the moment. A greedy algorithm never takes back\nits choices, but directly constructs the \ufb01nal solution. For this reason, greedy\nalgorithms are usually very ef\ufb01cient.\nThe dif\ufb01culty in designing greedy algorithms is to \ufb01nd a greedy strategy that\nalways produces an optimal solution to the problem. The locally optimal choices\nin a greedy algorithm should also be globally optimal. It is often dif\ufb01cult to argue\nthat a greedy algorithm works.\n6.1 Coin problem\nAs a \ufb01rst example, we consider a problem where we are given a set of coins and\nour task is to form a sum of money n using the coins. The values of the coins are\ncoins = {c1, c2,..., ck}, and each coin can be used as many times we want. What\nis the minimum number of coins needed?\nFor example, if the coins are the euro coins (in cents)\n{1,2,5,10,20,50,100,200}\nand n = 520, we need at least four coins. The optimal solution is to select coins\n200+200+100+20 whose sum is 520.\nGreedy algorithm\nA simple greedy algorithm to the problem always selects the largest possible coin,\nuntil the required sum of money has been constructed. This algorithm works in\nthe example case, because we \ufb01rst select two 200 cent coins, then one 100 cent\ncoin and \ufb01nally one 20 cent coin. But does this algorithm always work?\nIt turns out that if the coins are the euro coins, the greedy algorithm always\nworks, i.e., it always produces a solution with the fewest possible number of coins.\nThe correctness of the algorithm can be shown as follows:\nFirst, each coin 1, 5, 10, 50 and 100 appears at most once in an optimal\nsolution, because if the solution would contain two such coins, we could replace\n57",
            "them by one coin and obtain a better solution. For example, if the solution would\ncontain coins 5+5, we could replace them by coin 10.\nIn the same way, coins 2 and 20 appear at most twice in an optimal solution,\nbecause we could replace coins 2 +2+2 by coins 5 +1 and coins 20 +20+20 by\ncoins 50 +10. Moreover, an optimal solution cannot contain coins 2 +2 +1 or\n20+20+10, because we could replace them by coins 5 and 50.\nUsing these observations, we can show for each coin x that it is not possible\nto optimally construct a sum x or any larger sum by only using coins that are\nsmaller than x. For example, if x = 100, the largest optimal sum using the smaller\ncoins is 50+20+20+5+2+2 = 99. Thus, the greedy algorithm that always selects\nthe largest coin produces the optimal solution.\nThis example shows that it can be dif\ufb01cult to argue that a greedy algorithm\nworks, even if the algorithm itself is simple.\nGeneral case\nIn the general case, the coin set can contain any coins and the greedy algorithm\ndoes not necessarily produce an optimal solution.\nWe can prove that a greedy algorithm does not work by showing a counterex-\nample where the algorithm gives a wrong answer. In this problem we can easily\n\ufb01nd a counterexample: if the coins are {1,3,4} and the target sum is 6, the greedy\nalgorithm produces the solution 4+1+1 while the optimal solution is 3+3.\nIt is not known if the general coin problem can be solved using any greedy\nalgorithm1. However, as we will see in Chapter 7, in some cases, the general\nproblem can be ef\ufb01ciently solved using a dynamic programming algorithm that\nalways gives the correct answer.\n6.2 Scheduling\nMany scheduling problems can be solved using greedy algorithms. A classic\nproblem is as follows: Given n events with their starting and ending times, \ufb01nd a\nschedule that includes as many events as possible. It is not possible to select an\nevent partially. For example, consider the following events:\nevent starting time ending time\nA 1 3\nB 2 5\nC 3 9\nD 6 8\nIn this case the maximum number of events is two. For example, we can select\nevents B and D as follows:\n1However, it is possible to check in polynomial time if the greedy algorithm presented in this\nchapter works for a given set of coins [53].\n58",
            "A\nB\nC\nD\nIt is possible to invent several greedy algorithms for the problem, but which\nof them works in every case?\nAlgorithm 1\nThe \ufb01rst idea is to select as short events as possible. In the example case this\nalgorithm selects the following events:\nA\nB\nC\nD\nHowever, selecting short events is not always a correct strategy. For example,\nthe algorithm fails in the following case:\nIf we select the short event, we can only select one event. However, it would be\npossible to select both long events.\nAlgorithm 2\nAnother idea is to always select the next possible event that begins as early as\npossible. This algorithm selects the following events:\nA\nB\nC\nD\nHowever, we can \ufb01nd a counterexample also for this algorithm. For example,\nin the following case, the algorithm only selects one event:\nIf we select the \ufb01rst event, it is not possible to select any other events. However,\nit would be possible to select the other two events.\n59",
            "Algorithm 3\nThe third idea is to always select the next possible event that ends as early as\npossible. This algorithm selects the following events:\nA\nB\nC\nD\nIt turns out that this algorithm always produces an optimal solution. The\nreason for this is that it is always an optimal choice to \ufb01rst select an event that\nends as early as possible. After this, it is an optimal choice to select the next\nevent using the same strategy, etc., until we cannot select any more events.\nOne way to argue that the algorithm works is to consider what happens if we\n\ufb01rst select an event that ends later than the event that ends as early as possible.\nNow, we will have at most an equal number of choices how we can select the next\nevent. Hence, selecting an event that ends later can never yield a better solution,\nand the greedy algorithm is correct.\n6.3 Tasks and deadlines\nLet us now consider a problem where we are given n tasks with durations and\ndeadlines and our task is to choose an order to perform the tasks. For each task,\nwe earn d \u2212x points where d is the task\u2019s deadline andx is the moment when we\n\ufb01nish the task. What is the largest possible total score we can obtain?\nFor example, suppose that the tasks are as follows:\ntask duration deadline\nA 4 2\nB 3 5\nC 2 7\nD 4 5\nIn this case, an optimal schedule for the tasks is as follows:\nC B A D\n0 5 10\nIn this solution, C yields 5 points, B yields 0 points, A yields \u22127 points and D\nyields \u22128 points, so the total score is \u221210.\nSurprisingly, the optimal solution to the problem does not depend on the\ndeadlines at all, but a correct greedy strategy is to simply perform the tasks\nsorted by their durations in increasing order. The reason for this is that if we\never perform two tasks one after another such that the \ufb01rst task takes longer\nthan the second task, we can obtain a better solution if we swap the tasks. For\nexample, consider the following schedule:\n60",
            "X Y\na b\nHere a > b, so we should swap the tasks:\nY X\nb a\nNow X gives b points less and Y gives a points more, so the total score increases\nby a \u2212b > 0. In an optimal solution, for any two consecutive tasks, it must hold\nthat the shorter task comes before the longer task. Thus, the tasks must be\nperformed sorted by their durations.\n6.4 Minimizing sums\nWe next consider a problem where we are given n numbers a1,a2,..., an and our\ntask is to \ufb01nd a value x that minimizes the sum\n|a1 \u2212 x|c +|a2 \u2212 x|c +\u00b7\u00b7\u00b7+| an \u2212 x|c.\nWe focus on the cases c = 1 and c = 2.\nCase c = 1\nIn this case, we should minimize the sum\n|a1 \u2212 x|+| a2 \u2212 x|+\u00b7\u00b7\u00b7+| an \u2212 x|.\nFor example, if the numbers are [1,2,9,2,6], the best solution is to select x = 2\nwhich produces the sum\n|1\u22122|+| 2\u22122|+| 9\u22122|+| 2\u22122|+| 6\u22122| =12.\nIn the general case, the best choice for x is the median of the numbers, i.e., the\nmiddle number after sorting. For example, the list [1,2,9,2,6] becomes [1,2,2,6,9]\nafter sorting, so the median is 2.\nThe median is an optimal choice, because if x is smaller than the median, the\nsum becomes smaller by increasing x, and if x is larger then the median, the\nsum becomes smaller by decreasing x. Hence, the optimal solution is that x is\nthe median. If n is even and there are two medians, both medians and all values\nbetween them are optimal choices.\nCase c = 2\nIn this case, we should minimize the sum\n(a1 \u2212 x)2 +(a2 \u2212 x)2 +\u00b7\u00b7\u00b7+ (an \u2212 x)2.\n61",
            "For example, if the numbers are [1,2,9,2,6], the best solution is to select x = 4\nwhich produces the sum\n(1\u22124)2 +(2\u22124)2 +(9\u22124)2 +(2\u22124)2 +(6\u22124)2 = 46.\nIn the general case, the best choice for x is the average of the numbers. In the\nexample the average is (1 +2 +9 +2 +6)/5 = 4. This result can be derived by\npresenting the sum as follows:\nnx2 \u22122x(a1 +a2 +\u00b7\u00b7\u00b7+ an)+(a2\n1 +a2\n2 +\u00b7\u00b7\u00b7+ a2\nn)\nThe last part does not depend on x, so we can ignore it. The remaining parts\nform a function nx2 \u22122xs where s = a1 +a2 +\u00b7\u00b7\u00b7+ an. This is a parabola opening\nupwards with roots x = 0 and x = 2s/n, and the minimum value is the average of\nthe roots x = s/n, i.e., the average of the numbers a1,a2,..., an.\n6.5 Data compression\nA binary code assigns for each character of a string a codeword that consists\nof bits. We can compress the string using the binary code by replacing each\ncharacter by the corresponding codeword. For example, the following binary code\nassigns codewords for characters A\u2013D:\ncharacter codeword\nA 00\nB 01\nC 10\nD 11\nThis is a constant-length code which means that the length of each codeword is\nthe same. For example, we can compress the string AABACDACA as follows:\n000001001011001000\nUsing this code, the length of the compressed string is 18 bits. However, we can\ncompress the string better if we use a variable-length code where codewords\nmay have different lengths. Then we can give short codewords for characters\nthat appear often and long codewords for characters that appear rarely. It turns\nout that an optimal code for the above string is as follows:\ncharacter codeword\nA 0\nB 110\nC 10\nD 111\nAn optimal code produces a compressed string that is as short as possible. In this\ncase, the compressed string using the optimal code is\n001100101110100 ,\n62",
            "so only 15 bits are needed instead of 18 bits. Thus, thanks to a better code it was\npossible to save 3 bits in the compressed string.\nWe require that no codeword is a pre\ufb01x of another codeword. For example,\nit is not allowed that a code would contain both codewords 10 and 1011. The\nreason for this is that we want to be able to generate the original string from\nthe compressed string. If a codeword could be a pre\ufb01x of another codeword, this\nwould not always be possible. For example, the following code is not valid:\ncharacter codeword\nA 10\nB 11\nC 1011\nD 111\nUsing this code, it would not be possible to know if the compressed string 1011\ncorresponds to the string AB or the string C.\nHu\ufb00man coding\nHuffman coding2 is a greedy algorithm that constructs an optimal code for\ncompressing a given string. The algorithm builds a binary tree based on the\nfrequencies of the characters in the string, and each character\u2019s codeword can be\nread by following a path from the root to the corresponding node. A move to the\nleft corresponds to bit 0, and a move to the right corresponds to bit 1.\nInitially, each character of the string is represented by a node whose weight\nis the number of times the character occurs in the string. Then at each step two\nnodes with minimum weights are combined by creating a new node whose weight\nis the sum of the weights of the original nodes. The process continues until all\nnodes have been combined.\nNext we will see how Huffman coding creates the optimal code for the string\nAABACDACA. Initially, there are four nodes that correspond to the characters of the\nstring:\n5 1 2 1\nA B C D\nThe node that represents character A has weight 5 because character A appears 5\ntimes in the string. The other weights have been calculated in the same way.\nThe \ufb01rst step is to combine the nodes that correspond to characters B and D,\nboth with weight 1. The result is:\n5 2 1 1\n2\nA C B D\n0 1\n2D. A. Huffman discovered this method when solving a university course assignment and\npublished the algorithm in 1952 [40].\n63",
            "After this, the nodes with weight 2 are combined:\n5\n2\n1 1\n2\n4\nA\nC\nB D\n0 1\n0 1\nFinally, the two remaining nodes are combined:\n5\n2\n1 1\n2\n4\n9\nA\nC\nB D\n0 1\n0 1\n0 1\nNow all nodes are in the tree, so the code is ready. The following codewords\ncan be read from the tree:\ncharacter codeword\nA 0\nB 110\nC 10\nD 111\n64",
            "Chapter 7\nDynamic programming\nDynamic programming is a technique that combines the correctness of com-\nplete search and the ef\ufb01ciency of greedy algorithms. Dynamic programming can\nbe applied if the problem can be divided into overlapping subproblems that can\nbe solved independently.\nThere are two uses for dynamic programming:\n\u2022 Finding an optimal solution: We want to \ufb01nd a solution that is as large\nas possible or as small as possible.\n\u2022 Counting the number of solutions: We want to calculate the total num-\nber of possible solutions.\nWe will \ufb01rst see how dynamic programming can be used to \ufb01nd an optimal\nsolution, and then we will use the same idea for counting the solutions.\nUnderstanding dynamic programming is a milestone in every competitive\nprogrammer\u2019s career. While the basic idea is simple, the challenge is how to apply\ndynamic programming to different problems. This chapter introduces a set of\nclassic problems that are a good starting point.\n7.1 Coin problem\nWe \ufb01rst focus on a problem that we have already seen in Chapter 6: Given a set\nof coin values coins = {c1, c2,..., ck} and a target sum of money n, our task is to\nform the sum n using as few coins as possible.\nIn Chapter 6, we solved the problem using a greedy algorithm that always\nchooses the largest possible coin. The greedy algorithm works, for example, when\nthe coins are the euro coins, but in the general case the greedy algorithm does\nnot necessarily produce an optimal solution.\nNow is time to solve the problem ef\ufb01ciently using dynamic programming, so\nthat the algorithm works for any coin set. The dynamic programming algorithm\nis based on a recursive function that goes through all possibilities how to form\nthe sum, like a brute force algorithm. However, the dynamic programming\nalgorithm is ef\ufb01cient because it uses memoization and calculates the answer to\neach subproblem only once.\n65",
            "Recursive formulation\nThe idea in dynamic programming is to formulate the problem recursively so\nthat the solution to the problem can be calculated from solutions to smaller\nsubproblems. In the coin problem, a natural recursive problem is as follows: what\nis the smallest number of coins required to form a sum x?\nLet solve(x) denote the minimum number of coins required for a sum x.\nThe values of the function depend on the values of the coins. For example, if\ncoins = {1,3,4}, the \ufb01rst values of the function are as follows:\nsolve(0) = 0\nsolve(1) = 1\nsolve(2) = 2\nsolve(3) = 1\nsolve(4) = 1\nsolve(5) = 2\nsolve(6) = 2\nsolve(7) = 2\nsolve(8) = 2\nsolve(9) = 3\nsolve(10) = 3\nFor example, solve(10) = 3, because at least 3 coins are needed to form the\nsum 10. The optimal solution is 3 +3+4 = 10.\nThe essential property of solve is that its values can be recursively calculated\nfrom its smaller values. The idea is to focus on the \ufb01rst coin that we choose for\nthe sum. For example, in the above scenario, the \ufb01rst coin can be either 1, 3\nor 4. If we \ufb01rst choose coin 1, the remaining task is to form the sum 9 using\nthe minimum number of coins, which is a subproblem of the original problem.\nOf course, the same applies to coins 3 and 4. Thus, we can use the following\nrecursive formula to calculate the minimum number of coins:\nsolve(x) = min(solve(x \u22121)+1,\nsolve(x \u22123)+1,\nsolve(x \u22124)+1).\nThe base case of the recursion is solve(0) = 0, because no coins are needed to\nform an empty sum. For example,\nsolve(10) = solve(7)+1 = solve(4)+2 = solve(0)+3 = 3.\nNow we are ready to give a general recursive function that calculates the\nminimum number of coins needed to form a sum x:\nsolve(x) =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\u221e x < 0\n0 x = 0\nminc\u2208coins solve(x \u2212 c)+1 x > 0\nFirst, if x < 0, the value is \u221e, because it is impossible to form a negative sum\nof money. Then, if x = 0, the value is 0, because no coins are needed to form an\n66",
            "empty sum. Finally, if x > 0, the variable c goes through all possibilities how to\nchoose the \ufb01rst coin of the sum.\nOnce a recursive function that solves the problem has been found, we can\ndirectly implement a solution in C++ (the constant INF denotes in\ufb01nity):\nint solve(int x) {\nif (x < 0) return INF;\nif (x == 0) return 0;\nint best = INF;\nfor (auto c : coins) {\nbest = min(best, solve(x-c)+1);\n}\nreturn best;\n}\nStill, this function is not ef\ufb01cient, because there may be an exponential\nnumber of ways to construct the sum. However, next we will see how to make the\nfunction ef\ufb01cient using a technique called memoization.\nUsing memoization\nThe idea of dynamic programming is to use memoization to ef\ufb01ciently calculate\nvalues of a recursive function. This means that the values of the function are\nstored in an array after calculating them. For each parameter, the value of the\nfunction is calculated recursively only once, and after this, the value can be\ndirectly retrieved from the array.\nIn this problem, we use arrays\nbool ready[N];\nint value[N];\nwhere ready[x] indicates whether the value of solve(x) has been calculated,\nand if it is, value[x] contains this value. The constant N has been chosen so that\nall required values \ufb01t in the arrays.\nNow the function can be ef\ufb01ciently implemented as follows:\nint solve(int x) {\nif (x < 0) return INF;\nif (x == 0) return 0;\nif (ready[x]) return value[x];\nint best = INF;\nfor (auto c : coins) {\nbest = min(best, solve(x-c)+1);\n}\nvalue[x] = best;\nready[x] = true;\nreturn best;\n}\n67",
            "The function handles the base cases x < 0 and x = 0 as previously. Then the\nfunction checks from ready[x] if solve(x) has already been stored in value[x], and\nif it is, the function directly returns it. Otherwise the function calculates the\nvalue of solve(x) recursively and stores it in value[x].\nThis function works ef\ufb01ciently, because the answer for each parameter x is\ncalculated recursively only once. After a value of solve(x) has been stored in\nvalue[x], it can be ef\ufb01ciently retrieved whenever the function will be called again\nwith the parameter x. The time complexity of the algorithm is O(nk), where n is\nthe target sum and k is the number of coins.\nNote that we can also iteratively construct the array value using a loop that\nsimply calculates all the values of solve for parameters 0... n:\nvalue[0] = 0;\nfor (int x = 1; x <= n; x++) {\nvalue[x] = INF;\nfor (auto c : coins) {\nif (x-c >= 0) {\nvalue[x] = min(value[x], value[x-c]+1);\n}\n}\n}\nIn fact, most competitive programmers prefer this implementation, because\nit is shorter and has lower constant factors. From now on, we also use iterative\nimplementations in our examples. Still, it is often easier to think about dynamic\nprogramming solutions in terms of recursive functions.\nConstructing a solution\nSometimes we are asked both to \ufb01nd the value of an optimal solution and to give\nan example how such a solution can be constructed. In the coin problem, for\nexample, we can declare another array that indicates for each sum of money the\n\ufb01rst coin in an optimal solution:\nint first[N];\nThen, we can modify the algorithm as follows:\nvalue[0] = 0;\nfor (int x = 1; x <= n; x++) {\nvalue[x] = INF;\nfor (auto c : coins) {\nif (x-c >= 0 && value[x-c]+1 < value[x]) {\nvalue[x] = value[x-c]+1;\nfirst[x] = c;\n}\n}\n}\n68",
            "After this, the following code can be used to print the coins that appear in an\noptimal solution for the sum n:\nwhile (n > 0) {\ncout << first[n] << \"\\n\";\nn -= first[n];\n}\nCounting the number of solutions\nLet us now consider another version of the coin problem where our task is to\ncalculate the total number of ways to produce a sum x using the coins. For\nexample, if coins = {1,3,4} and x = 5, there are a total of 6 ways:\n\u2022 1 +1+1+1+1\n\u2022 1 +1+3\n\u2022 1 +3+1\n\u2022 3 +1+1\n\u2022 1 +4\n\u2022 4 +1\nAgain, we can solve the problem recursively. Let solve(x) denote the number\nof ways we can form the sum x. For example, if coins = {1,3,4}, then solve(5) = 6\nand the recursive formula is\nsolve(x) =solve(x \u22121)+\nsolve(x \u22123)+\nsolve(x \u22124).\nThen, the general recursive function is as follows:\nsolve(x) =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n0 x < 0\n1 x = 0\n\u2211\nc\u2208coins solve(x \u2212 c) x > 0\nIf x < 0, the value is 0, because there are no solutions. If x = 0, the value is 1,\nbecause there is only one way to form an empty sum. Otherwise we calculate the\nsum of all values of the form solve(x \u2212 c) where c is in coins.\nThe following code constructs an array count such that count[x] equals the\nvalue of solve(x) for 0 \u2264 x \u2264 n:\ncount[0] = 1;\nfor (int x = 1; x <= n; x++) {\nfor (auto c : coins) {\nif (x-c >= 0) {\ncount[x] += count[x-c];\n}\n}\n}\n69",
            "Often the number of solutions is so large that it is not required to calculate the\nexact number but it is enough to give the answer modulo m where, for example,\nm = 109 +7. This can be done by changing the code so that all calculations are\ndone modulo m. In the above code, it suf\ufb01ces to add the line\ncount[x] %= m;\nafter the line\ncount[x] += count[x-c];\nNow we have discussed all basic ideas of dynamic programming. Since\ndynamic programming can be used in many different situations, we will now go\nthrough a set of problems that show further examples about the possibilities of\ndynamic programming.\n7.2 Longest increasing subsequence\nOur \ufb01rst problem is to \ufb01nd the longest increasing subsequence in an array\nof n elements. This is a maximum-length sequence of array elements that goes\nfrom left to right, and each element in the sequence is larger than the previous\nelement. For example, in the array\n6 2 5 1 7 4 8 3\n0 1 2 3 4 5 6 7\nthe longest increasing subsequence contains 4 elements:\n6 2 5 1 7 4 8 3\n0 1 2 3 4 5 6 7\nLet length(k) denote the length of the longest increasing subsequence that\nends at position k. Thus, if we calculate all values oflength(k) where 0\u2264 k \u2264 n\u22121,\nwe will \ufb01nd out the length of the longest increasing subsequence. For example,\nthe values of the function for the above array are as follows:\nlength(0) = 1\nlength(1) = 1\nlength(2) = 2\nlength(3) = 1\nlength(4) = 3\nlength(5) = 2\nlength(6) = 4\nlength(7) = 2\nFor example, length(6) = 4, because the longest increasing subsequence that\nends at position 6 consists of 4 elements.\n70",
            "To calculate a value of length(k), we should \ufb01nd a position i < k for which\narray[i] < array[k] and length(i) is as large as possible. Then we know that\nlength(k) = length(i) +1, because this is an optimal way to add array[k] to a\nsubsequence. However, if there is no such position i, then length(k) = 1, which\nmeans that the subsequence only contains array[k].\nSince all values of the function can be calculated from its smaller values, we\ncan use dynamic programming. In the following code, the values of the function\nwill be stored in an array length.\nfor (int k = 0; k < n; k++) {\nlength[k] = 1;\nfor (int i = 0; i < k; i++) {\nif (array[i] < array[k]) {\nlength[k] = max(length[k],length[i]+1);\n}\n}\n}\nThis code works in O(n2) time, because it consists of two nested loops. How-\never, it is also possible to implement the dynamic programming calculation more\nef\ufb01ciently in O(nlogn) time. Can you \ufb01nd a way to do this?\n7.3 Paths in a grid\nOur next problem is to \ufb01nd a path from the upper-left corner to the lower-right\ncorner of an n \u00d7 n grid, such that we only move down and right. Each square\ncontains a positive integer, and the path should be constructed so that the sum of\nthe values along the path is as large as possible.\nThe following picture shows an optimal path in a grid:\n3 7 9 2 7\n9 8 3 5 5\n1 7 9 8 5\n3 8 6 4 10\n6 3 9 7 8\nThe sum of the values on the path is 67, and this is the largest possible sum on a\npath from the upper-left corner to the lower-right corner.\nAssume that the rows and columns of the grid are numbered from 1 to n, and\nvalue[y][x] equals the value of square (y, x). Let sum(y, x) denote the maximum\nsum on a path from the upper-left corner to square ( y, x). Now sum(n,n) tells\nus the maximum sum from the upper-left corner to the lower-right corner. For\nexample, in the above grid, sum(5,5) = 67.\nWe can recursively calculate the sums as follows:\nsum(y, x) = max(sum(y, x \u22121),sum(y\u22121, x))+value[y][x]\n71",
            "The recursive formula is based on the observation that a path that ends at\nsquare (y, x) can come either from square (y, x \u22121) or square (y\u22121, x):\n\u2192\n\u2193\nThus, we select the direction that maximizes the sum. We assume that\nsum(y, x) = 0 if y = 0 or x = 0 (because no such paths exist), so the recursive\nformula also works when y = 1 or x = 1.\nSince the function sum has two parameters, the dynamic programming array\nalso has two dimensions. For example, we can use an array\nint sum[N][N];\nand calculate the sums as follows:\nfor (int y = 1; y <= n; y++) {\nfor (int x = 1; x <= n; x++) {\nsum[y][x] = max(sum[y][x-1],sum[y-1][x])+value[y][x];\n}\n}\nThe time complexity of the algorithm is O(n2).\n7.4 Knapsack problems\nThe term knapsack refers to problems where a set of objects is given, and\nsubsets with some properties have to be found. Knapsack problems can often be\nsolved using dynamic programming.\nIn this section, we focus on the following problem: Given a list of weights\n[w1,w2,..., wn], determine all sums that can be constructed using the weights.\nFor example, if the weights are [1,3,3,5], the following sums are possible:\n0 1 2 3 4 5 6 7 8 9 10 11 12\nX X X X X X X X X X X\nIn this case, all sums between 0 ... 12 are possible, except 2 and 10. For\nexample, the sum 7 is possible because we can select the weights [1,3,3].\nTo solve the problem, we focus on subproblems where we only use the \ufb01rst k\nweights to construct sums. Let possible(x,k) = true if we can construct a sum x\nusing the \ufb01rst k weights, and otherwise possible(x,k) = false. The values of the\nfunction can be recursively calculated as follows:\npossible(x,k) = possible(x \u2212wk,k \u22121)\u2228possible(x,k \u22121)\n72",
            "The formula is based on the fact that we can either use or not use the weight wk\nin the sum. If we use wk, the remaining task is to form the sum x \u2212wk using the\n\ufb01rst k\u22121 weights, and if we do not use wk, the remaining task is to form the sum\nx using the \ufb01rst k \u22121 weights. As the base cases,\npossible(x,0) =\n{\ntrue x = 0\nfalse x \u0338= 0\nbecause if no weights are used, we can only form the sum 0.\nThe following table shows all values of the function for the weights [1,3,3,5]\n(the symbol \u201dX\u201d indicates the true values):\nk\\x 0 1 2 3 4 5 6 7 8 9 10 11 12\n0 X\n1 X X\n2 X X X X\n3 X X X X X X\n4 X X X X X X X X X X X\nAfter calculating those values, possible(x,n) tells us whether we can con-\nstruct a sum x using all weights.\nLet W denote the total sum of the weights. The followingO(nW) time dynamic\nprogramming solution corresponds to the recursive function:\npossible[0][0] = true;\nfor (int k = 1; k <= n; k++) {\nfor (int x = 0; x <= W; x++) {\nif (x-w[k] >= 0) possible[x][k] |= possible[x-w[k]][k-1];\npossible[x][k] |= possible[x][k-1];\n}\n}\nHowever, here is a better implementation that only uses a one-dimensional\narray possible[x] that indicates whether we can construct a subset with sum x.\nThe trick is to update the array from right to left for each new weight:\npossible[0] = true;\nfor (int k = 1; k <= n; k++) {\nfor (int x = W; x >= 0; x--) {\nif (possible[x]) possible[x+w[k]] = true;\n}\n}\nNote that the general idea presented here can be used in many knapsack\nproblems. For example, if we are given objects with weights and values, we can\ndetermine for each weight sum the maximum value sum of a subset.\n73",
            "7.5 Edit distance\nThe edit distance or Levenshtein distance1 is the minimum number of edit-\ning operations needed to transform a string into another string. The allowed\nediting operations are as follows:\n\u2022 insert a character (e.g. ABC \u2192 ABCA)\n\u2022 remove a character (e.g. ABC \u2192 AC)\n\u2022 modify a character (e.g. ABC \u2192 ADC)\nFor example, the edit distance between LOVE and MOVIE is 2, because we can\n\ufb01rst perform the operation LOVE \u2192 MOVE (modify) and then the operation MOVE \u2192\nMOVIE (insert). This is the smallest possible number of operations, because it is\nclear that only one operation is not enough.\nSuppose that we are given a string x of length n and a string y of length m,\nand we want to calculate the edit distance between x and y. To solve the problem,\nwe de\ufb01ne a function distance(a,b) that gives the edit distance between pre\ufb01xes\nx[0... a] and y[0... b]. Thus, using this function, the edit distance between x and\ny equals distance(n \u22121,m \u22121).\nWe can calculate values of distance as follows:\ndistance(a,b) = min(distance(a,b \u22121)+1,\ndistance(a \u22121,b)+1,\ndistance(a \u22121,b \u22121)+cost(a,b)).\nHere cost(a,b) = 0 if x[a] = y[b], and otherwise cost(a,b) = 1. The formula\nconsiders the following ways to edit the string x:\n\u2022 distance(a,b \u22121): insert a character at the end of x\n\u2022 distance(a \u22121,b): remove the last character from x\n\u2022 distance(a \u22121,b \u22121): match or modify the last character of x\nIn the two \ufb01rst cases, one editing operation is needed (insert or remove). In the\nlast case, if x[a] = y[b], we can match the last characters without editing, and\notherwise one editing operation is needed (modify).\nThe following table shows the values of distance in the example case:\nL\nO\nV\nE\nM O V I E\n0\n1\n2\n3\n4\n1\n1\n2\n3\n4\n2\n2\n1\n2\n3\n3\n3\n2\n1\n2\n4\n4\n3\n2\n2\n5\n5\n4\n3\n2\n1The distance is named after V. I. Levenshtein who studied it in connection with binary codes\n[49].\n74",
            "The lower-right corner of the table tells us that the edit distance between\nLOVE and MOVIE is 2. The table also shows how to construct the shortest sequence\nof editing operations. In this case the path is as follows:\nL\nO\nV\nE\nM O V I E\n0\n1\n2\n3\n4\n1\n1\n2\n3\n4\n2\n2\n1\n2\n3\n3\n3\n2\n1\n2\n4\n4\n3\n2\n2\n5\n5\n4\n3\n2\nThe last characters of LOVE and MOVIE are equal, so the edit distance between\nthem equals the edit distance between LOV and MOVI. We can use one editing\noperation to remove the character I from MOVI. Thus, the edit distance is one\nlarger than the edit distance between LOV and MOV, etc.\n7.6 Counting tilings\nSometimes the states of a dynamic programming solution are more complex\nthan \ufb01xed combinations of numbers. As an example, consider the problem of\ncalculating the number of distinct ways to \ufb01ll an n \u00d7m grid using 1\u00d72 and 2\u00d71\nsize tiles. For example, one valid solution for the 4\u00d77 grid is\nand the total number of solutions is 781.\nThe problem can be solved using dynamic programming by going through\nthe grid row by row. Each row in a solution can be represented as a string that\ncontains m characters from the set {\u2293,\u2294,\u228f,\u2290}. For example, the above solution\nconsists of four rows that correspond to the following strings:\n\u2022 \u2293 \u228f\u2290\u2293 \u228f\u2290\u2293\n\u2022 \u2294 \u228f\u2290\u2294\u2293\u2293\u2294\n\u2022 \u228f\u2290\u228f\u2290\u2294\u2294\u2293\n\u2022 \u228f\u2290\u228f\u2290\u228f\u2290\u2294\nLet count(k, x) denote the number of ways to construct a solution for rows\n1... k of the grid such that string x corresponds to row k. It is possible to use\ndynamic programming here, because the state of a row is constrained only by the\nstate of the previous row.\n75",
            "A solution is valid if row 1 does not contain the character \u2294, row n does not\ncontain the character\u2293, and all consecutive rows arecompatible. For example, the\nrows \u2294 \u228f\u2290\u2294\u2293\u2293\u2294 and \u228f\u2290\u228f\u2290\u2294\u2294\u2293 are compatible, while the rows \u2293 \u228f\u2290\u2293 \u228f\u2290\u2293\nand \u228f\u2290\u228f\u2290\u228f\u2290\u2294 are not compatible.\nSince a row consists of m characters and there are four choices for each\ncharacter, the number of distinct rows is at most 4m. Thus, the time complexity\nof the solution is O(n42m) because we can go through the O(4m) possible states\nfor each row, and for each state, there areO(4m) possible states for the previous\nrow. In practice, it is a good idea to rotate the grid so that the shorter side has\nlength m, because the factor 42m dominates the time complexity.\nIt is possible to make the solution more ef\ufb01cient by using a more compact\nrepresentation for the rows. It turns out that it is suf\ufb01cient to know which\ncolumns of the previous row contain the upper square of a vertical tile. Thus, we\ncan represent a row using only characters \u2293 and \u25a1, where \u25a1is a combination\nof characters \u2294, \u228fand \u2290. Using this representation, there are only 2m distinct\nrows and the time complexity is O(n22m).\nAs a \ufb01nal note, there is also a surprising direct formula for calculating the\nnumber of tilings2:\n\u2308n/2\u2309\u220f\na=1\n\u2308m/2\u2309\u220f\nb=1\n4\u00b7(cos2 \u03c0a\nn +1 +cos2 \u03c0b\nm +1)\nThis formula is very ef\ufb01cient, because it calculates the number of tilings inO(nm)\ntime, but since the answer is a product of real numbers, a problem when using\nthe formula is how to store the intermediate results accurately.\n2Surprisingly, this formula was discovered in 1961 by two research teams [43, 67] that worked\nindependently.\n76",
            "Chapter 8\nAmortized analysis\nThe time complexity of an algorithm is often easy to analyze just by examining\nthe structure of the algorithm: what loops does the algorithm contain and how\nmany times the loops are performed. However, sometimes a straightforward\nanalysis does not give a true picture of the ef\ufb01ciency of the algorithm.\nAmortized analysis can be used to analyze algorithms that contain opera-\ntions whose time complexity varies. The idea is to estimate the total time used to\nall such operations during the execution of the algorithm, instead of focusing on\nindividual operations.\n8.1 Two pointers method\nIn the two pointers method, two pointers are used to iterate through the array\nvalues. Both pointers can move to one direction only, which ensures that the\nalgorithm works ef\ufb01ciently. Next we discuss two problems that can be solved\nusing the two pointers method.\nSubarray sum\nAs the \ufb01rst example, consider a problem where we are given an array ofn positive\nintegers and a target sum x, and we want to \ufb01nd a subarray whose sum is x or\nreport that there is no such subarray.\nFor example, the array\n1 3 2 5 1 1 2 3\ncontains a subarray whose sum is 8:\n1 3 2 5 1 1 2 3\nThis problem can be solved in O(n) time by using the two pointers method.\nThe idea is to maintain pointers that point to the \ufb01rst and last value of a subarray.\nOn each turn, the left pointer moves one step to the right, and the right pointer\nmoves to the right as long as the resulting subarray sum is at most x. If the sum\nbecomes exactly x, a solution has been found.\n77",
            "As an example, consider the following array and a target sum x = 8:\n1 3 2 5 1 1 2 3\nThe initial subarray contains the values 1, 3 and 2 whose sum is 6:\n1 3 2 5 1 1 2 3\nThen, the left pointer moves one step to the right. The right pointer does not\nmove, because otherwise the subarray sum would exceed x.\n1 3 2 5 1 1 2 3\nAgain, the left pointer moves one step to the right, and this time the right\npointer moves three steps to the right. The subarray sum is 2 +5 +1 = 8, so a\nsubarray whose sum is x has been found.\n1 3 2 5 1 1 2 3\nThe running time of the algorithm depends on the number of steps the right\npointer moves. While there is no useful upper bound on how many steps the\npointer can move on a single turn. we know that the pointer moves a total of\nO(n) steps during the algorithm, because it only moves to the right.\nSince both the left and right pointer move O(n) steps during the algorithm,\nthe algorithm works in O(n) time.\n2SUM problem\nAnother problem that can be solved using the two pointers method is the following\nproblem, also known as the 2SUM problem: given an array of n numbers and a\ntarget sum x, \ufb01nd two array values such that their sum is x, or report that no\nsuch values exist.\nTo solve the problem, we \ufb01rst sort the array values in increasing order. After\nthat, we iterate through the array using two pointers. The left pointer starts at\nthe \ufb01rst value and moves one step to the right on each turn. The right pointer\nbegins at the last value and always moves to the left until the sum of the left and\nright value is at most x. If the sum is exactly x, a solution has been found.\nFor example, consider the following array and a target sum x = 12:\n1 4 5 6 7 9 9 10\nThe initial positions of the pointers are as follows. The sum of the values is\n1+10 = 11 that is smaller than x.\n78",
            "1 4 5 6 7 9 9 10\nThen the left pointer moves one step to the right. The right pointer moves\nthree steps to the left, and the sum becomes 4+7 = 11.\n1 4 5 6 7 9 9 10\nAfter this, the left pointer moves one step to the right again. The right pointer\ndoes not move, and a solution 5+7 = 12 has been found.\n1 4 5 6 7 9 9 10\nThe running time of the algorithm is O(nlogn), because it \ufb01rst sorts the array\nin O(nlogn) time, and then both pointers move O(n) steps.\nNote that it is possible to solve the problem in another way in O(nlogn) time\nusing binary search. In such a solution, we iterate through the array and for\neach array value, we try to \ufb01nd another value that yields the sum x. This can be\ndone by performing n binary searches, each of which takes O(logn) time.\nA more dif\ufb01cult problem is the 3SUM problem that asks to \ufb01nd three array\nvalues whose sum is x. Using the idea of the above algorithm, this problem can\nbe solved in O(n2) time1. Can you see how?\n8.2 Nearest smaller elements\nAmortized analysis is often used to estimate the number of operations performed\non a data structure. The operations may be distributed unevenly so that most\noperations occur during a certain phase of the algorithm, but the total number of\nthe operations is limited.\nAs an example, consider the problem of \ufb01nding for each array element the\nnearest smaller element , i.e., the \ufb01rst smaller element that precedes the\nelement in the array. It is possible that no such element exists, in which case the\nalgorithm should report this. Next we will see how the problem can be ef\ufb01ciently\nsolved using a stack structure.\nWe go through the array from left to right and maintain a stack of array\nelements. At each array position, we remove elements from the stack until the\ntop element is smaller than the current element, or the stack is empty. Then, we\nreport that the top element is the nearest smaller element of the current element,\nor if the stack is empty, there is no such element. Finally, we add the current\nelement to the stack.\nAs an example, consider the following array:\n1For a long time, it was thought that solving the 3SUM problem more ef\ufb01ciently than in O(n2)\ntime would not be possible. However, in 2014, it turned out [30] that this is not the case.\n79",
            "1 3 4 2 5 3 4 2\nFirst, the elements 1, 3 and 4 are added to the stack, because each element is\nlarger than the previous element. Thus, the nearest smaller element of 4 is 3,\nand the nearest smaller element of 3 is 1.\n1 3 4 2 5 3 4 2\n1 3 4\nThe next element 2 is smaller than the two top elements in the stack. Thus,\nthe elements 3 and 4 are removed from the stack, and then the element 2 is\nadded to the stack. Its nearest smaller element is 1:\n1 3 4 2 5 3 4 2\n1 2\nThen, the element 5 is larger than the element 2, so it will be added to the\nstack, and its nearest smaller element is 2:\n1 3 4 2 5 3 4 2\n1 2 5\nAfter this, the element 5 is removed from the stack and the elements 3 and 4\nare added to the stack:\n1 3 4 2 5 3 4 2\n1 2 3 4\nFinally, all elements except 1 are removed from the stack and the last element\n2 is added to the stack:\n1 3 4 2 5 3 4 2\n1 2\nThe ef\ufb01ciency of the algorithm depends on the total number of stack opera-\ntions. If the current element is larger than the top element in the stack, it is\ndirectly added to the stack, which is ef\ufb01cient. However, sometimes the stack can\ncontain several larger elements and it takes time to remove them. Still, each\nelement is added exactly once to the stack and removed at most once from the\nstack. Thus, each element causes O(1) stack operations, and the algorithm works\nin O(n) time.\n80",
            "8.3 Sliding window minimum\nA sliding window is a constant-size subarray that moves from left to right\nthrough the array. At each window position, we want to calculate some infor-\nmation about the elements inside the window. In this section, we focus on the\nproblem of maintaining the sliding window minimum, which means that we\nshould report the smallest value inside each window.\nThe sliding window minimum can be calculated using a similar idea that\nwe used to calculate the nearest smaller elements. We maintain a queue where\neach element is larger than the previous element, and the \ufb01rst element always\ncorresponds to the minimum element inside the window. After each window\nmove, we remove elements from the end of the queue until the last queue element\nis smaller than the new window element, or the queue becomes empty. We also\nremove the \ufb01rst queue element if it is not inside the window anymore. Finally,\nwe add the new window element to the end of the queue.\nAs an example, consider the following array:\n2 1 4 5 3 4 1 2\nSuppose that the size of the sliding window is 4. At the \ufb01rst window position,\nthe smallest value is 1:\n2 1 4 5 3 4 1 2\n1 4 5\nThen the window moves one step right. The new element 3 is smaller than\nthe elements 4 and 5 in the queue, so the elements 4 and 5 are removed from the\nqueue and the element 3 is added to the queue. The smallest value is still 1.\n2 1 4 5 3 4 1 2\n1 3\nAfter this, the window moves again, and the smallest element 1 does not\nbelong to the window anymore. Thus, it is removed from the queue and the\nsmallest value is now 3. Also the new element 4 is added to the queue.\n2 1 4 5 3 4 1 2\n3 4\nThe next new element 1 is smaller than all elements in the queue. Thus, all\nelements are removed from the queue and it will only contain the element 1:\n2 1 4 5 3 4 1 2\n1\n81",
            "Finally the window reaches its last position. The element 2 is added to the\nqueue, but the smallest value inside the window is still 1.\n2 1 4 5 3 4 1 2\n1 2\nSince each array element is added to the queue exactly once and removed\nfrom the queue at most once, the algorithm works in O(n) time.\n82",
            "Chapter 9\nRange queries\nIn this chapter, we discuss data structures that allow us to ef\ufb01ciently process\nrange queries. In a range query, our task is to calculate a value based on a\nsubarray of an array. Typical range queries are:\n\u2022 sumq(a,b): calculate the sum of values in range [a,b]\n\u2022 minq(a,b): \ufb01nd the minimum value in range [ a,b]\n\u2022 maxq(a,b): \ufb01nd the maximum value in range [ a,b]\nFor example, consider the range [3,6] in the following array:\n1 3 8 4 6 1 3 4\n0 1 2 3 4 5 6 7\nIn this case, sumq(3,6) = 14, minq(3,6) = 1 and maxq(3,6) = 6.\nA simple way to process range queries is to use a loop that goes through all\narray values in the range. For example, the following function can be used to\nprocess sum queries on an array:\nint sum(int a, int b) {\nint s = 0;\nfor (int i = a; i <= b; i++) {\ns += array[i];\n}\nreturn s;\n}\nThis function works in O(n) time, where n is the size of the array. Thus, we\ncan process q queries in O(nq) time using the function. However, if both n and q\nare large, this approach is slow. Fortunately, it turns out that there are ways to\nprocess range queries much more ef\ufb01ciently.\n83",
            "9.1 Static array queries\nWe \ufb01rst focus on a situation where the array is static, i.e., the array values are\nnever updated between the queries. In this case, it suf\ufb01ces to construct a static\ndata structure that tells us the answer for any possible query.\nSum queries\nWe can easily process sum queries on a static array by constructing a pre\ufb01x\nsum array. Each value in the pre\ufb01x sum array equals the sum of values in the\noriginal array up to that position, i.e., the value at position k is sumq(0,k). The\npre\ufb01x sum array can be constructed in O(n) time.\nFor example, consider the following array:\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\nThe corresponding pre\ufb01x sum array is as follows:\n1 4 8 16 22 23 27 29\n0 1 2 3 4 5 6 7\nSince the pre\ufb01x sum array contains all values of sumq(0,k), we can calculate any\nvalue of sumq(a,b) in O(1) time as follows:\nsumq(a,b) = sumq(0,b)\u2212sumq(0,a \u22121)\nBy de\ufb01ning sumq(0,\u22121) = 0, the above formula also holds when a = 0.\nFor example, consider the range [3,6]:\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\nIn this case sumq(3,6) = 8+6+1+4 = 19. This sum can be calculated from two\nvalues of the pre\ufb01x sum array:\n1 4 8 16 22 23 27 29\n0 1 2 3 4 5 6 7\nThus, sumq(3,6) = sumq(0,6)\u2212sumq(0,2) = 27\u22128 = 19.\nIt is also possible to generalize this idea to higher dimensions. For example,\nwe can construct a two-dimensional pre\ufb01x sum array that can be used to calculate\nthe sum of any rectangular subarray in O(1) time. Each sum in such an array\ncorresponds to a subarray that begins at the upper-left corner of the array.\n84",
            "The following picture illustrates the idea:\nAB\nCD\nThe sum of the gray subarray can be calculated using the formula\nS(A)\u2212S(B)\u2212S(C)+S(D),\nwhere S(X) denotes the sum of values in a rectangular subarray from the upper-\nleft corner to the position of X.\nMinimum queries\nMinimum queries are more dif\ufb01cult to process than sum queries. Still, there is\na quite simple O(nlogn) time preprocessing method after which we can answer\nany minimum query in O(1) time1. Note that since minimum and maximum\nqueries can be processed similarly, we can focus on minimum queries.\nThe idea is to precalculate all values of minq(a,b) where b \u2212a +1 (the length\nof the range) is a power of two. For example, for the array\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\nthe following values are calculated:\na b minq(a,b)\n0 0 1\n1 1 3\n2 2 4\n3 3 8\n4 4 6\n5 5 1\n6 6 4\n7 7 2\na b minq(a,b)\n0 1 1\n1 2 3\n2 3 4\n3 4 6\n4 5 1\n5 6 1\n6 7 2\na b minq(a,b)\n0 3 1\n1 4 3\n2 5 1\n3 6 1\n4 7 1\n0 7 1\nThe number of precalculated values is O(nlogn), because there are O(logn)\nrange lengths that are powers of two. The values can be calculated ef\ufb01ciently\nusing the recursive formula\nminq(a,b) = min(minq(a,a +w \u22121),minq(a +w,b)),\n1This technique was introduced in [7] and sometimes called the sparse table method. There\nare also more sophisticated techniques [22] where the preprocessing time is only O(n), but such\nalgorithms are not needed in competitive programming.\n85",
            "where b\u2212a+1 is a power of two and w = (b\u2212a+1)/2. Calculating all those values\ntakes O(nlogn) time.\nAfter this, any value ofminq(a,b) can be calculated inO(1) time as a minimum\nof two precalculated values. Let k be the largest power of two that does not exceed\nb \u2212a +1. We can calculate the value of minq(a,b) using the formula\nminq(a,b) = min(minq(a,a +k \u22121),minq(b \u2212k +1,b)).\nIn the above formula, the range [a,b] is represented as the union of the ranges\n[a,a +k \u22121] and [b \u2212k +1,b], both of length k.\nAs an example, consider the range [1,6]:\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\nThe length of the range is 6, and the largest power of two that does not exceed 6\nis 4. Thus the range [1,6] is the union of the ranges [1,4] and [3,6]:\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\n1 3 4 8 6 1 4 2\n0 1 2 3 4 5 6 7\nSince minq(1,4) = 3 and minq(3,6) = 1, we conclude that minq(1,6) = 1.\n9.2 Binary indexed tree\nA binary indexed tree or a Fenwick tree2 can be seen as a dynamic variant\nof a pre\ufb01x sum array. It supports two O(logn) time operations on an array:\nprocessing a range sum query and updating a value.\nThe advantage of a binary indexed tree is that it allows us to ef\ufb01ciently update\narray values between sum queries. This would not be possible using a pre\ufb01x sum\narray, because after each update, it would be necessary to build the whole pre\ufb01x\nsum array again in O(n) time.\nStructure\nEven if the name of the structure is a binary indexedtree, it is usually represented\nas an array. In this section we assume that all arrays are one-indexed, because it\nmakes the implementation easier.\nLet p(k) denote the largest power of two that divides k. We store a binary\nindexed tree as an array tree such that\ntree[k] = sumq(k \u2212 p(k)+1,k),\n2The binary indexed tree structure was presented by P. M. Fenwick in 1994 [21].\n86",
            "i.e., each position k contains the sum of values in a range of the original array\nwhose length is p(k) and that ends at position k. For example, since p(6) = 2,\ntree[6] contains the value of sumq(5,6).\nFor example, consider the following array:\n1 3 4 8 6 1 4 2\n1 2 3 4 5 6 7 8\nThe corresponding binary indexed tree is as follows:\n1 4 4 16 6 7 4 29\n1 2 3 4 5 6 7 8\nThe following picture shows more clearly how each value in the binary indexed\ntree corresponds to a range in the original array:\n1 4 4 16 6 7 4 29\n1 2 3 4 5 6 7 8\nUsing a binary indexed tree, any value of sumq(1,k) can be calculated in\nO(logn) time, because a range [1,k] can always be divided into O(logn) ranges\nwhose sums are stored in the tree.\nFor example, the range [1,7] consists of the following ranges:\n1 4 4 16 6 7 4 29\n1 2 3 4 5 6 7 8\nThus, we can calculate the corresponding sum as follows:\nsumq(1,7) = sumq(1,4)+sumq(5,6)+sumq(7,7) = 16+7+4 = 27\nTo calculate the value of sumq(a,b) where a > 1, we can use the same trick\nthat we used with pre\ufb01x sum arrays:\nsumq(a,b) = sumq(1,b)\u2212sumq(1,a \u22121).\n87",
            "Since we can calculate both sumq(1,b) and sumq(1,a\u22121) in O(logn) time, the total\ntime complexity is O(logn).\nThen, after updating a value in the original array, several values in the binary\nindexed tree should be updated. For example, if the value at position 3 changes,\nthe sums of the following ranges change:\n1 4 4 16 6 7 4 29\n1 2 3 4 5 6 7 8\nSince each array element belongs to O(logn) ranges in the binary indexed\ntree, it suf\ufb01ces to update O(logn) values in the tree.\nImplementation\nThe operations of a binary indexed tree can be ef\ufb01ciently implemented using bit\noperations. The key fact needed is that we can calculate any value of p(k) using\nthe formula\np(k) = k&\u2212k.\nThe following function calculates the value of sumq(1,k):\nint sum(int k) {\nint s = 0;\nwhile (k >= 1) {\ns += tree[k];\nk -= k&-k;\n}\nreturn s;\n}\nThe following function increases the array value at position k by x (x can be\npositive or negative):\nvoid add(int k, int x) {\nwhile (k <= n) {\ntree[k] += x;\nk += k&-k;\n}\n}\nThe time complexity of both the functions is O(logn), because the functions\naccess O(logn) values in the binary indexed tree, and each move to the next\nposition takes O(1) time.\n88",
            "9.3 Segment tree\nA segment tree3 is a data structure that supports two operations: processing\na range query and updating an array value. Segment trees can support sum\nqueries, minimum and maximum queries and many other queries so that both\noperations work in O(logn) time.\nCompared to a binary indexed tree, the advantage of a segment tree is that it\nis a more general data structure. While binary indexed trees only support sum\nqueries4, segment trees also support other queries. On the other hand, a segment\ntree requires more memory and is a bit more dif\ufb01cult to implement.\nStructure\nA segment tree is a binary tree such that the nodes on the bottom level of the\ntree correspond to the array elements, and the other nodes contain information\nneeded for processing range queries.\nIn this section, we assume that the size of the array is a power of two and\nzero-based indexing is used, because it is convenient to build a segment tree for\nsuch an array. If the size of the array is not a power of two, we can always append\nextra elements to it.\nWe will \ufb01rst discuss segment trees that support sum queries. As an example,\nconsider the following array:\n5 8 6 3 2 7 2 6\n0 1 2 3 4 5 6 7\nThe corresponding segment tree is as follows:\n5 8 6 3 2 7 2 6\n13 9 9 8\n22 17\n39\nEach internal tree node corresponds to an array range whose size is a power\nof two. In the above tree, the value of each internal node is the sum of the\ncorresponding array values, and it can be calculated as the sum of the values of\nits left and right child node.\n3The bottom-up-implementation in this chapter corresponds to that in [62]. Similar structures\nwere used in late 1970\u2019s to solve geometric problems [9].\n4In fact, using two binary indexed trees it is possible to support minimum queries [16], but\nthis is more complicated than to use a segment tree.\n89",
            "It turns out that any range [a,b] can be divided into O(logn) ranges whose\nvalues are stored in tree nodes. For example, consider the range [2,7]:\n5 8 6 3 2 7 2 6\n0 1 2 3 4 5 6 7\nHere sumq(2,7) = 6+3+2+7+2+6 = 26. In this case, the following two tree nodes\ncorrespond to the range:\n5 8 6 3 2 7 2 6\n13 9 9 8\n22 17\n39\nThus, another way to calculate the sum is 9+17 = 26.\nWhen the sum is calculated using nodes located as high as possible in the\ntree, at most two nodes on each level of the tree are needed. Hence, the total\nnumber of nodes is O(logn).\nAfter an array update, we should update all nodes whose value depends on\nthe updated value. This can be done by traversing the path from the updated\narray element to the top node and updating the nodes along the path.\nThe following picture shows which tree nodes change if the array value 7\nchanges:\n5 8 6 3 2 7 2 6\n13 9 9 8\n22 17\n39\nThe path from bottom to top always consists of O(logn) nodes, so each update\nchanges O(logn) nodes in the tree.\nImplementation\nWe store a segment tree as an array of 2 n elements where n is the size of the\noriginal array and a power of two. The tree nodes are stored from top to bottom:\n90",
            "tree[1] is the top node, tree[2] and tree[3] are its children, and so on. Finally,\nthe values from tree[n] to tree[2n \u22121] correspond to the values of the original\narray on the bottom level of the tree.\nFor example, the segment tree\n5 8 6 3 2 7 2 6\n13 9 9 8\n22 17\n39\nis stored as follows:\n39 22 17 13 9 9 8 5 8 6 3 2 7 2 6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nUsing this representation, the parent of tree[k] is tree[\u230ak/2\u230b], and its children\nare tree[2k] and tree[2k +1]. Note that this implies that the position of a node\nis even if it is a left child and odd if it is a right child.\nThe following function calculates the value of sumq(a,b):\nint sum(int a, int b) {\na += n; b += n;\nint s = 0;\nwhile (a <= b) {\nif (a%2 == 1) s += tree[a++];\nif (b%2 == 0) s += tree[b--];\na /= 2; b /= 2;\n}\nreturn s;\n}\nThe function maintains a range that is initially [a +n,b +n]. Then, at each step,\nthe range is moved one level higher in the tree, and before that, the values of the\nnodes that do not belong to the higher range are added to the sum.\nThe following function increases the array value at position k by x:\nvoid add(int k, int x) {\nk += n;\ntree[k] += x;\nfor (k /= 2; k >= 1; k /= 2) {\ntree[k] = tree[2*k]+tree[2*k+1];\n}\n}\n91",
            "First the function updates the value at the bottom level of the tree. After this,\nthe function updates the values of all internal tree nodes, until it reaches the top\nnode of the tree.\nBoth the above functions work in O(logn) time, because a segment tree of n\nelements consists of O(logn) levels, and the functions move one level higher in\nthe tree at each step.\nOther queries\nSegment trees can support all range queries where it is possible to divide a range\ninto two parts, calculate the answer separately for both parts and then ef\ufb01ciently\ncombine the answers. Examples of such queries are minimum and maximum,\ngreatest common divisor, and bit operations and, or and xor.\nFor example, the following segment tree supports minimum queries:\n5 8 6 3 1 7 2 6\n5 3 1 2\n3 1\n1\nIn this case, every tree node contains the smallest value in the corresponding\narray range. The top node of the tree contains the smallest value in the whole\narray. The operations can be implemented like previously, but instead of sums,\nminima are calculated.\nThe structure of a segment tree also allows us to use binary search for locating\narray elements. For example, if the tree supports minimum queries, we can \ufb01nd\nthe position of an element with the smallest value in O(logn) time.\nFor example, in the above tree, an element with the smallest value 1 can be\nfound by traversing a path downwards from the top node:\n5 8 6 3 1 7 2 6\n5 3 1 2\n3 1\n1\n92",
            "9.4 Additional techniques\nIndex compression\nA limitation in data structures that are built upon an array is that the elements\nare indexed using consecutive integers. Dif\ufb01culties arise when large indices are\nneeded. For example, if we wish to use the index 109, the array should contain\n109 elements which would require too much memory.\nHowever, we can often bypass this limitation by using index compression,\nwhere the original indices are replaced with indices 1,2,3, etc. This can be done\nif we know all the indices needed during the algorithm beforehand.\nThe idea is to replace each original indexx with c(x) where c is a function that\ncompresses the indices. We require that the order of the indices does not change,\nso if a < b, then c(a) < c(b). This allows us to conveniently perform queries even\nif the indices are compressed.\nFor example, if the original indices are 555, 109 and 8, the new indices are:\nc(8) = 1\nc(555) = 2\nc(109) = 3\nRange updates\nSo far, we have implemented data structures that support range queries and\nupdates of single values. Let us now consider an opposite situation, where we\nshould update ranges and retrieve single values. We focus on an operation that\nincreases all elements in a range [a,b] by x.\nSurprisingly, we can use the data structures presented in this chapter also in\nthis situation. To do this, we build adifference array whose values indicate the\ndifferences between consecutive values in the original array. Thus, the original\narray is the pre\ufb01x sum array of the difference array. For example, consider the\nfollowing array:\n3 3 1 1 1 5 2 2\n0 1 2 3 4 5 6 7\nThe difference array for the above array is as follows:\n3 0 \u22122 0 0 4 \u22123 0\n0 1 2 3 4 5 6 7\nFor example, the value 2 at position 6 in the original array corresponds to the\nsum 3\u22122+4\u22123 = 2 in the difference array.\nThe advantage of the difference array is that we can update a range in the\noriginal array by changing just two elements in the difference array. For example,\nif we want to increase the original array values between positions 1 and 4 by 5, it\nsuf\ufb01ces to increase the difference array value at position 1 by 5 and decrease the\nvalue at position 5 by 5. The result is as follows:\n93",
            "3 5 \u22122 0 0 \u22121 \u22123 0\n0 1 2 3 4 5 6 7\nMore generally, to increase the values in range [ a,b] by x, we increase the\nvalue at position a by x and decrease the value at position b +1 by x. Thus, it is\nonly needed to update single values and process sum queries, so we can use a\nbinary indexed tree or a segment tree.\nA more dif\ufb01cult problem is to support both range queries and range updates.\nIn Chapter 28 we will see that even this is possible.\n94",
            "Chapter 10\nBit manipulation\nAll data in computer programs is internally stored as bits, i.e., as numbers 0\nand 1. This chapter discusses the bit representation of integers, and shows\nexamples of how to use bit operations. It turns out that there are many uses for\nbit manipulation in algorithm programming.\n10.1 Bit representation\nIn programming, an n bit integer is internally stored as a binary number that\nconsists of n bits. For example, the C++ type int is a 32-bit type, which means\nthat every int number consists of 32 bits.\nHere is the bit representation of the int number 43:\n00000000000000000000000000101011\nThe bits in the representation are indexed from right to left. To convert a bit\nrepresentation bk \u00b7\u00b7\u00b7 b2b1b0 into a number, we can use the formula\nbk2k +... +b222 +b121 +b020.\nFor example,\n1\u00b725 +1\u00b723 +1\u00b721 +1\u00b720 = 43.\nThe bit representation of a number is either signed or unsigned. Usually\na signed representation is used, which means that both negative and positive\nnumbers can be represented. A signed variable of n bits can contain any integer\nbetween \u22122n\u22121 and 2n\u22121 \u22121. For example, the int type in C++ is a signed type,\nso an int variable can contain any integer between \u2212231 and 231 \u22121.\nThe \ufb01rst bit in a signed representation is the sign of the number (0 for\nnonnegative numbers and 1 for negative numbers), and the remaining n \u22121 bits\ncontain the magnitude of the number. Two\u2019s complementis used, which means\nthat the opposite number of a number is calculated by \ufb01rst inverting all the bits\nin the number, and then increasing the number by one.\nFor example, the bit representation of the int number \u221243 is\n11111111111111111111111111010101.\n95",
            "In an unsigned representation, only nonnegative numbers can be used, but\nthe upper bound for the values is larger. An unsigned variable of n bits can\ncontain any integer between 0 and 2n \u22121. For example, in C++, an unsigned int\nvariable can contain any integer between 0 and 232 \u22121.\nThere is a connection between the representations: a signed number \u2212x\nequals an unsigned number 2n \u2212x. For example, the following code shows that\nthe signed number x = \u221243 equals the unsigned number y = 232 \u221243:\nint x = -43;\nunsigned int y = x;\ncout << x << \"\\n\"; // -43\ncout << y << \"\\n\"; // 4294967253\nIf a number is larger than the upper bound of the bit representation, the\nnumber will over\ufb02ow. In a signed representation, the next number after 2n\u22121 \u22121\nis \u22122n\u22121, and in an unsigned representation, the next number after 2n \u22121 is 0.\nFor example, consider the following code:\nint x = 2147483647\ncout << x << \"\\n\"; // 2147483647\nx++;\ncout << x << \"\\n\"; // -2147483648\nInitially, the value of x is 231 \u22121. This is the largest value that can be stored\nin an int variable, so the next number after 231 \u22121 is \u2212231.\n10.2 Bit operations\nAnd operation\nThe and operation x & y produces a number that has one bits in positions where\nboth x and y have one bits. For example, 22 & 26 = 18, because\n10110 (22)\n& 11010 (26)\n= 10010 (18)\nUsing the and operation, we can check if a number x is even because x & 1 =\n0 if x is even, and x & 1 = 1 if x is odd. More generally, x is divisible by 2k exactly\nwhen x & (2k \u22121) = 0.\nOr operation\nThe or operation x | y produces a number that has one bits in positions where at\nleast one of x and y have one bits. For example, 22 | 26 = 30, because\n10110 (22)\n| 11010 (26)\n= 11110 (30)\n96",
            "Xor operation\nThe xor operation x ^ y produces a number that has one bits in positions where\nexactly one of x and y have one bits. For example, 22 ^ 26 = 12, because\n10110 (22)\n^ 11010 (26)\n= 01100 (12)\nNot operation\nThe not operation ~ x produces a number where all the bits of x have been\ninverted. The formula ~x = \u2212x \u22121 holds, for example, ~29 = \u221230.\nThe result of the not operation at the bit level depends on the length of the\nbit representation, because the operation inverts all bits. For example, if the\nnumbers are 32-bit int numbers, the result is as follows:\nx = 29 00000000000000000000000000011101\n~x = \u221230 11111111111111111111111111100010\nBit shifts\nThe left bit shift x << k appends k zero bits to the number, and the right bit\nshift x >> k removes the k last bits from the number. For example, 14<< 2 = 56,\nbecause 14 and 56 correspond to 1110 and 111000. Similarly, 49>> 3 = 6, because\n49 and 6 correspond to 110001 and 110.\nNote that x << k corresponds to multiplying x by 2k, and x >> k corresponds\nto dividing x by 2k rounded down to an integer.\nApplications\nA number of the form 1<< k has a one bit in position k and all other bits are zero,\nso we can use such numbers to access single bits of numbers. In particular, the\nkth bit of a number is one exactly when x & (1 << k) is not zero. The following\ncode prints the bit representation of an int number x:\nfor (int i = 31; i >= 0; i--) {\nif (x&(1<<i)) cout << \"1\";\nelse cout << \"0\";\n}\nIt is also possible to modify single bits of numbers using similar ideas. For\nexample, the formula x | (1 << k) sets the kth bit of x to one, the formula x &\n~(1 << k) sets the kth bit of x to zero, and the formula x ^ (1 << k) inverts the\nkth bit of x.\nThe formula x & (x \u22121) sets the last one bit of x to zero, and the formula x &\n\u2212x sets all the one bits to zero, except for the last one bit. The formula x | (x \u22121)\ninverts all the bits after the last one bit. Also note that a positive number x is a\npower of two exactly when x & (x \u22121) = 0.\n97",
            "Additional functions\nThe g++ compiler provides the following functions for counting bits:\n\u2022 __builtin_clz(x): the number of zeros at the beginning of the number\n\u2022 __builtin_ctz(x): the number of zeros at the end of the number\n\u2022 __builtin_popcount(x): the number of ones in the number\n\u2022 __builtin_parity(x): the parity (even or odd) of the number of ones\nThe functions can be used as follows:\nint x = 5328; // 00000000000000000001010011010000\ncout << __builtin_clz(x) << \"\\n\"; // 19\ncout << __builtin_ctz(x) << \"\\n\"; // 4\ncout << __builtin_popcount(x) << \"\\n\"; // 5\ncout << __builtin_parity(x) << \"\\n\"; // 1\nWhile the above functions only support int numbers, there are also long long\nversions of the functions available with the suf\ufb01x ll.\n10.3 Representing sets\nEvery subset of a set{0,1,2,..., n\u22121} can be represented as an n bit integer whose\none bits indicate which elements belong to the subset. This is an ef\ufb01cient way to\nrepresent sets, because every element requires only one bit of memory, and set\noperations can be implemented as bit operations.\nFor example, since int is a 32-bit type, an int number can represent any\nsubset of the set {0,1,2,..., 31}. The bit representation of the set {1,3,4,8} is\n00000000000000000000000100011010,\nwhich corresponds to the number 28 +24 +23 +21 = 282.\nSet implementation\nThe following code declares an int variable x that can contain a subset of\n{0,1,2,..., 31}. After this, the code adds the elements 1, 3, 4 and 8 to the set\nand prints the size of the set.\nint x = 0;\nx |= (1<<1);\nx |= (1<<3);\nx |= (1<<4);\nx |= (1<<8);\ncout << __builtin_popcount(x) << \"\\n\"; // 4\n98",
            "Then, the following code prints all elements that belong to the set:\nfor (int i = 0; i < 32; i++) {\nif (x&(1<<i)) cout << i << \" \";\n}\n// output: 1 3 4 8\nSet operations\nSet operations can be implemented as follows as bit operations:\nset syntax bit syntax\nintersection a \u2229b a & b\nunion a \u222ab a | b\ncomplement \u00afa ~a\ndifference a \\ b a & (~b)\nFor example, the following code \ufb01rst constructs the sets x = {1,3,4,8} and\ny = {3,6,8,9}, and then constructs the set z = x \u222a y = {1,3,4,6,8,9}:\nint x = (1<<1)|(1<<3)|(1<<4)|(1<<8);\nint y = (1<<3)|(1<<6)|(1<<8)|(1<<9);\nint z = x|y;\ncout << __builtin_popcount(z) << \"\\n\"; // 6\nIterating through subsets\nThe following code goes through the subsets of {0,1,..., n \u22121}:\nfor (int b = 0; b < (1<<n); b++) {\n// process subset b\n}\nThe following code goes through the subsets with exactly k elements:\nfor (int b = 0; b < (1<<n); b++) {\nif (__builtin_popcount(b) == k) {\n// process subset b\n}\n}\nThe following code goes through the subsets of a set x:\nint b = 0;\ndo {\n// process subset b\n} while (b=(b-x)&x);\n99",
            "10.4 Bit optimizations\nMany algorithms can be optimized using bit operations. Such optimizations\ndo not change the time complexity of the algorithm, but they may have a large\nimpact on the actual running time of the code. In this section we discuss examples\nof such situations.\nHamming distances\nThe Hamming distance hamming(a,b) between two strings a and b of equal\nlength is the number of positions where the strings differ. For example,\nhamming(01101,11001) = 2.\nConsider the following problem: Given a list of n bit strings, each of length k,\ncalculate the minimum Hamming distance between two strings in the list. For\nexample, the answer for [00111,01101,11110] is 2, because\n\u2022 hamming(00111,01101) = 2,\n\u2022 hamming(00111,11110) = 3, and\n\u2022 hamming(01101,11110) = 3.\nA straightforward way to solve the problem is to go through all pairs of strings\nand calculate their Hamming distances, which yields an O(n2k) time algorithm.\nThe following function can be used to calculate distances:\nint hamming(string a, string b) {\nint d = 0;\nfor (int i = 0; i < k; i++) {\nif (a[i] != b[i]) d++;\n}\nreturn d;\n}\nHowever, if k is small, we can optimize the code by storing the bit strings\nas integers and calculating the Hamming distances using bit operations. In\nparticular, if k \u2264 32, we can just store the strings as int values and use the\nfollowing function to calculate distances:\nint hamming(int a, int b) {\nreturn __builtin_popcount(a^b);\n}\nIn the above function, the xor operation constructs a bit string that has one bits\nin positions where a and b differ. Then, the number of bits is calculated using\nthe __builtin_popcount function.\nTo compare the implementations, we generated a list of 10000 random bit\nstrings of length 30. Using the \ufb01rst approach, the search took 13.5 seconds, and\nafter the bit optimization, it only took 0.5 seconds. Thus, the bit optimized code\nwas almost 30 times faster than the original code.\n100",
            "Counting subgrids\nAs another example, consider the following problem: Given an n \u00d7n grid whose\neach square is either black (1) or white (0), calculate the number of subgrids\nwhose all corners are black. For example, the grid\ncontains two such subgrids:\nThere is an O(n3) time algorithm for solving the problem: go through all\nO(n2) pairs of rows and for each pair (a,b) calculate the number of columns that\ncontain a black square in both rows in O(n) time. The following code assumes\nthat color[y][x] denotes the color in row y and column x:\nint count = 0;\nfor (int i = 0; i < n; i++) {\nif (color[a][i] == 1 && color[b][i] == 1) count++;\n}\nThen, those columns account for count(count \u22121)/2 subgrids with black corners,\nbecause we can choose any two of them to form a subgrid.\nTo optimize this algorithm, we divide the grid into blocks of columns such that\neach block consists of N consecutive columns. Then, each row is stored as a list\nof N-bit numbers that describe the colors of the squares. Now we can process N\ncolumns at the same time using bit operations. In the following code, color[y][k]\nrepresents a block of N colors as bits.\nint count = 0;\nfor (int i = 0; i <= n/N; i++) {\ncount += __builtin_popcount(color[a][i]&color[b][i]);\n}\nThe resulting algorithm works in O(n3/N) time.\nWe generated a random grid of size 2500\u00d72500 and compared the original\nand bit optimized implementation. While the original code took 29.6 seconds, the\nbit optimized version only took 3.1 seconds with N = 32 (int numbers) and 1.7\nseconds with N = 64 (long long numbers).\n101",
            "10.5 Dynamic programming\nBit operations provide an ef\ufb01cient and convenient way to implement dynamic\nprogramming algorithms whose states contain subsets of elements, because such\nstates can be stored as integers. Next we discuss examples of combining bit\noperations and dynamic programming.\nOptimal selection\nAs a \ufb01rst example, consider the following problem: We are given the prices of k\nproducts over n days, and we want to buy each product exactly once. However,\nwe are allowed to buy at most one product in a day. What is the minimum total\nprice? For example, consider the following scenario (k = 3 and n = 8):\nproduct 0\nproduct 1\nproduct 2\n0 1 2 3 4 5 6 7\n6 9 5 2 8 9 1 6\n8 2 6 2 7 5 7 2\n5 3 9 7 3 5 1 4\nIn this scenario, the minimum total price is 5:\nproduct 0\nproduct 1\nproduct 2\n0 1 2 3 4 5 6 7\n6 9 5 2 8 9 1 6\n8 2 6 2 7 5 7 2\n5 3 9 7 3 5 1 4\nLet price[x][d] denote the price of product x on day d. For example, in the\nabove scenario price[2][3] = 7. Then, let total(S,d) denote the minimum total\nprice for buying a subset S of products by day d. Using this function, the solution\nto the problem is total({0... k \u22121},n \u22121).\nFirst, total(\u2208,d) = 0, because it does not cost anything to buy an empty set,\nand total({x},0) = price[x][0], because there is one way to buy one product on\nthe \ufb01rst day. Then, the following recurrence can be used:\ntotal(S,d) = min(total(S,d \u22121),\nmin\nx\u2208S\n(total(S \\ x,d \u22121)+price[x][d]))\nThis means that we either do not buy any product on day d or buy a product x\nthat belongs to S. In the latter case, we remove x from S and add the price of x\nto the total price.\nThe next step is to calculate the values of the function using dynamic pro-\ngramming. To store the function values, we declare an array\nint total[1<<K][N];\n102",
            "where K and N are suitably large constants. The \ufb01rst dimension of the array\ncorresponds to a bit representation of a subset.\nFirst, the cases where d = 0 can be processed as follows:\nfor (int x = 0; x < k; x++) {\ntotal[1<<x][0] = price[x][0];\n}\nThen, the recurrence translates into the following code:\nfor (int d = 1; d < n; d++) {\nfor (int s = 0; s < (1<<k); s++) {\ntotal[s][d] = total[s][d-1];\nfor (int x = 0; x < k; x++) {\nif (s&(1<<x)) {\ntotal[s][d] = min(total[s][d],\ntotal[s^(1<<x)][d-1]+price[x][d]);\n}\n}\n}\n}\nThe time complexity of the algorithm is O(n2kk).\nFrom permutations to subsets\nUsing dynamic programming, it is often possible to change an iteration over\npermutations into an iteration over subsets1. The bene\ufb01t of this is that n!, the\nnumber of permutations, is much larger than 2 n, the number of subsets. For\nexample, if n = 20, then n! \u2248 2.4\u00b71018 and 2n \u2248 106. Thus, for certain values of n,\nwe can ef\ufb01ciently go through the subsets but not through the permutations.\nAs an example, consider the following problem: There is an elevator with\nmaximum weight x, and n people with known weights who want to get from the\nground \ufb02oor to the top \ufb02oor. What is the minimum number of rides needed if the\npeople enter the elevator in an optimal order?\nFor example, suppose that x = 10, n = 5 and the weights are as follows:\nperson weight\n0 2\n1 3\n2 3\n3 5\n4 6\nIn this case, the minimum number of rides is 2. One optimal order is {0,2,3,1,4},\nwhich partitions the people into two rides: \ufb01rst {0,2,3} (total weight 10), and then\n{1,4} (total weight 9).\n1This technique was introduced in 1962 by M. Held and R. M. Karp [34].\n103",
            "The problem can be easily solved in O(n!n) time by testing all possible permu-\ntations of n people. However, we can use dynamic programming to get a more\nef\ufb01cient O(2nn) time algorithm. The idea is to calculate for each subset of people\ntwo values: the minimum number of rides needed and the minimum weight of\npeople who ride in the last group.\nLet weight[p] denote the weight of personp. We de\ufb01ne two functions: rides(S)\nis the minimum number of rides for a subset S, and last(S) is the minimum\nweight of the last ride. For example, in the above scenario\nrides({1,3,4}) = 2 and last({1,3,4}) = 5,\nbecause the optimal rides are {1,4} and {3}, and the second ride has weight 5. Of\ncourse, our \ufb01nal goal is to calculate the value of rides({0... n \u22121}).\nWe can calculate the values of the functions recursively and then apply\ndynamic programming. The idea is to go through all people who belong to S and\noptimally choose the last person p who enters the elevator. Each such choice\nyields a subproblem for a smaller subset of people. If last(S \\ p)+weight[p] \u2264 x,\nwe can add p to the last ride. Otherwise, we have to reserve a new ride that\ninitially only contains p.\nTo implement dynamic programming, we declare an array\npair<int,int> best[1<<N];\nthat contains for each subset S a pair (rides(S),last(S)). We set the value for an\nempty group as follows:\nbest[0] = {1,0};\nThen, we can \ufb01ll the array as follows:\nfor (int s = 1; s < (1<<n); s++) {\n// initial value: n+1 rides are needed\nbest[s] = {n+1,0};\nfor (int p = 0; p < n; p++) {\nif (s&(1<<p)) {\nauto option = best[s^(1<<p)];\nif (option.second+weight[p] <= x) {\n// add p to an existing ride\noption.second += weight[p];\n} else {\n// reserve a new ride for p\noption.first++;\noption.second = weight[p];\n}\nbest[s] = min(best[s], option);\n}\n}\n}\n104",
            "Note that the above loop guarantees that for any two subsets S1 and S2 such\nthat S1 \u2282 S2, we process S1 before S2. Thus, the dynamic programming values\nare calculated in the correct order.\nCounting subsets\nOur last problem in this chapter is as follows: Let X = {0... n\u22121}, and each subset\nS \u2282 X is assigned an integer value[S]. Our task is to calculate for each S\nsum(S) =\n\u2211\nA\u2282S\nvalue[A],\ni.e., the sum of values of subsets of S.\nFor example, suppose that n = 3 and the values are as follows:\n\u2022 value[\u2208] = 3\n\u2022 value[{0}] = 1\n\u2022 value[{1}] = 4\n\u2022 value[{0,1}] = 5\n\u2022 value[{2}] = 5\n\u2022 value[{0,2}] = 1\n\u2022 value[{1,2}] = 3\n\u2022 value[{0,1,2}] = 3\nIn this case, for example,\nsum({0,2}) = value[\u2208]+value[{0}]+value[{2}]+value[{0,2}]\n= 3+1+5+1 = 10.\nBecause there are a total of 2n subsets, one possible solution is to go through\nall pairs of subsets in O(22n) time. However, using dynamic programming, we\ncan solve the problem in O(2nn) time. The idea is to focus on sums where the\nelements that may be removed from S are restricted.\nLet partial(S,k) denote the sum of values of subsets of S with the restriction\nthat only elements 0... k may be removed from S. For example,\npartial({0,2},1) = value[{2}]+value[{0,2}],\nbecause we may only remove elements 0... 1. We can calculate values ofsum using\nvalues of partial, because\nsum(S) = partial(S,n \u22121).\nThe base cases for the function are\npartial(S,\u22121) = value[S],\nbecause in this case no elements can be removed from S. Then, in the general\ncase we can use the following recurrence:\npartial(S,k) =\n{\npartial(S,k \u22121) k \u2209 S\npartial(S,k \u22121)+partial(S \\ {k},k \u22121) k \u2208 S\n105",
            "Here we focus on the element k. If k \u2208 S, we have two options: we may either\nkeep k in S or remove it from S.\nThere is a particularly clever way to implement the calculation of sums. We\ncan declare an array\nint sum[1<<N];\nthat will contain the sum of each subset. The array is initialized as follows:\nfor (int s = 0; s < (1<<n); s++) {\nsum[s] = value[s];\n}\nThen, we can \ufb01ll the array as follows:\nfor (int k = 0; k < n; k++) {\nfor (int s = 0; s < (1<<n); s++) {\nif (s&(1<<k)) sum[s] += sum[s^(1<<k)];\n}\n}\nThis code calculates the values of partial(S,k) for k = 0... n \u22121 to the array sum.\nSince partial(S,k) is always based on partial(S,k \u22121), we can reuse the array\nsum, which yields a very ef\ufb01cient implementation.\n106",
            "Part II\nGraph algorithms\n107",
            "",
            "Chapter 11\nBasics of graphs\nMany programming problems can be solved by modeling the problem as a graph\nproblem and using an appropriate graph algorithm. A typical example of a graph\nis a network of roads and cities in a country. Sometimes, though, the graph is\nhidden in the problem and it may be dif\ufb01cult to detect it.\nThis part of the book discusses graph algorithms, especially focusing on topics\nthat are important in competitive programming. In this chapter, we go through\nconcepts related to graphs, and study different ways to represent graphs in\nalgorithms.\n11.1 Graph terminology\nA graph consists of nodes and edges. In this book, the variable n denotes the\nnumber of nodes in a graph, and the variable m denotes the number of edges.\nThe nodes are numbered using integers 1,2,..., n.\nFor example, the following graph consists of 5 nodes and 7 edges:\n1 2\n3 4\n5\nA path leads from node a to node b through edges of the graph. The length\nof a path is the number of edges in it. For example, the above graph contains a\npath 1 \u2192 3 \u2192 4 \u2192 5 of length 3 from node 1 to node 5:\n1 2\n3 4\n5\nA path is a cycle if the \ufb01rst and last node is the same. For example, the above\ngraph contains a cycle 1 \u2192 3 \u2192 4 \u2192 1. A path is simple if each node appears at\nmost once in the path.\n109",
            "Connectivity\nA graph is connected if there is a path between any two nodes. For example,\nthe following graph is connected:\n1 2\n3 4\nThe following graph is not connected, because it is not possible to get from\nnode 4 to any other node:\n1 2\n3 4\nThe connected parts of a graph are called its components. For example, the\nfollowing graph contains three components: {1, 2, 3}, {4, 5, 6, 7} and {8}.\n1 2\n3 6 7\n4 5\n8\nA tree is a connected graph that consists of n nodes and n \u22121 edges. There is\na unique path between any two nodes of a tree. For example, the following graph\nis a tree:\n1 2\n3 4\n5\nEdge directions\nA graph is directed if the edges can be traversed in one direction only. For\nexample, the following graph is directed:\n1 2\n3 4\n5\nThe above graph contains a path 3 \u2192 1 \u2192 2 \u2192 5 from node 3 to node 5, but\nthere is no path from node 5 to node 3.\n110",
            "Edge weights\nIn a weighted graph, each edge is assigned a weight. The weights are often\ninterpreted as edge lengths. For example, the following graph is weighted:\n1 2\n3 4\n5\n5\n1\n7\n6\n7\n3\nThe length of a path in a weighted graph is the sum of the edge weights on\nthe path. For example, in the above graph, the length of the path 1 \u2192 2 \u2192 5 is 12,\nand the length of the path 1 \u2192 3 \u2192 4 \u2192 5 is 11. The latter path is the shortest\npath from node 1 to node 5.\nNeighbors and degrees\nTwo nodes are neighbors or adjacent if there is an edge between them. The\ndegree of a node is the number of its neighbors. For example, in the following\ngraph, the neighbors of node 2 are 1, 4 and 5, so its degree is 3.\n1 2\n3 4\n5\nThe sum of degrees in a graph is always 2m, where m is the number of edges,\nbecause each edge increases the degree of exactly two nodes by one. For this\nreason, the sum of degrees is always even.\nA graph is regular if the degree of every node is a constant d. A graph is\ncomplete if the degree of every node is n \u22121, i.e., the graph contains all possible\nedges between the nodes.\nIn a directed graph, the indegree of a node is the number of edges that end\nat the node, and the outdegree of a node is the number of edges that start at\nthe node. For example, in the following graph, the indegree of node 2 is 2, and\nthe outdegree of node 2 is 1.\n1 2\n3 4\n5\n111",
            "Colorings\nIn a coloring of a graph, each node is assigned a color so that no adjacent nodes\nhave the same color.\nA graph is bipartite if it is possible to color it using two colors. It turns out\nthat a graph is bipartite exactly when it does not contain a cycle with an odd\nnumber of edges. For example, the graph\n2 3\n5 64\n1\nis bipartite, because it can be colored as follows:\n2 3\n5 64\n1\nHowever, the graph\n2 3\n5 64\n1\nis not bipartite, because it is not possible to color the following cycle of three\nnodes using two colors:\n2 3\n5 64\n1\nSimplicity\nA graph is simple if no edge starts and ends at the same node, and there are no\nmultiple edges between two nodes. Often we assume that graphs are simple. For\nexample, the following graph is not simple:\n2 3\n5 64\n1\n112",
            "11.2 Graph representation\nThere are several ways to represent graphs in algorithms. The choice of a data\nstructure depends on the size of the graph and the way the algorithm processes\nit. Next we will go through three common representations.\nAdjacency list representation\nIn the adjacency list representation, each node x in the graph is assigned an\nadjacency list that consists of nodes to which there is an edge fromx. Adjacency\nlists are the most popular way to represent graphs, and most algorithms can be\nef\ufb01ciently implemented using them.\nA convenient way to store the adjacency lists is to declare an array of vectors\nas follows:\nvector<int> adj[N];\nThe constant N is chosen so that all adjacency lists can be stored. For example,\nthe graph\n1 2 3\n4\ncan be stored as follows:\nadj[1].push_back(2);\nadj[2].push_back(3);\nadj[2].push_back(4);\nadj[3].push_back(4);\nadj[4].push_back(1);\nIf the graph is undirected, it can be stored in a similar way, but each edge is\nadded in both directions.\nFor a weighted graph, the structure can be extended as follows:\nvector<pair<int,int>> adj[N];\nIn this case, the adjacency list of node a contains the pair (b,w) always when\nthere is an edge from node a to node b with weight w. For example, the graph\n1 2 3\n4\n5 7\n6 52\n113",
            "can be stored as follows:\nadj[1].push_back({2,5});\nadj[2].push_back({3,7});\nadj[2].push_back({4,6});\nadj[3].push_back({4,5});\nadj[4].push_back({1,2});\nThe bene\ufb01t of using adjacency lists is that we can ef\ufb01ciently \ufb01nd the nodes\nto which we can move from a given node through an edge. For example, the\nfollowing loop goes through all nodes to which we can move from node s:\nfor (auto u : adj[s]) {\n// process node u\n}\nAdjacency matrix representation\nAn adjacency matrix is a two-dimensional array that indicates which edges\nthe graph contains. We can ef\ufb01ciently check from an adjacency matrix if there is\nan edge between two nodes. The matrix can be stored as an array\nint adj[N][N];\nwhere each value adj[a][b] indicates whether the graph contains an edge from\nnode a to node b. If the edge is included in the graph, then adj[a][b] = 1, and\notherwise adj[a][b] = 0. For example, the graph\n1 2 3\n4\ncan be represented as follows:\n1 0 0 0\n0 0 0 1\n0 0 1 1\n0 1 0 0\n4\n3\n2\n1\n1 2 3 4\nIf the graph is weighted, the adjacency matrix representation can be extended\nso that the matrix contains the weight of the edge if the edge exists. Using this\nrepresentation, the graph\n114",
            "1 2 3\n4\n5 7\n6 52\ncorresponds to the following matrix:\n2 0 0 0\n0 0 0 5\n0 0 7 6\n0 5 0 0\n4\n3\n2\n1\n1 2 3 4\nThe drawback of the adjacency matrix representation is that the matrix\ncontains n2 elements, and usually most of them are zero. For this reason, the\nrepresentation cannot be used if the graph is large.\nEdge list representation\nAn edge list contains all edges of a graph in some order. This is a convenient\nway to represent a graph if the algorithm processes all edges of the graph and it\nis not needed to \ufb01nd edges that start at a given node.\nThe edge list can be stored in a vector\nvector<pair<int,int>> edges;\nwhere each pair (a,b) denotes that there is an edge from node a to node b. Thus,\nthe graph\n1 2 3\n4\ncan be represented as follows:\nedges.push_back({1,2});\nedges.push_back({2,3});\nedges.push_back({2,4});\nedges.push_back({3,4});\nedges.push_back({4,1});\nIf the graph is weighted, the structure can be extended as follows:\n115",
            "vector<tuple<int,int,int>> edges;\nEach element in this list is of the form ( a,b,w), which means that there is an\nedge from node a to node b with weight w. For example, the graph\n1 2 3\n4\n5 7\n6 52\ncan be represented as follows1:\nedges.push_back({1,2,5});\nedges.push_back({2,3,7});\nedges.push_back({2,4,6});\nedges.push_back({3,4,5});\nedges.push_back({4,1,2});\n1In some older compilers, the function make_tuple must be used instead of the braces (for\nexample, make_tuple(1,2,5) instead of {1,2,5}).\n116",
            "Chapter 12\nGraph traversal\nThis chapter discusses two fundamental graph algorithms: depth-\ufb01rst search and\nbreadth-\ufb01rst search. Both algorithms are given a starting node in the graph, and\nthey visit all nodes that can be reached from the starting node. The difference in\nthe algorithms is the order in which they visit the nodes.\n12.1 Depth-\ufb01rst search\nDepth-\ufb01rst search (DFS) is a straightforward graph traversal technique. The\nalgorithm begins at a starting node, and proceeds to all other nodes that are\nreachable from the starting node using the edges of the graph.\nDepth-\ufb01rst search always follows a single path in the graph as long as it\n\ufb01nds new nodes. After this, it returns to previous nodes and begins to explore\nother parts of the graph. The algorithm keeps track of visited nodes, so that it\nprocesses each node only once.\nExample\nLet us consider how depth-\ufb01rst search processes the following graph:\n1 2\n3\n4 5\nWe may begin the search at any node of the graph; now we will begin the search\nat node 1.\nThe search \ufb01rst proceeds to node 2:\n1 2\n3\n4 5\n117",
            "After this, nodes 3 and 5 will be visited:\n1 2\n3\n4 5\nThe neighbors of node 5 are 2 and 3, but the search has already visited both of\nthem, so it is time to return to the previous nodes. Also the neighbors of nodes 3\nand 2 have been visited, so we next move from node 1 to node 4:\n1 2\n3\n4 5\nAfter this, the search terminates because it has visited all nodes.\nThe time complexity of depth-\ufb01rst search is O(n +m) where n is the number\nof nodes and m is the number of edges, because the algorithm processes each\nnode and edge once.\nImplementation\nDepth-\ufb01rst search can be conveniently implemented using recursion. The fol-\nlowing function dfs begins a depth-\ufb01rst search at a given node. The function\nassumes that the graph is stored as adjacency lists in an array\nvector<int> adj[N];\nand also maintains an array\nbool visited[N];\nthat keeps track of the visited nodes. Initially, each array value is false, and\nwhen the search arrives at node s, the value of visited[s] becomes true. The\nfunction can be implemented as follows:\nvoid dfs(int s) {\nif (visited[s]) return;\nvisited[s] = true;\n// process node s\nfor (auto u: adj[s]) {\ndfs(u);\n}\n}\n118",
            "12.2 Breadth-\ufb01rst search\nBreadth-\ufb01rst search (BFS) visits the nodes in increasing order of their distance\nfrom the starting node. Thus, we can calculate the distance from the starting\nnode to all other nodes using breadth-\ufb01rst search. However, breadth-\ufb01rst search\nis more dif\ufb01cult to implement than depth-\ufb01rst search.\nBreadth-\ufb01rst search goes through the nodes one level after another. First the\nsearch explores the nodes whose distance from the starting node is 1, then the\nnodes whose distance is 2, and so on. This process continues until all nodes have\nbeen visited.\nExample\nLet us consider how breadth-\ufb01rst search processes the following graph:\n1 2 3\n4 5 6\nSuppose that the search begins at node 1. First, we process all nodes that can be\nreached from node 1 using a single edge:\n1 2 3\n4 5 6\nAfter this, we proceed to nodes 3 and 5:\n1 2 3\n4 5 6\nFinally, we visit node 6:\n1 2 3\n4 5 6\n119",
            "Now we have calculated the distances from the starting node to all nodes of the\ngraph. The distances are as follows:\nnode distance\n1 0\n2 1\n3 2\n4 1\n5 2\n6 3\nLike in depth-\ufb01rst search, the time complexity of breadth-\ufb01rst search is\nO(n +m), where n is the number of nodes and m is the number of edges.\nImplementation\nBreadth-\ufb01rst search is more dif\ufb01cult to implement than depth-\ufb01rst search, be-\ncause the algorithm visits nodes in different parts of the graph. A typical imple-\nmentation is based on a queue that contains nodes. At each step, the next node\nin the queue will be processed.\nThe following code assumes that the graph is stored as adjacency lists and\nmaintains the following data structures:\nqueue<int> q;\nbool visited[N];\nint distance[N];\nThe queue q contains nodes to be processed in increasing order of their\ndistance. New nodes are always added to the end of the queue, and the node at\nthe beginning of the queue is the next node to be processed. The array visited\nindicates which nodes the search has already visited, and the array distance will\ncontain the distances from the starting node to all nodes of the graph.\nThe search can be implemented as follows, starting at node x:\nvisited[x] = true;\ndistance[x] = 0;\nq.push(x);\nwhile (!q.empty()) {\nint s = q.front(); q.pop();\n// process node s\nfor (auto u : adj[s]) {\nif (visited[u]) continue;\nvisited[u] = true;\ndistance[u] = distance[s]+1;\nq.push(u);\n}\n}\n120",
            "12.3 Applications\nUsing the graph traversal algorithms, we can check many properties of graphs.\nUsually, both depth-\ufb01rst search and breadth-\ufb01rst search may be used, but in\npractice, depth-\ufb01rst search is a better choice, because it is easier to implement.\nIn the following applications we will assume that the graph is undirected.\nConnectivity check\nA graph is connected if there is a path between any two nodes of the graph. Thus,\nwe can check if a graph is connected by starting at an arbitrary node and \ufb01nding\nout if we can reach all other nodes.\nFor example, in the graph\n21\n3\n54\na depth-\ufb01rst search from node 1 visits the following nodes:\n21\n3\n54\nSince the search did not visit all the nodes, we can conclude that the graph\nis not connected. In a similar way, we can also \ufb01nd all connected components of\na graph by iterating through the nodes and always starting a new depth-\ufb01rst\nsearch if the current node does not belong to any component yet.\nFinding cycles\nA graph contains a cycle if during a graph traversal, we \ufb01nd a node whose\nneighbor (other than the previous node in the current path) has already been\nvisited. For example, the graph\n21\n3\n54\ncontains two cycles and we can \ufb01nd one of them as follows:\n121",
            "21\n3\n54\nAfter moving from node 2 to node 5 we notice that the neighbor 3 of node 5 has\nalready been visited. Thus, the graph contains a cycle that goes through node 3,\nfor example, 3 \u2192 2 \u2192 5 \u2192 3.\nAnother way to \ufb01nd out whether a graph contains a cycle is to simply calculate\nthe number of nodes and edges in every component. If a component contains c\nnodes and no cycle, it must contain exactly c \u22121 edges (so it has to be a tree). If\nthere are c or more edges, the component surely contains a cycle.\nBipartiteness check\nA graph is bipartite if its nodes can be colored using two colors so that there are\nno adjacent nodes with the same color. It is surprisingly easy to check if a graph\nis bipartite using graph traversal algorithms.\nThe idea is to color the starting node blue, all its neighbors red, all their\nneighbors blue, and so on. If at some point of the search we notice that two\nadjacent nodes have the same color, this means that the graph is not bipartite.\nOtherwise the graph is bipartite and one coloring has been found.\nFor example, the graph\n21\n3\n54\nis not bipartite, because a search from node 1 proceeds as follows:\n21\n3\n54\nWe notice that the color or both nodes 2 and 5 is red, while they are adjacent\nnodes in the graph. Thus, the graph is not bipartite.\nThis algorithm always works, because when there are only two colors avail-\nable, the color of the starting node in a component determines the colors of all\nother nodes in the component. It does not make any difference whether the\nstarting node is red or blue.\nNote that in the general case, it is dif\ufb01cult to \ufb01nd out if the nodes in a graph\ncan be colored using k colors so that no adjacent nodes have the same color. Even\nwhen k = 3, no ef\ufb01cient algorithm is known but the problem is NP-hard.\n122",
            "Chapter 13\nShortest paths\nFinding a shortest path between two nodes of a graph is an important problem\nthat has many practical applications. For example, a natural problem related to\na road network is to calculate the shortest possible length of a route between two\ncities, given the lengths of the roads.\nIn an unweighted graph, the length of a path equals the number of its edges,\nand we can simply use breadth-\ufb01rst search to \ufb01nd a shortest path. However, in\nthis chapter we focus on weighted graphs where more sophisticated algorithms\nare needed for \ufb01nding shortest paths.\n13.1 Bellman\u2013Ford algorithm\nThe Bellman\u2013Ford algorithm1 \ufb01nds shortest paths from a starting node to all\nnodes of the graph. The algorithm can process all kinds of graphs, provided that\nthe graph does not contain a cycle with negative length. If the graph contains a\nnegative cycle, the algorithm can detect this.\nThe algorithm keeps track of distances from the starting node to all nodes\nof the graph. Initially, the distance to the starting node is 0 and the distance to\nall other nodes in in\ufb01nite. The algorithm reduces the distances by \ufb01nding edges\nthat shorten the paths until it is not possible to reduce any distance.\nExample\nLet us consider how the Bellman\u2013Ford algorithm works in the following graph:\n1 2\n3 4\n6\n0 \u221e\n\u221e \u221e\n\u221e\n5\n3\n1\n3\n2\n2\n7\n1The algorithm is named after R. E. Bellman and L. R. Ford who published it independently\nin 1958 and 1956, respectively [5, 24].\n123",
            "Each node of the graph is assigned a distance. Initially, the distance to the\nstarting node is 0, and the distance to all other nodes is in\ufb01nite.\nThe algorithm searches for edges that reduce distances. First, all edges from\nnode 1 reduce distances:\n1 2\n3 4\n5\n0 5\n3 7\n\u221e\n5\n3\n1\n3\n2\n2\n7\nAfter this, edges 2 \u2192 5 and 3 \u2192 4 reduce distances:\n1 2\n3 4\n5\n0 5\n3 4\n7\n5\n3\n1\n3\n2\n2\n7\nFinally, there is one more change:\n1 2\n3 4\n5\n0 5\n3 4\n6\n5\n3\n1\n3\n2\n2\n7\nAfter this, no edge can reduce any distance. This means that the distances\nare \ufb01nal, and we have successfully calculated the shortest distances from the\nstarting node to all nodes of the graph.\nFor example, the shortest distance 3 from node 1 to node 5 corresponds to the\nfollowing path:\n1 2\n3 4\n5\n0 5\n3 4\n6\n5\n3\n1\n3\n2\n2\n7\n124",
            "Implementation\nThe following implementation of the Bellman\u2013Ford algorithm determines the\nshortest distances from a node x to all nodes of the graph. The code assumes\nthat the graph is stored as an edge list edges that consists of tuples of the form\n(a,b,w), meaning that there is an edge from node a to node b with weight w.\nThe algorithm consists of n \u22121 rounds, and on each round the algorithm goes\nthrough all edges of the graph and tries to reduce the distances. The algorithm\nconstructs an array distance that will contain the distances from x to all nodes\nof the graph. The constant INF denotes an in\ufb01nite distance.\nfor (int i = 1; i <= n; i++) distance[i] = INF;\ndistance[x] = 0;\nfor (int i = 1; i <= n-1; i++) {\nfor (auto e : edges) {\nint a, b, w;\ntie(a, b, w) = e;\ndistance[b] = min(distance[b], distance[a]+w);\n}\n}\nThe time complexity of the algorithm isO(nm), because the algorithm consists\nof n \u22121 rounds and iterates through all m edges during a round. If there are no\nnegative cycles in the graph, all distances are \ufb01nal after n \u22121 rounds, because\neach shortest path can contain at most n \u22121 edges.\nIn practice, the \ufb01nal distances can usually be found faster than inn\u22121 rounds.\nThus, a possible way to make the algorithm more ef\ufb01cient is to stop the algorithm\nif no distance can be reduced during a round.\nNegative cycles\nThe Bellman\u2013Ford algorithm can also be used to check if the graph contains a\ncycle with negative length. For example, the graph\n1\n2\n3\n4\n3 1\n5 \u22127\n2\ncontains a negative cycle 2 \u2192 3 \u2192 4 \u2192 2 with length \u22124.\nIf the graph contains a negative cycle, we can shorten in\ufb01nitely many times\nany path that contains the cycle by repeating the cycle again and again. Thus,\nthe concept of a shortest path is not meaningful in this situation.\nA negative cycle can be detected using the Bellman\u2013Ford algorithm by running\nthe algorithm for n rounds. If the last round reduces any distance, the graph\ncontains a negative cycle. Note that this algorithm can be used to search for a\nnegative cycle in the whole graph regardless of the starting node.\n125",
            "SPFA algorithm\nThe SPFA algorithm (\u201dShortest Path Faster Algorithm\u201d) [20] is a variant of the\nBellman\u2013Ford algorithm, that is often more ef\ufb01cient than the original algorithm.\nThe SPFA algorithm does not go through all the edges on each round, but instead,\nit chooses the edges to be examined in a more intelligent way.\nThe algorithm maintains a queue of nodes that might be used for reducing\nthe distances. First, the algorithm adds the starting node x to the queue. Then,\nthe algorithm always processes the \ufb01rst node in the queue, and when an edge\na \u2192 b reduces a distance, node b is added to the queue.\nThe ef\ufb01ciency of the SPFA algorithm depends on the structure of the graph:\nthe algorithm is often ef\ufb01cient, but its worst case time complexity is still O(nm)\nand it is possible to create inputs that make the algorithm as slow as the original\nBellman\u2013Ford algorithm.\n13.2 Dijkstra\u2019s algorithm\nDijkstra\u2019s algorithm2 \ufb01nds shortest paths from the starting node to all nodes of\nthe graph, like the Bellman\u2013Ford algorithm. The bene\ufb01t of Dijsktra\u2019s algorithm\nis that it is more ef\ufb01cient and can be used for processing large graphs. However,\nthe algorithm requires that there are no negative weight edges in the graph.\nLike the Bellman\u2013Ford algorithm, Dijkstra\u2019s algorithm maintains distances\nto the nodes and reduces them during the search. Dijkstra\u2019s algorithm is ef\ufb01cient,\nbecause it only processes each edge in the graph once, using the fact that there\nare no negative edges.\nExample\nLet us consider how Dijkstra\u2019s algorithm works in the following graph when the\nstarting node is node 1:\n3 4\n2 1\n5\n\u221e \u221e\n\u221e 0\n\u221e\n6\n2\n5\n9\n2\n1\nLike in the Bellman\u2013Ford algorithm, initially the distance to the starting node is\n0 and the distance to all other nodes is in\ufb01nite.\nAt each step, Dijkstra\u2019s algorithm selects a node that has not been processed\nyet and whose distance is as small as possible. The \ufb01rst such node is node 1 with\ndistance 0.\n2E. W. Dijkstra published the algorithm in 1959 [ 14]; however, his original paper does not\nmention how to implement the algorithm ef\ufb01ciently.\n126",
            "When a node is selected, the algorithm goes through all edges that start at\nthe node and reduces the distances using them:\n3 4\n2 1\n5\n\u221e 9\n5 0\n1\n6\n2\n5\n9\n2\n1\nIn this case, the edges from node 1 reduced the distances of nodes 2, 4 and 5,\nwhose distances are now 5, 9 and 1.\nThe next node to be processed is node 5 with distance 1. This reduces the\ndistance to node 4 from 9 to 3:\n3 4\n2 1\n5\n\u221e 3\n5 0\n1\n6\n2\n5\n9\n2\n1\nAfter this, the next node is node 4, which reduces the distance to node 3 to 9:\n3 4\n2 1\n5\n9 3\n5 0\n1\n6\n2\n5\n9\n2\n1\nA remarkable property in Dijkstra\u2019s algorithm is that whenever a node is\nselected, its distance is \ufb01nal. For example, at this point of the algorithm, the\ndistances 0, 1 and 3 are the \ufb01nal distances to nodes 1, 5 and 4.\nAfter this, the algorithm processes the two remaining nodes, and the \ufb01nal\ndistances are as follows:\n3 4\n2 1\n5\n7 3\n5 0\n1\n6\n2\n5\n9\n2\n1\n127",
            "Negative edges\nThe ef\ufb01ciency of Dijkstra\u2019s algorithm is based on the fact that the graph does\nnot contain negative edges. If there is a negative edge, the algorithm may give\nincorrect results. As an example, consider the following graph:\n1\n2\n3\n4\n2 3\n6 \u22125\nThe shortest path from node 1 to node 4 is 1 \u2192 3 \u2192 4 and its length is 1. However,\nDijkstra\u2019s algorithm \ufb01nds the path 1\u2192 2 \u2192 4 by following the minimum weight\nedges. The algorithm does not take into account that on the other path, the\nweight \u22125 compensates the previous large weight 6.\nImplementation\nThe following implementation of Dijkstra\u2019s algorithm calculates the minimum\ndistances from a node x to other nodes of the graph. The graph is stored as\nadjacency lists so that adj[a] contains a pair (b,w) always when there is an edge\nfrom node a to node b with weight w.\nAn ef\ufb01cient implementation of Dijkstra\u2019s algorithm requires that it is possible\nto ef\ufb01ciently \ufb01nd the minimum distance node that has not been processed. An\nappropriate data structure for this is a priority queue that contains the nodes\nordered by their distances. Using a priority queue, the next node to be processed\ncan be retrieved in logarithmic time.\nIn the following code, the priority queue q contains pairs of the form (\u2212d, x),\nmeaning that the current distance to node x is d. The array distance contains\nthe distance to each node, and the array processed indicates whether a node has\nbeen processed. Initially the distance is 0 to x and \u221e to all other nodes.\nfor (int i = 1; i <= n; i++) distance[i] = INF;\ndistance[x] = 0;\nq.push({0,x});\nwhile (!q.empty()) {\nint a = q.top().second; q.pop();\nif (processed[a]) continue;\nprocessed[a] = true;\nfor (auto u : adj[a]) {\nint b = u.first, w = u.second;\nif (distance[a]+w < distance[b]) {\ndistance[b] = distance[a]+w;\nq.push({-distance[b],b});\n}\n}\n}\n128",
            "Note that the priority queue contains negative distances to nodes. The reason\nfor this is that the default version of the C++ priority queue \ufb01nds maximum\nelements, while we want to \ufb01nd minimum elements. By using negative distances,\nwe can directly use the default priority queue 3. Also note that there may be\nseveral instances of the same node in the priority queue; however, only the\ninstance with the minimum distance will be processed.\nThe time complexity of the above implementation is O(n +mlogm), because\nthe algorithm goes through all nodes of the graph and adds for each edge at most\none distance to the priority queue.\n13.3 Floyd\u2013Warshall algorithm\nThe Floyd\u2013Warshall algorithm4 provides an alternative way to approach the\nproblem of \ufb01nding shortest paths. Unlike the other algorithms of this chapter, it\n\ufb01nds all shortest paths between the nodes in a single run.\nThe algorithm maintains a two-dimensional array that contains distances\nbetween the nodes. First, distances are calculated only using direct edges between\nthe nodes, and after this, the algorithm reduces distances by using intermediate\nnodes in paths.\nExample\nLet us consider how the Floyd\u2013Warshall algorithm works in the following graph:\n3 4\n2 1\n5\n7\n2\n5\n9\n2\n1\nInitially, the distance from each node to itself is 0, and the distance between\nnodes a and b is x if there is an edge between nodes a and b with weight x. All\nother distances are in\ufb01nite.\nIn this graph, the initial array is as follows:\n1 2 3 4 5\n1 0 5 \u221e 9 1\n2 5 0 2 \u221e \u221e\n3 \u221e 2 0 7 \u221e\n4 9 \u221e 7 0 2\n5 1 \u221e \u221e 2 0\n3Of course, we could also declare the priority queue as in Chapter 4.5 and use positive distances,\nbut the implementation would be a bit longer.\n4The algorithm is named after R. W. Floyd and S. Warshall who published it independently in\n1962 [23, 70].\n129",
            "The algorithm consists of consecutive rounds. On each round, the algorithm\nselects a new node that can act as an intermediate node in paths from now on,\nand distances are reduced using this node.\nOn the \ufb01rst round, node 1 is the new intermediate node. There is a new path\nbetween nodes 2 and 4 with length 14, because node 1 connects them. There is\nalso a new path between nodes 2 and 5 with length 6.\n1 2 3 4 5\n1 0 5 \u221e 9 1\n2 5 0 2 14 6\n3 \u221e 2 0 7 \u221e\n4 9 14 7 0 2\n5 1 6 \u221e 2 0\nOn the second round, node 2 is the new intermediate node. This creates new\npaths between nodes 1 and 3 and between nodes 3 and 5:\n1 2 3 4 5\n1 0 5 7 9 1\n2 5 0 2 14 6\n3 7 2 0 7 8\n4 9 14 7 0 2\n5 1 6 8 2 0\nOn the third round, node 3 is the new intermediate round. There is a new\npath between nodes 2 and 4:\n1 2 3 4 5\n1 0 5 7 9 1\n2 5 0 2 9 6\n3 7 2 0 7 8\n4 9 9 7 0 2\n5 1 6 8 2 0\nThe algorithm continues like this, until all nodes have been appointed inter-\nmediate nodes. After the algorithm has \ufb01nished, the array contains the minimum\ndistances between any two nodes:\n1 2 3 4 5\n1 0 5 7 3 1\n2 5 0 2 8 6\n3 7 2 0 7 8\n4 3 8 7 0 2\n5 1 6 8 2 0\nFor example, the array tells us that the shortest distance between nodes 2\nand 4 is 8. This corresponds to the following path:\n130",
            "3 4\n2 1\n5\n7\n2\n5\n9\n2\n1\nImplementation\nThe advantage of the Floyd\u2013Warshall algorithm that it is easy to implement. The\nfollowing code constructs a distance matrix where distance[a][b] is the shortest\ndistance between nodes a and b. First, the algorithm initializes distance using\nthe adjacency matrix adj of the graph:\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= n; j++) {\nif (i == j) distance[i][j] = 0;\nelse if (adj[i][j]) distance[i][j] = adj[i][j];\nelse distance[i][j] = INF;\n}\n}\nAfter this, the shortest distances can be found as follows:\nfor (int k = 1; k <= n; k++) {\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= n; j++) {\ndistance[i][j] = min(distance[i][j],\ndistance[i][k]+distance[k][j]);\n}\n}\n}\nThe time complexity of the algorithm is O(n3), because it contains three\nnested loops that go through the nodes of the graph.\nSince the implementation of the Floyd\u2013Warshall algorithm is simple, the\nalgorithm can be a good choice even if it is only needed to \ufb01nd a single shortest\npath in the graph. However, the algorithm can only be used when the graph is so\nsmall that a cubic time complexity is fast enough.\n131",
            "132",
            "Chapter 14\nTree algorithms\nA tree is a connected, acyclic graph that consists of n nodes and n \u22121 edges.\nRemoving any edge from a tree divides it into two components, and adding any\nedge to a tree creates a cycle. Moreover, there is always a unique path between\nany two nodes of a tree.\nFor example, the following tree consists of 8 nodes and 7 edges:\n1 4\n2 3 7\n5\n68\nThe leaves of a tree are the nodes with degree 1, i.e., with only one neighbor.\nFor example, the leaves of the above tree are nodes 3, 5, 7 and 8.\nIn a rooted tree, one of the nodes is appointed the root of the tree, and all\nother nodes are placed underneath the root. For example, in the following tree,\nnode 1 is the root node.\n1\n42 3\n75 6\n8\nIn a rooted tree, the children of a node are its lower neighbors, and the\nparent of a node is its upper neighbor. Each node has exactly one parent, except\nfor the root that does not have a parent. For example, in the above tree, the\nchildren of node 2 are nodes 5 and 6, and its parent is node 1.\n133",
            "The structure of a rooted tree is recursive: each node of the tree acts as the\nroot of a subtree that contains the node itself and all nodes that are in the\nsubtrees of its children. For example, in the above tree, the subtree of node 2\nconsists of nodes 2, 5, 6 and 8:\n2\n5 6\n8\n14.1 Tree traversal\nGeneral graph traversal algorithms can be used to traverse the nodes of a tree.\nHowever, the traversal of a tree is easier to implement than that of a general\ngraph, because there are no cycles in the tree and it is not possible to reach a\nnode from multiple directions.\nThe typical way to traverse a tree is to start a depth-\ufb01rst search at an arbitrary\nnode. The following recursive function can be used:\nvoid dfs(int s, int e) {\n// process node s\nfor (auto u : adj[s]) {\nif (u != e) dfs(u, s);\n}\n}\nThe function is given two parameters: the current node s and the previous\nnode e. The purpose of the parameter e is to make sure that the search only\nmoves to nodes that have not been visited yet.\nThe following function call starts the search at node x:\ndfs(x, 0);\nIn the \ufb01rst call e = 0, because there is no previous node, and it is allowed to\nproceed to any direction in the tree.\nDynamic programming\nDynamic programming can be used to calculate some information during a tree\ntraversal. Using dynamic programming, we can, for example, calculate in O(n)\ntime for each node of a rooted tree the number of nodes in its subtree or the\nlength of the longest path from the node to a leaf.\n134",
            "As an example, let us calculate for each node s a value count[s]: the number\nof nodes in its subtree. The subtree contains the node itself and all nodes in\nthe subtrees of its children, so we can calculate the number of nodes recursively\nusing the following code:\nvoid dfs(int s, int e) {\ncount[s] = 1;\nfor (auto u : adj[s]) {\nif (u == e) continue;\ndfs(u, s);\ncount[s] += count[u];\n}\n}\n14.2 Diameter\nThe diameter of a tree is the maximum length of a path between two nodes. For\nexample, consider the following tree:\n1 4\n2 3 7\n5\n6\nThe diameter of this tree is 4, which corresponds to the following path:\n1 4\n2 3 7\n5\n6\nNote that there may be several maximum-length paths. In the above path, we\ncould replace node 6 with node 5 to obtain another path with length 4.\nNext we will discuss two O(n) time algorithms for calculating the diameter\nof a tree. The \ufb01rst algorithm is based on dynamic programming, and the second\nalgorithm uses two depth-\ufb01rst searches.\nAlgorithm 1\nA general way to approach many tree problems is to \ufb01rst root the tree arbitrarily.\nAfter this, we can try to solve the problem separately for each subtree. Our \ufb01rst\nalgorithm for calculating the diameter is based on this idea.\nAn important observation is that every path in a rooted tree has a highest\npoint: the highest node that belongs to the path. Thus, we can calculate for each\n135",
            "node the length of the longest path whose highest point is the node. One of those\npaths corresponds to the diameter of the tree.\nFor example, in the following tree, node 1 is the highest point on the path\nthat corresponds to the diameter:\n1\n42 3\n75 6\nWe calculate for each node x two values:\n\u2022 toLeaf(x): the maximum length of a path from x to any leaf\n\u2022 maxLength(x): the maximum length of a path whose highest point is x\nFor example, in the above tree, toLeaf(1) = 2, because there is a path 1 \u2192 2 \u2192 6,\nand maxLength(1) = 4, because there is a path 6 \u2192 2 \u2192 1 \u2192 4 \u2192 7. In this case,\nmaxLength(1) equals the diameter.\nDynamic programming can be used to calculate the above values for all nodes\nin O(n) time. First, to calculate toLeaf(x), we go through the children of x,\nchoose a child c with maximum toLeaf(c) and add one to this value. Then, to\ncalculate maxLength(x), we choose two distinct children a and b such that the sum\ntoLeaf(a)+toLeaf(b) is maximum and add two to this sum.\nAlgorithm 2\nAnother ef\ufb01cient way to calculate the diameter of a tree is based on two depth-\n\ufb01rst searches. First, we choose an arbitrary node a in the tree and \ufb01nd the\nfarthest node b from a. Then, we \ufb01nd the farthest node c from b. The diameter\nof the tree is the distance between b and c.\nIn the following graph, a, b and c could be:\n1 4\n2 3 7\n5\n6\nab c\nThis is an elegant method, but why does it work?\nIt helps to draw the tree differently so that the path that corresponds to the\ndiameter is horizontal, and all other nodes hang from it:\n136",
            "1 42\n3\n7\n5\n6\na\nb cx\nNode x indicates the place where the path from node a joins the path that\ncorresponds to the diameter. The farthest node from a is node b, node c or some\nother node that is at least as far from node x. Thus, this node is always a valid\nchoice for an endpoint of a path that corresponds to the diameter.\n14.3 All longest paths\nOur next problem is to calculate for every node in the tree the maximum length\nof a path that begins at the node. This can be seen as a generalization of the tree\ndiameter problem, because the largest of those lengths equals the diameter of\nthe tree. Also this problem can be solved in O(n) time.\nAs an example, consider the following tree:\n1\n4\n2\n3\n6\n5\nLet maxLength(x) denote the maximum length of a path that begins at node\nx. For example, in the above tree, maxLength(4) = 3, because there is a path\n4 \u2192 1 \u2192 2 \u2192 6. Here is a complete table of the values:\nnode x 1 2 3 4 5 6\nmaxLength(x) 2 2 3 3 3 3\nAlso in this problem, a good starting point for solving the problem is to root\nthe tree arbitrarily:\n1\n42 3\n5 6\nThe \ufb01rst part of the problem is to calculate for every node x the maximum\nlength of a path that goes through a child of x. For example, the longest path\nfrom node 1 goes through its child 2:\n137",
            "1\n42 3\n5 6\nThis part is easy to solve in O(n) time, because we can use dynamic programming\nas we have done previously.\nThen, the second part of the problem is to calculate for every node x the\nmaximum length of a path through its parent p. For example, the longest path\nfrom node 3 goes through its parent 1:\n1\n42 3\n5 6\nAt \ufb01rst glance, it seems that we should choose the longest path from p.\nHowever, this does not always work, because the longest path from p may go\nthrough x. Here is an example of this situation:\n1\n42 3\n5 6\nStill, we can solve the second part in O(n) time by storing two maximum\nlengths for each node x:\n\u2022 maxLength1(x): the maximum length of a path from x\n\u2022 maxLength2(x) the maximum length of a path from x in another direction\nthan the \ufb01rst path\nFor example, in the above graph, maxLength1(1) = 2 using the path 1 \u2192 2 \u2192 5, and\nmaxLength2(1) = 1 using the path 1 \u2192 3.\nFinally, if the path that corresponds to maxLength1(p) goes through x, we con-\nclude that the maximum length ismaxLength2(p)+1, and otherwise the maximum\nlength is maxLength1(p)+1.\n138",
            "14.4 Binary trees\nA binary tree is a rooted tree where each node has a left and right subtree. It is\npossible that a subtree of a node is empty. Thus, every node in a binary tree has\nzero, one or two children.\nFor example, the following tree is a binary tree:\n1\n2 3\n4 5\n6\n7\nThe nodes of a binary tree have three natural orderings that correspond to\ndifferent ways to recursively traverse the tree:\n\u2022 pre-order: \ufb01rst process the root, then traverse the left subtree, then\ntraverse the right subtree\n\u2022 in-order: \ufb01rst traverse the left subtree, then process the root, then traverse\nthe right subtree\n\u2022 post-order: \ufb01rst traverse the left subtree, then traverse the right subtree,\nthen process the root\nFor the above tree, the nodes in pre-order are [1 ,2,4,5,6,3,7], in in-order\n[4,2,6,5,1,3,7] and in post-order [4,6,5,2,7,3,1].\nIf we know the pre-order and in-order of a tree, we can reconstruct the exact\nstructure of the tree. For example, the above tree is the only possible tree with\npre-order [1,2,4,5,6,3,7] and in-order [4 ,2,6,5,1,3,7]. In a similar way, the\npost-order and in-order also determine the structure of a tree.\nHowever, the situation is different if we only know the pre-order and post-\norder of a tree. In this case, there may be more than one tree that match the\norderings. For example, in both of the trees\n1\n2\n1\n2\nthe pre-order is [1,2] and the post-order is [2,1], but the structures of the trees\nare different.\n139",
            "140",
            "Chapter 15\nSpanning trees\nA spanning tree of a graph consists of all nodes of the graph and some of the\nedges of the graph so that there is a path between any two nodes. Like trees\nin general, spanning trees are connected and acyclic. Usually there are several\nways to construct a spanning tree.\nFor example, consider the following graph:\n1\n2 3\n4\n5 6\n3\n5\n9\n5\n2\n7\n6 3\nOne spanning tree for the graph is as follows:\n1\n2 3\n4\n5 6\n3\n5\n9\n2\n3\nThe weight of a spanning tree is the sum of its edge weights. For example,\nthe weight of the above spanning tree is 3+5+9+3+2 = 22.\nA minimum spanning tree is a spanning tree whose weight is as small as\npossible. The weight of a minimum spanning tree for the example graph is 20,\nand such a tree can be constructed as follows:\n1\n2 3\n4\n5 6\n3\n5\n2\n7\n3\n141",
            "In a similar way, a maximum spanning tree is a spanning tree whose\nweight is as large as possible. The weight of a maximum spanning tree for the\nexample graph is 32:\n1\n2 3\n4\n5 6\n5\n9\n5 7\n6\nNote that a graph may have several minimum and maximum spanning trees,\nso the trees are not unique.\nIt turns out that several greedy methods can be used to construct minimum\nand maximum spanning trees. In this chapter, we discuss two algorithms that\nprocess the edges of the graph ordered by their weights. We focus on \ufb01nding\nminimum spanning trees, but the same algorithms can \ufb01nd maximum spanning\ntrees by processing the edges in reverse order.\n15.1 Kruskal\u2019s algorithm\nIn Kruskal\u2019s algorithm1, the initial spanning tree only contains the nodes of\nthe graph and does not contain any edges. Then the algorithm goes through the\nedges ordered by their weights, and always adds an edge to the tree if it does not\ncreate a cycle.\nThe algorithm maintains the components of the tree. Initially, each node of\nthe graph belongs to a separate component. Always when an edge is added to the\ntree, two components are joined. Finally, all nodes belong to the same component,\nand a minimum spanning tree has been found.\nExample\nLet us consider how Kruskal\u2019s algorithm processes the following graph:\n1\n2 3\n4\n5 6\n3\n5\n9\n5\n2\n7\n6 3\nThe \ufb01rst step of the algorithm is to sort the edges in increasing order of their\nweights. The result is the following list:\n1The algorithm was published in 1956 by J. B. Kruskal [48].\n142",
            "edge weight\n5\u20136 2\n1\u20132 3\n3\u20136 3\n1\u20135 5\n2\u20133 5\n2\u20135 6\n4\u20136 7\n3\u20134 9\nAfter this, the algorithm goes through the list and adds each edge to the tree\nif it joins two separate components.\nInitially, each node is in its own component:\n1\n2 3\n4\n5 6\nThe \ufb01rst edge to be added to the tree is the edge 5\u20136 that creates a component\n{5,6} by joining the components {5} and {6}:\n1\n2 3\n4\n5 6\n2\nAfter this, the edges 1\u20132, 3\u20136 and 1\u20135 are added in a similar way:\n1\n2 3\n4\n5 6\n3\n5\n2\n3\nAfter those steps, most components have been joined and there are two\ncomponents in the tree: {1,2,3,5,6} and {4}.\nThe next edge in the list is the edge 2\u20133, but it will not be included in the tree,\nbecause nodes 2 and 3 are already in the same component. For the same reason,\nthe edge 2\u20135 will not be included in the tree.\n143",
            "Finally, the edge 4\u20136 will be included in the tree:\n1\n2 3\n4\n5 6\n3\n5\n2\n7\n3\nAfter this, the algorithm will not add any new edges, because the graph is\nconnected and there is a path between any two nodes. The resulting graph is a\nminimum spanning tree with weight 2+3+3+5+7 = 20.\nWhy does this work?\nIt is a good question why Kruskal\u2019s algorithm works. Why does the greedy\nstrategy guarantee that we will \ufb01nd a minimum spanning tree?\nLet us see what happens if the minimum weight edge of the graph is not\nincluded in the spanning tree. For example, suppose that a spanning tree for the\nprevious graph would not contain the minimum weight edge 5\u20136. We do not know\nthe exact structure of such a spanning tree, but in any case it has to contain some\nedges. Assume that the tree would be as follows:\n1\n2 3\n4\n5 6\nHowever, it is not possible that the above tree would be a minimum spanning\ntree for the graph. The reason for this is that we can remove an edge from the\ntree and replace it with the minimum weight edge 5\u20136. This produces a spanning\ntree whose weight is smaller:\n1\n2 3\n4\n5 6\n2\nFor this reason, it is always optimal to include the minimum weight edge in\nthe tree to produce a minimum spanning tree. Using a similar argument, we\ncan show that it is also optimal to add the next edge in weight order to the tree,\nand so on. Hence, Kruskal\u2019s algorithm works correctly and always produces a\nminimum spanning tree.\n144",
            "Implementation\nWhen implementing Kruskal\u2019s algorithm, it is convenient to use the edge list\nrepresentation of the graph. The \ufb01rst phase of the algorithm sorts the edges in\nthe list in O(mlogm) time. After this, the second phase of the algorithm builds\nthe minimum spanning tree as follows:\nfor (...) {\nif (!same(a,b)) unite(a,b);\n}\nThe loop goes through the edges in the list and always processes an edge\na\u2013b where a and b are two nodes. Two functions are needed: the function same\ndetermines if a and b are in the same component, and the function unite joins\nthe components that contain a and b.\nThe problem is how to ef\ufb01ciently implement the functions same and unite.\nOne possibility is to implement the function same as a graph traversal and check\nif we can get from node a to node b. However, the time complexity of such a\nfunction would be O(n +m) and the resulting algorithm would be slow, because\nthe function same will be called for each edge in the graph.\nWe will solve the problem using a union-\ufb01nd structure that implements both\nfunctions in O(logn) time. Thus, the time complexity of Kruskal\u2019s algorithm will\nbe O(mlogn) after sorting the edge list.\n15.2 Union-\ufb01nd structure\nA union-\ufb01nd structure maintains a collection of sets. The sets are disjoint,\nso no element belongs to more than one set. Two O(logn) time operations are\nsupported: the unite operation joins two sets, and the find operation \ufb01nds the\nrepresentative of the set that contains a given element2.\nStructure\nIn a union-\ufb01nd structure, one element in each set is the representative of the set,\nand there is a chain from any other element of the set to the representative. For\nexample, assume that the sets are {1,4,7}, {5} and {2,3,6,8}:\n1\n2\n3\n4 5\n6\n7\n8\n2The structure presented here was introduced in 1971 by J. D. Hopcroft and J. D. Ullman [38].\nLater, in 1975, R. E. Tarjan studied a more sophisticated variant of the structure [ 64] that is\ndiscussed in many algorithm textbooks nowadays.\n145",
            "In this case the representatives of the sets are 4, 5 and 2. We can \ufb01nd the\nrepresentative of any element by following the chain that begins at the element.\nFor example, the element 2 is the representative for the element 6, because we\nfollow the chain 6 \u2192 3 \u2192 2. Two elements belong to the same set exactly when\ntheir representatives are the same.\nTwo sets can be joined by connecting the representative of one set to the\nrepresentative of the other set. For example, the sets {1,4,7} and {2,3,6,8} can be\njoined as follows:\n1\n2\n3\n4\n6\n7\n8\nThe resulting set contains the elements {1,2,3,4,6,7,8}. From this on, the\nelement 2 is the representative for the entire set and the old representative 4\npoints to the element 2.\nThe ef\ufb01ciency of the union-\ufb01nd structure depends on how the sets are joined.\nIt turns out that we can follow a simple strategy: always connect the representa-\ntive of the smaller set to the representative of the larger set (or if the sets are\nof equal size, we can make an arbitrary choice). Using this strategy, the length\nof any chain will be O(logn), so we can \ufb01nd the representative of any element\nef\ufb01ciently by following the corresponding chain.\nImplementation\nThe union-\ufb01nd structure can be implemented using arrays. In the following\nimplementation, the array link contains for each element the next element in the\nchain or the element itself if it is a representative, and the array size indicates\nfor each representative the size of the corresponding set.\nInitially, each element belongs to a separate set:\nfor (int i = 1; i <= n; i++) link[i] = i;\nfor (int i = 1; i <= n; i++) size[i] = 1;\nThe function find returns the representative for an element x. The represen-\ntative can be found by following the chain that begins at x.\nint find(int x) {\nwhile (x != link[x]) x = link[x];\nreturn x;\n}\nThe function same checks whether elements a and b belong to the same set.\nThis can easily be done by using the function find:\n146",
            "bool same(int a, int b) {\nreturn find(a) == find(b);\n}\nThe function unite joins the sets that contain elements a and b (the elements\nhave to be in different sets). The function \ufb01rst \ufb01nds the representatives of the\nsets and then connects the smaller set to the larger set.\nvoid unite(int a, int b) {\na = find(a);\nb = find(b);\nif (size[a] < size[b]) swap(a,b);\nsize[a] += size[b];\nlink[b] = a;\n}\nThe time complexity of the function find is O(logn) assuming that the length\nof each chain is O(logn). In this case, the functions same and unite also work in\nO(logn) time. The function unite makes sure that the length of each chain is\nO(logn) by connecting the smaller set to the larger set.\n15.3 Prim\u2019s algorithm\nPrim\u2019s algorithm3 is an alternative method for \ufb01nding a minimum spanning\ntree. The algorithm \ufb01rst adds an arbitrary node to the tree. After this, the\nalgorithm always chooses a minimum-weight edge that adds a new node to the\ntree. Finally, all nodes have been added to the tree and a minimum spanning\ntree has been found.\nPrim\u2019s algorithm resembles Dijkstra\u2019s algorithm. The difference is that Dijk-\nstra\u2019s algorithm always selects an edge whose distance from the starting node is\nminimum, but Prim\u2019s algorithm simply selects the minimum weight edge that\nadds a new node to the tree.\nExample\nLet us consider how Prim\u2019s algorithm works in the following graph:\n1\n2 3\n4\n5 6\n3\n5\n9\n5\n2\n7\n6 3\n3The algorithm is named after R. C. Prim who published it in 1957 [54]. However, the same\nalgorithm was discovered already in 1930 by V. Jarn\u00edk.\n147",
            "Initially, there are no edges between the nodes:\n1\n2 3\n4\n5 6\nAn arbitrary node can be the starting node, so let us choose node 1. First, we add\nnode 2 that is connected by an edge of weight 3:\n1\n2 3\n4\n5 6\n3\nAfter this, there are two edges with weight 5, so we can add either node 3 or\nnode 5 to the tree. Let us add node 3 \ufb01rst:\n1\n2 3\n4\n5 6\n3\n5\nThe process continues until all nodes have been included in the tree:\n1\n2 3\n4\n5 6\n3\n5\n2\n7\n3\nImplementation\nLike Dijkstra\u2019s algorithm, Prim\u2019s algorithm can be ef\ufb01ciently implemented using a\npriority queue. The priority queue should contain all nodes that can be connected\nto the current component using a single edge, in increasing order of the weights\nof the corresponding edges.\nThe time complexity of Prim\u2019s algorithm isO(n+mlogm) that equals the time\ncomplexity of Dijkstra\u2019s algorithm. In practice, Prim\u2019s and Kruskal\u2019s algorithms\nare both ef\ufb01cient, and the choice of the algorithm is a matter of taste. Still, most\ncompetitive programmers use Kruskal\u2019s algorithm.\n148",
            "Chapter 16\nDirected graphs\nIn this chapter, we focus on two classes of directed graphs:\n\u2022 Acyclic graphs: There are no cycles in the graph, so there is no path from\nany node to itself1.\n\u2022 Successor graphs: The outdegree of each node is 1, so each node has a\nunique successor.\nIt turns out that in both cases, we can design ef\ufb01cient algorithms that are based\non the special properties of the graphs.\n16.1 Topological sorting\nA topological sort is an ordering of the nodes of a directed graph such that if\nthere is a path from node a to node b, then node a appears before node b in the\nordering. For example, for the graph\n1 2 3\n4 5 6\none topological sort is [4,1,5,2,3,6]:\n1 2 34 5 6\nAn acyclic graph always has a topological sort. However, if the graph contains\na cycle, it is not possible to form a topological sort, because no node of the cycle\ncan appear before the other nodes of the cycle in the ordering. It turns out that\ndepth-\ufb01rst search can be used to both check if a directed graph contains a cycle\nand, if it does not contain a cycle, to construct a topological sort.\n1Directed acyclic graphs are sometimes called DAGs.\n149",
            "Algorithm\nThe idea is to go through the nodes of the graph and always begin a depth-\ufb01rst\nsearch at the current node if it has not been processed yet. During the searches,\nthe nodes have three possible states:\n\u2022 state 0: the node has not been processed (white)\n\u2022 state 1: the node is under processing (light gray)\n\u2022 state 2: the node has been processed (dark gray)\nInitially, the state of each node is 0. When a search reaches a node for the\n\ufb01rst time, its state becomes 1. Finally, after all successors of the node have been\nprocessed, its state becomes 2.\nIf the graph contains a cycle, we will \ufb01nd this out during the search, because\nsooner or later we will arrive at a node whose state is 1. In this case, it is not\npossible to construct a topological sort.\nIf the graph does not contain a cycle, we can construct a topological sort by\nadding each node to a list when the state of the node becomes 2. This list in\nreverse order is a topological sort.\nExample 1\nIn the example graph, the search \ufb01rst proceeds from node 1 to node 6:\n1 2 3\n4 5 6\nNow node 6 has been processed, so it is added to the list. After this, also nodes\n3, 2 and 1 are added to the list:\n1 2 3\n4 5 6\nAt this point, the list is [6,3,2,1]. The next search begins at node 4:\n1 2 3\n4 5 6\n150",
            "Thus, the \ufb01nal list is [6,3,2,1,5,4]. We have processed all nodes, so a topologi-\ncal sort has been found. The topological sort is the reverse list [4,5,1,2,3,6]:\n1 2 34 5 6\nNote that a topological sort is not unique, and there can be several topological\nsorts for a graph.\nExample 2\nLet us now consider a graph for which we cannot construct a topological sort,\nbecause the graph contains a cycle:\n1 2 3\n4 5 6\nThe search proceeds as follows:\n1 2 3\n4 5 6\nThe search reaches node 2 whose state is 1, which means that the graph contains\na cycle. In this example, there is a cycle 2 \u2192 3 \u2192 5 \u2192 2.\n16.2 Dynamic programming\nIf a directed graph is acyclic, dynamic programming can be applied to it. For\nexample, we can ef\ufb01ciently solve the following problems concerning paths from a\nstarting node to an ending node:\n\u2022 how many different paths are there?\n\u2022 what is the shortest/longest path?\n\u2022 what is the minimum/maximum number of edges in a path?\n\u2022 which nodes certainly appear in any path?\n151",
            "Counting the number of paths\nAs an example, let us calculate the number of paths from node 1 to node 6 in the\nfollowing graph:\n1 2 3\n4 5 6\nThere are a total of three such paths:\n\u2022 1 \u2192 2 \u2192 3 \u2192 6\n\u2022 1 \u2192 4 \u2192 5 \u2192 2 \u2192 3 \u2192 6\n\u2022 1 \u2192 4 \u2192 5 \u2192 3 \u2192 6\nLet paths(x) denote the number of paths from node 1 to node x. As a base\ncase, paths(1) = 1. Then, to calculate other values of paths(x), we may use the\nrecursion\npaths(x) = paths(a1)+paths(a2)+\u00b7\u00b7\u00b7+ paths(ak)\nwhere a1,a2,..., ak are the nodes from which there is an edge tox. Since the graph\nis acyclic, the values of paths(x) can be calculated in the order of a topological\nsort. A topological sort for the above graph is as follows:\n1 2 34 5 6\nHence, the numbers of paths are as follows:\n1 2 3\n4 5 6\n1 1 3\n1 2 3\nFor example, to calculate the value of paths(3), we can use the formula\npaths(2)+paths(5), because there are edges from nodes 2 and 5 to node 3. Since\npaths(2) = 2 and paths(5) = 1, we conclude that paths(3) = 3.\n152",
            "Extending Dijkstra\u2019s algorithm\nA by-product of Dijkstra\u2019s algorithm is a directed, acyclic graph that indicates\nfor each node of the original graph the possible ways to reach the node using a\nshortest path from the starting node. Dynamic programming can be applied to\nthat graph. For example, in the graph\n1 2\n3 4\n5\n3\n5 4\n8\n2\n1\n2\nthe shortest paths from node 1 may use the following edges:\n1 2\n3 4\n5\n3\n5 4\n2\n1\n2\nNow we can, for example, calculate the number of shortest paths from node 1\nto node 5 using dynamic programming:\n1 2\n3 4\n5\n3\n5 4\n2\n1\n2\n1 1\n2 3\n3\nRepresenting problems as graphs\nActually, any dynamic programming problem can be represented as a directed,\nacyclic graph. In such a graph, each node corresponds to a dynamic programming\nstate and the edges indicate how the states depend on each other.\nAs an example, consider the problem of forming a sum of money n using\ncoins {c1, c2,..., ck}. In this problem, we can construct a graph where each node\ncorresponds to a sum of money, and the edges show how the coins can be chosen.\nFor example, for coins {1,3,4} and n = 6, the graph is as follows:\n153",
            "0 1 2 3 4 5 6\nUsing this representation, the shortest path from node 0 to noden corresponds\nto a solution with the minimum number of coins, and the total number of paths\nfrom node 0 to node n equals the total number of solutions.\n16.3 Successor paths\nFor the rest of the chapter, we will focus on successor graphs. In those graphs,\nthe outdegree of each node is 1, i.e., exactly one edge starts at each node. A\nsuccessor graph consists of one or more components, each of which contains one\ncycle and some paths that lead to it.\nSuccessor graphs are sometimes called functional graphs. The reason for\nthis is that any successor graph corresponds to a function that de\ufb01nes the edges\nof the graph. The parameter for the function is a node of the graph, and the\nfunction gives the successor of that node.\nFor example, the function\nx 1 2 3 4 5 6 7 8 9\nsucc(x) 3 5 7 6 2 2 1 6 3\nde\ufb01nes the following graph:\n1 23\n4\n5\n67\n8\n9\nSince each node of a successor graph has a unique successor, we can also\nde\ufb01ne a function succ(x,k) that gives the node that we will reach if we begin at\nnode x and walk k steps forward. For example, in the above graph succ(4,6) = 2,\nbecause we will reach node 2 by walking 6 steps from node 4:\n4 6 2 5 2 5 2\nA straightforward way to calculate a value of succ(x,k) is to start at node x\nand walk k steps forward, which takes O(k) time. However, using preprocessing,\nany value of succ(x,k) can be calculated in only O(logk) time.\nThe idea is to precalculate all values of succ(x,k) where k is a power of two\nand at most u, where u is the maximum number of steps we will ever walk. This\ncan be ef\ufb01ciently done, because we can use the following recursion:\n154",
            "succ(x,k) =\n{\nsucc(x) k = 1\nsucc(succ(x,k/2),k/2) k > 1\nPrecalculating the values takes O(nlogu) time, because O(logu) values are\ncalculated for each node. In the above graph, the \ufb01rst values are as follows:\nx 1 2 3 4 5 6 7 8 9\nsucc(x,1) 3 5 7 6 2 2 1 6 3\nsucc(x,2) 7 2 1 2 5 5 3 2 7\nsucc(x,4) 3 2 7 2 5 5 1 2 3\nsucc(x,8) 7 2 1 2 5 5 3 2 7\n\u00b7\u00b7\u00b7\nAfter this, any value of succ(x,k) can be calculated by presenting the number\nof steps k as a sum of powers of two. For example, if we want to calculate the\nvalue of succ(x,11), we \ufb01rst form the representation 11 = 8+2+1. Using that,\nsucc(x,11) = succ(succ(succ(x,8),2),1).\nFor example, in the previous graph\nsucc(4,11) = succ(succ(succ(4,8),2),1) = 5.\nSuch a representation always consists of O(logk) parts, so calculating a value\nof succ(x,k) takes O(logk) time.\n16.4 Cycle detection\nConsider a successor graph that only contains a path that ends in a cycle. We\nmay ask the following questions: if we begin our walk at the starting node, what\nis the \ufb01rst node in the cycle and how many nodes does the cycle contain?\nFor example, in the graph\n54\n6\n321\nwe begin our walk at node 1, the \ufb01rst node that belongs to the cycle is node 4,\nand the cycle consists of three nodes (4, 5 and 6).\nA simple way to detect the cycle is to walk in the graph and keep track of all\nnodes that have been visited. Once a node is visited for the second time, we can\nconclude that the node is the \ufb01rst node in the cycle. This method works in O(n)\ntime and also uses O(n) memory.\nHowever, there are better algorithms for cycle detection. The time complexity\nof such algorithms is still O(n), but they only use O(1) memory. This is an\nimportant improvement if n is large. Next we will discuss Floyd\u2019s algorithm that\nachieves these properties.\n155",
            "Floyd\u2019s algorithm\nFloyd\u2019s algorithm2 walks forward in the graph using two pointers a and b.\nBoth pointers begin at a node x that is the starting node of the graph. Then,\non each turn, the pointer a walks one step forward and the pointer b walks two\nsteps forward. The process continues until the pointers meet each other:\na = succ(x);\nb = succ(succ(x));\nwhile (a != b) {\na = succ(a);\nb = succ(succ(b));\n}\nAt this point, the pointer a has walked k steps and the pointer b has walked\n2k steps, so the length of the cycle divides k. Thus, the \ufb01rst node that belongs\nto the cycle can be found by moving the pointer a to node x and advancing the\npointers step by step until they meet again.\na = x;\nwhile (a != b) {\na = succ(a);\nb = succ(b);\n}\nfirst = a;\nAfter this, the length of the cycle can be calculated as follows:\nb = succ(a);\nlength = 1;\nwhile (a != b) {\nb = succ(b);\nlength++;\n}\n2The idea of the algorithm is mentioned in [46] and attributed to R. W. Floyd; however, it is\nnot known if Floyd actually discovered the algorithm.\n156",
            "Chapter 17\nStrong connectivity\nIn a directed graph, the edges can be traversed in one direction only, so even if\nthe graph is connected, this does not guarantee that there would be a path from\na node to another node. For this reason, it is meaningful to de\ufb01ne a new concept\nthat requires more than connectivity.\nA graph is strongly connected if there is a path from any node to all other\nnodes in the graph. For example, in the following picture, the left graph is\nstrongly connected while the right graph is not.\n1 2\n3 4\n1 2\n3 4\nThe right graph is not strongly connected because, for example, there is no\npath from node 2 to node 1.\nThe strongly connected components of a graph divide the graph into\nstrongly connected parts that are as large as possible. The strongly connected\ncomponents form an acyclic component graph that represents the deep struc-\nture of the original graph.\nFor example, for the graph\n7\n321\n654\nthe strongly connected components are as follows:\n7\n321\n654\n157",
            "The corresponding component graph is as follows:\nB\nA\nDC\nThe components are A = {1,2}, B = {3,6,7}, C = {4} and D = {5}.\nA component graph is an acyclic, directed graph, so it is easier to process\nthan the original graph. Since the graph does not contain cycles, we can always\nconstruct a topological sort and use dynamic programming techniques like those\npresented in Chapter 16.\n17.1 Kosaraju\u2019s algorithm\nKosaraju\u2019s algorithm1 is an ef\ufb01cient method for \ufb01nding the strongly connected\ncomponents of a directed graph. The algorithm performs two depth-\ufb01rst searches:\nthe \ufb01rst search constructs a list of nodes according to the structure of the graph,\nand the second search forms the strongly connected components.\nSearch 1\nThe \ufb01rst phase of Kosaraju\u2019s algorithm constructs a list of nodes in the order\nin which a depth-\ufb01rst search processes them. The algorithm goes through the\nnodes, and begins a depth-\ufb01rst search at each unprocessed node. Each node will\nbe added to the list after it has been processed.\nIn the example graph, the nodes are processed in the following order:\n7\n321\n654\n1/8 2/7 9/14\n4/5 3/6 11/12\n10/13\nThe notation x/y means that processing the node started at timex and \ufb01nished\nat time y. Thus, the corresponding list is as follows:\n1According to [1], S. R. Kosaraju invented this algorithm in 1978 but did not publish it. In\n1981, the same algorithm was rediscovered and published by M. Sharir [57].\n158",
            "node processing time\n4 5\n5 6\n2 7\n1 8\n6 12\n7 13\n3 14\nSearch 2\nThe second phase of the algorithm forms the strongly connected components of\nthe graph. First, the algorithm reverses every edge in the graph. This guarantees\nthat during the second search, we will always \ufb01nd strongly connected components\nthat do not have extra nodes.\nAfter reversing the edges, the example graph is as follows:\n7\n321\n654\nAfter this, the algorithm goes through the list of nodes created by the \ufb01rst\nsearch, in reverse order. If a node does not belong to a component, the algorithm\ncreates a new component and starts a depth-\ufb01rst search that adds all new nodes\nfound during the search to the new component.\nIn the example graph, the \ufb01rst component begins at node 3:\n7\n321\n654\nNote that since all edges are reversed, the component does not \u201dleak\u201d to other\nparts in the graph.\n159",
            "The next nodes in the list are nodes 7 and 6, but they already belong to a\ncomponent, so the next new component begins at node 1:\n7\n321\n654\nFinally, the algorithm processes nodes 5 and 4 that create the remaining\nstrongly connected components:\n7\n321\n654\nThe time complexity of the algorithm is O(n + m), because the algorithm\nperforms two depth-\ufb01rst searches.\n17.2 2SAT problem\nStrong connectivity is also linked with the 2SAT problem2. In this problem, we\nare given a logical formula\n(a1 \u2228b1)\u2227(a2 \u2228b2)\u2227\u00b7\u00b7\u00b7\u2227 (am \u2228bm),\nwhere each ai and bi is either a logical variable ( x1, x2,..., xn) or a negation of\na logical variable ( \u00acx1,\u00acx2,..., \u00acxn). The symbols \u201d \u2227\u201d and \u201d\u2228\u201d denote logical\noperators \u201dand\u201d and \u201dor\u201d. Our task is to assign each variable a value so that the\nformula is true, or state that this is not possible.\nFor example, the formula\nL1 = (x2 \u2228\u00acx1)\u2227(\u00acx1 \u2228\u00acx2)\u2227(x1 \u2228 x3)\u2227(\u00acx2 \u2228\u00acx3)\u2227(x1 \u2228 x4)\nis true when the variables are assigned as follows:\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx1 = false\nx2 = false\nx3 = true\nx4 = true\n2The algorithm presented here was introduced in [ 4]. There is also another well-known\nlinear-time algorithm [19] that is based on backtracking.\n160",
            "However, the formula\nL2 = (x1 \u2228 x2)\u2227(x1 \u2228\u00acx2)\u2227(\u00acx1 \u2228 x3)\u2227(\u00acx1 \u2228\u00acx3)\nis always false, regardless of how we assign the values. The reason for this is\nthat we cannot choose a value for x1 without creating a contradiction. If x1 is\nfalse, both x2 and \u00acx2 should be true which is impossible, and if x1 is true, both\nx3 and \u00acx3 should be true which is also impossible.\nThe 2SAT problem can be represented as a graph whose nodes correspond to\nvariables xi and negations \u00acxi, and edges determine the connections between\nthe variables. Each pair ( ai \u2228 bi) generates two edges: \u00acai \u2192 bi and \u00acbi \u2192 ai.\nThis means that if ai does not hold, bi must hold, and vice versa.\nThe graph for the formula L1 is:\n\u00acx3 x2\n\u00acx4 x1\n\u00acx1 x4\n\u00acx2 x3\nAnd the graph for the formula L2 is:\nx3 x2 \u00acx2 \u00acx3\n\u00acx1\nx1\nThe structure of the graph tells us whether it is possible to assign the values\nof the variables so that the formula is true. It turns out that this can be done\nexactly when there are no nodes xi and \u00acxi such that both nodes belong to the\nsame strongly connected component. If there are such nodes, the graph contains\na path from xi to \u00acxi and also a path from \u00acxi to xi, so both xi and \u00acxi should be\ntrue which is not possible.\nIn the graph of the formula L1 there are no nodes xi and \u00acxi such that both\nnodes belong to the same strongly connected component, so a solution exists. In\nthe graph of the formula L2 all nodes belong to the same strongly connected\ncomponent, so a solution does not exist.\nIf a solution exists, the values for the variables can be found by going through\nthe nodes of the component graph in a reverse topological sort order. At each step,\nwe process a component that does not contain edges that lead to an unprocessed\ncomponent. If the variables in the component have not been assigned values,\ntheir values will be determined according to the values in the component, and if\n161",
            "they already have values, they remain unchanged. The process continues until\neach variable has been assigned a value.\nThe component graph for the formula L1 is as follows:\nA B C D\nThe components are A = {\u00acx4}, B = {x1, x2,\u00acx3}, C = {\u00acx1,\u00acx2, x3} and D = {x4}.\nWhen constructing the solution, we \ufb01rst process the component D where x4\nbecomes true. After this, we process the component C where x1 and x2 become\nfalse and x3 becomes true. All variables have been assigned values, so the\nremaining components A and B do not change the variables.\nNote that this method works, because the graph has a special structure: if\nthere are paths from node xi to node xj and from node xj to node \u00acxj, then node\nxi never becomes true. The reason for this is that there is also a path from node\n\u00acxj to node \u00acxi, and both xi and xj become false.\nA more dif\ufb01cult problem is the3SAT problem, where each part of the formula\nis of the form (ai \u2228bi \u2228 ci). This problem is NP-hard, so no ef\ufb01cient algorithm for\nsolving the problem is known.\n162",
            "Chapter 18\nTree queries\nThis chapter discusses techniques for processing queries on subtrees and paths\nof a rooted tree. For example, such queries are:\n\u2022 what is the kth ancestor of a node?\n\u2022 what is the sum of values in the subtree of a node?\n\u2022 what is the sum of values on a path between two nodes?\n\u2022 what is the lowest common ancestor of two nodes?\n18.1 Finding ancestors\nThe kth ancestor of a node x in a rooted tree is the node that we will reach\nif we move k levels up from x. Let ancestor(x,k) denote the kth ancestor of a\nnode x (or 0 if there is no such an ancestor). For example, in the following tree,\nancestor(2,1) = 1 and ancestor(8,2) = 4.\n1\n24 5\n63 7\n8\nAn easy way to calculate any value of ancestor(x,k) is to perform a sequence\nof k moves in the tree. However, the time complexity of this method is O(k),\nwhich may be slow, because a tree of n nodes may have a chain of n nodes.\n163",
            "Fortunately, using a technique similar to that used in Chapter 16.3, any value\nof ancestor(x,k) can be ef\ufb01ciently calculated in O(logk) time after preprocessing.\nThe idea is to precalculate all values ancestor(x,k) where k \u2264 n is a power of two.\nFor example, the values for the above tree are as follows:\nx 1 2 3 4 5 6 7 8\nancestor(x,1) 0 1 4 1 1 2 4 7\nancestor(x,2) 0 0 1 0 0 1 1 4\nancestor(x,4) 0 0 0 0 0 0 0 0\n\u00b7\u00b7\u00b7\nThe preprocessing takesO(nlogn) time, becauseO(logn) values are calculated\nfor each node. After this, any value of ancestor(x,k) can be calculated in O(logk)\ntime by representing k as a sum where each term is a power of two.\n18.2 Subtrees and paths\nA tree traversal array contains the nodes of a rooted tree in the order in which\na depth-\ufb01rst search from the root node visits them. For example, in the tree\n1\n2 3 4 5\n6 7 8 9\na depth-\ufb01rst search proceeds as follows:\n1\n2 3 4 5\n6 7 8 9\nHence, the corresponding tree traversal array is as follows:\n1 2 6 3 4 7 8 9 5\n164",
            "Subtree queries\nEach subtree of a tree corresponds to a subarray of the tree traversal array such\nthat the \ufb01rst element of the subarray is the root node. For example, the following\nsubarray contains the nodes of the subtree of node 4:\n1 2 6 3 4 7 8 9 5\nUsing this fact, we can ef\ufb01ciently process queries that are related to subtrees of\na tree. As an example, consider a problem where each node is assigned a value,\nand our task is to support the following queries:\n\u2022 update the value of a node\n\u2022 calculate the sum of values in the subtree of a node\nConsider the following tree where the blue numbers are the values of the\nnodes. For example, the sum of the subtree of node 4 is 3+4+3+1 = 11.\n1\n2 3 4 5\n6 7 8 9\n2\n3 5 3 1\n4 4 3 1\nThe idea is to construct a tree traversal array that contains three values for\neach node: the identi\ufb01er of the node, the size of the subtree, and the value of the\nnode. For example, the array for the above tree is as follows:\nnode id\nsubtree size\nnode value\n1 2 6 3 4 7 8 9 5\n9 2 1 1 4 1 1 1 1\n2 3 4 5 3 4 3 1 1\nUsing this array, we can calculate the sum of values in any subtree by \ufb01rst\n\ufb01nding out the size of the subtree and then the values of the corresponding nodes.\nFor example, the values in the subtree of node 4 can be found as follows:\nnode id\nsubtree size\nnode value\n1 2 6 3 4 7 8 9 5\n9 2 1 1 4 1 1 1 1\n2 3 4 5 3 4 3 1 1\nTo answer the queries ef\ufb01ciently, it suf\ufb01ces to store the values of the nodes\nin a binary indexed or segment tree. After this, we can both update a value and\ncalculate the sum of values in O(logn) time.\n165",
            "Path queries\nUsing a tree traversal array, we can also ef\ufb01ciently calculate sums of values on\npaths from the root node to any node of the tree. Consider a problem where our\ntask is to support the following queries:\n\u2022 change the value of a node\n\u2022 calculate the sum of values on a path from the root to a node\nFor example, in the following tree, the sum of values from the root node to\nnode 7 is 4+5+5 = 14:\n1\n2 3 4 5\n6 7 8 9\n4\n5 3 5 2\n3 5 3 1\nWe can solve this problem like before, but now each value in the last row of\nthe array is the sum of values on a path from the root to the node. For example,\nthe following array corresponds to the above tree:\nnode id\nsubtree size\npath sum\n1 2 6 3 4 7 8 9 5\n9 2 1 1 4 1 1 1 1\n4 9 12 7 9 14 12 10 6\nWhen the value of a node increases by x, the sums of all nodes in its subtree\nincrease by x. For example, if the value of node 4 increases by 1, the array\nchanges as follows:\nnode id\nsubtree size\npath sum\n1 2 6 3 4 7 8 9 5\n9 2 1 1 4 1 1 1 1\n4 9 12 7 10 15 13 11 6\nThus, to support both the operations, we should be able to increase all values\nin a range and retrieve a single value. This can be done in O(logn) time using a\nbinary indexed or segment tree (see Chapter 9.4).\n166",
            "18.3 Lowest common ancestor\nThe lowest common ancestor of two nodes of a rooted tree is the lowest node\nwhose subtree contains both the nodes. A typical problem is to ef\ufb01ciently process\nqueries that ask to \ufb01nd the lowest common ancestor of two nodes.\nFor example, in the following tree, the lowest common ancestor of nodes 5 and\n8 is node 2:\n1\n42 3\n75 6\n8\nNext we will discuss two ef\ufb01cient techniques for \ufb01nding the lowest common\nancestor of two nodes.\nMethod 1\nOne way to solve the problem is to use the fact that we can ef\ufb01ciently \ufb01nd the\nkth ancestor of any node in the tree. Using this, we can divide the problem of\n\ufb01nding the lowest common ancestor into two parts.\nWe use two pointers that initially point to the two nodes whose lowest common\nancestor we should \ufb01nd. First, we move one of the pointers upwards so that both\npointers point to nodes at the same level.\nIn the example scenario, we move the second pointer one level up so that it\npoints to node 6 which is at the same level with node 5:\n1\n42 3\n75 6\n8\n167",
            "After this, we determine the minimum number of steps needed to move both\npointers upwards so that they will point to the same node. The node to which the\npointers point after this is the lowest common ancestor.\nIn the example scenario, it suf\ufb01ces to move both pointers one step upwards to\nnode 2, which is the lowest common ancestor:\n1\n42 3\n75 6\n8\nSince both parts of the algorithm can be performed in O(logn) time using\nprecomputed information, we can \ufb01nd the lowest common ancestor of any two\nnodes in O(logn) time.\nMethod 2\nAnother way to solve the problem is based on a tree traversal array1. Once again,\nthe idea is to traverse the nodes using a depth-\ufb01rst search:\n1\n42 3\n75 6\n8\nHowever, we use a different tree traversal array than before: we add each\nnode to the array always when the depth-\ufb01rst search walks through the node,\nand not only at the \ufb01rst visit. Hence, a node that has k children appears k +1\ntimes in the array and there are a total of 2n \u22121 nodes in the array.\n1This lowest common ancestor algorithm was presented in [7]. This technique is sometimes\ncalled the Euler tour technique [66].\n168",
            "We store two values in the array: the identi\ufb01er of the node and the depth of\nthe node in the tree. The following array corresponds to the above tree:\nnode id\ndepth\n1 2 5 2 6 8 6 2 1 3 1 4 7 4 1\n1 2 3 2 3 4 3 2 1 2 1 2 3 2 1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nNow we can \ufb01nd the lowest common ancestor of nodes a and b by \ufb01nding the\nnode with the minimum depth between nodes a and b in the array. For example,\nthe lowest common ancestor of nodes 5 and 8 can be found as follows:\nnode id\ndepth\n\u2191\n1 2 5 2 6 8 6 2 1 3 1 4 7 4 1\n1 2 3 2 3 4 3 2 1 2 1 2 3 2 1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nNode 5 is at position 2, node 8 is at position 5, and the node with minimum\ndepth between positions 2... 5 is node 2 at position 3 whose depth is 2. Thus, the\nlowest common ancestor of nodes 5 and 8 is node 2.\nThus, to \ufb01nd the lowest common ancestor of two nodes it suf\ufb01ces to process a\nrange minimum query. Since the array is static, we can process such queries in\nO(1) time after an O(nlogn) time preprocessing.\nDistances of nodes\nThe distance between nodes a and b equals the length of the path from a to b. It\nturns out that the problem of calculating the distance between nodes reduces to\n\ufb01nding their lowest common ancestor.\nFirst, we root the tree arbitrarily. After this, the distance of nodes a and b\ncan be calculated using the formula\ndepth(a)+depth(b)\u22122\u00b7depth(c),\nwhere c is the lowest common ancestor of a and b and depth(s) denotes the depth\nof node s. For example, consider the distance of nodes 5 and 8:\n1\n42 3\n75 6\n8\n169",
            "The lowest common ancestor of nodes 5 and 8 is node 2. The depths of the\nnodes are depth(5) = 3, depth(8) = 4 and depth(2) = 2, so the distance between\nnodes 5 and 8 is 3+4\u22122\u00b72 = 3.\n18.4 Of\ufb02ine algorithms\nSo far, we have discussed online algorithms for tree queries. Those algorithms\nare able to process queries one after another so that each query is answered\nbefore receiving the next query.\nHowever, in many problems, the online property is not necessary. In this\nsection, we focus on of\ufb02ine algorithms. Those algorithms are given a set of\nqueries which can be answered in any order. It is often easier to design an of\ufb02ine\nalgorithm compared to an online algorithm.\nMerging data structures\nOne method to construct an of\ufb02ine algorithm is to perform a depth-\ufb01rst tree\ntraversal and maintain data structures in nodes. At each node s, we create a\ndata structure d[s] that is based on the data structures of the children of s. Then,\nusing this data structure, all queries related to s are processed.\nAs an example, consider the following problem: We are given a tree where\neach node has some value. Our task is to process queries of the form \u201dcalculate\nthe number of nodes with value x in the subtree of node s\u201d. For example, in the\nfollowing tree, the subtree of node 4 contains two nodes whose value is 3.\n1\n2 3 4 5\n6 7 8 9\n2\n3 5 3 1\n4 4 3 1\nIn this problem, we can use map structures to answer the queries. For\nexample, the maps for node 4 and its children are as follows:\n4\n1\n3\n1\n1\n1\n1 3 4\n1 2 1\n170",
            "If we create such a data structure for each node, we can easily process all\ngiven queries, because we can handle all queries related to a node immediately\nafter creating its data structure. For example, the above map structure for node\n4 tells us that its subtree contains two nodes whose value is 3.\nHowever, it would be too slow to create all data structures from scratch.\nInstead, at each node s, we create an initial data structure d[s] that only contains\nthe value of s. After this, we go through the children of s and merge d[s] and all\ndata structures d[u] where u is a child of s.\nFor example, in the above tree, the map for node 4 is created by merging the\nfollowing maps:\n4\n1\n3\n1\n1\n1\n3\n1\nHere the \ufb01rst map is the initial data structure for node 4, and the other three\nmaps correspond to nodes 7, 8 and 9.\nThe merging at node s can be done as follows: We go through the children\nof s and at each child u merge d[s] and d[u]. We always copy the contents from\nd[u] to d[s]. However, before this, we swap the contents of d[s] and d[u] if d[s] is\nsmaller than d[u]. By doing this, each value is copied only O(logn) times during\nthe tree traversal, which ensures that the algorithm is ef\ufb01cient.\nTo swap the contents of two data structures a and b ef\ufb01ciently, we can just\nuse the following code:\nswap(a,b);\nIt is guaranteed that the above code works in constant time when a and b are\nC++ standard library data structures.\nLowest common ancestors\nThere is also an of\ufb02ine algorithm for processing a set of lowest common ancestor\nqueries2. The algorithm is based on the union-\ufb01nd data structure (see Chapter\n15.2), and the bene\ufb01t of the algorithm is that it is easier to implement than the\nalgorithms discussed earlier in this chapter.\nThe algorithm is given as input a set of pairs of nodes, and it determines for\neach such pair the lowest common ancestor of the nodes. The algorithm performs\na depth-\ufb01rst tree traversal and maintains disjoint sets of nodes. Initially, each\nnode belongs to a separate set. For each set, we also store the highest node in the\ntree that belongs to the set.\nWhen the algorithm visits a node x, it goes through all nodes y such that the\nlowest common ancestor of x and y has to be found. If y has already been visited,\nthe algorithm reports that the lowest common ancestor of x and y is the highest\nnode in the set of y. Then, after processing node x, the algorithm joins the sets of\nx and its parent.\n2This algorithm was published by R. E. Tarjan in 1979 [65].\n171",
            "For example, suppose that we want to \ufb01nd the lowest common ancestors of\nnode pairs (5,8) and (2,7) in the following tree:\n1\n42 3\n75 6\n8\nIn the following trees, gray nodes denote visited nodes and dashed groups of\nnodes belong to the same set. When the algorithm visits node 8, it notices that\nnode 5 has been visited and the highest node in its set is 2. Thus, the lowest\ncommon ancestor of nodes 5 and 8 is 2:\n1\n42 3\n75 6\n8\nLater, when visiting node 7, the algorithm determines that the lowest common\nancestor of nodes 2 and 7 is 1:\n1\n42 3\n75 6\n8\n172",
            "Chapter 19\nPaths and circuits\nThis chapter focuses on two types of paths in graphs:\n\u2022 An Eulerian path is a path that goes through each edge exactly once.\n\u2022 A Hamiltonian path is a path that visits each node exactly once.\nWhile Eulerian and Hamiltonian paths look like similar concepts at \ufb01rst\nglance, the computational problems related to them are very different. It turns\nout that there is a simple rule that determines whether a graph contains an\nEulerian path, and there is also an ef\ufb01cient algorithm to \ufb01nd such a path if\nit exists. On the contrary, checking the existence of a Hamiltonian path is a\nNP-hard problem, and no ef\ufb01cient algorithm is known for solving the problem.\n19.1 Eulerian paths\nAn Eulerian path1 is a path that goes exactly once through each edge of the\ngraph. For example, the graph\n1 2\n3\n4 5\nhas an Eulerian path from node 2 to node 5:\n1 2\n3\n4 5\n1.\n2.\n3.\n4.\n5.\n6.\n1L. Euler studied such paths in 1736 when he solved the famous K\u00f6nigsberg bridge problem.\nThis was the birth of graph theory.\n173",
            "An Eulerian circuit is an Eulerian path that starts and ends at the same node.\nFor example, the graph\n1 2\n3\n4 5\nhas an Eulerian circuit that starts and ends at node 1:\n1 2\n3\n4 5\n1.\n2.\n3.\n4.\n5.\n6.\nExistence\nThe existence of Eulerian paths and circuits depends on the degrees of the nodes.\nFirst, an undirected graph has an Eulerian path exactly when all the edges\nbelong to the same connected component and\n\u2022 the degree of each node is even or\n\u2022 the degree of exactly two nodes is odd, and the degree of all other nodes is\neven.\nIn the \ufb01rst case, each Eulerian path is also an Eulerian circuit. In the second\ncase, the odd-degree nodes are the starting and ending nodes of an Eulerian path\nwhich is not an Eulerian circuit.\nFor example, in the graph\n1 2\n3\n4 5\nnodes 1, 3 and 4 have a degree of 2, and nodes 2 and 5 have a degree of 3. Exactly\ntwo nodes have an odd degree, so there is an Eulerian path between nodes 2 and\n5, but the graph does not contain an Eulerian circuit.\nIn a directed graph, we focus on indegrees and outdegrees of the nodes. A\ndirected graph contains an Eulerian path exactly when all the edges belong to\nthe same connected component and\n\u2022 in each node, the indegree equals the outdegree, or\n174",
            "\u2022 in one node, the indegree is one larger than the outdegree, in another node,\nthe outdegree is one larger than the indegree, and in all other nodes, the\nindegree equals the outdegree.\nIn the \ufb01rst case, each Eulerian path is also an Eulerian circuit, and in the\nsecond case, the graph contains an Eulerian path that begins at the node whose\noutdegree is larger and ends at the node whose indegree is larger.\nFor example, in the graph\n1 2\n3\n4 5\nnodes 1, 3 and 4 have both indegree 1 and outdegree 1, node 2 has indegree 1\nand outdegree 2, and node 5 has indegree 2 and outdegree 1. Hence, the graph\ncontains an Eulerian path from node 2 to node 5:\n1 2\n3\n4 5\n1.\n2.\n3.\n4.\n5.\n6.\nHierholzer\u2019s algorithm\nHierholzer\u2019s algorithm2 is an ef\ufb01cient method for constructing an Eulerian\ncircuit. The algorithm consists of several rounds, each of which adds new edges\nto the circuit. Of course, we assume that the graph contains an Eulerian circuit;\notherwise Hierholzer\u2019s algorithm cannot \ufb01nd it.\nFirst, the algorithm constructs a circuit that contains some (not necessarily\nall) of the edges of the graph. After this, the algorithm extends the circuit step by\nstep by adding subcircuits to it. The process continues until all edges have been\nadded to the circuit.\nThe algorithm extends the circuit by always \ufb01nding a node x that belongs\nto the circuit but has an outgoing edge that is not included in the circuit. The\nalgorithm constructs a new path from node x that only contains edges that are\nnot yet in the circuit. Sooner or later, the path will return to nodex, which creates\na subcircuit.\nIf the graph only contains an Eulerian path, we can still use Hierholzer\u2019s\nalgorithm to \ufb01nd it by adding an extra edge to the graph and removing the edge\nafter the circuit has been constructed. For example, in an undirected graph, we\nadd the extra edge between the two odd-degree nodes.\nNext we will see how Hierholzer\u2019s algorithm constructs an Eulerian circuit\nfor an undirected graph.\n2The algorithm was published in 1873 after Hierholzer\u2019s death [35].\n175",
            "Example\nLet us consider the following graph:\n1\n2 3 4\n5 6 7\nSuppose that the algorithm \ufb01rst creates a circuit that begins at node 1. A\npossible circuit is 1 \u2192 2 \u2192 3 \u2192 1:\n1\n2 3 4\n5 6 7\n1.\n2.\n3.\nAfter this, the algorithm adds the subcircuit 2 \u2192 5 \u2192 6 \u2192 2 to the circuit:\n1\n2 3 4\n5 6 7\n1.\n2.\n3.\n4.\n5.\n6.\nFinally, the algorithm adds the subcircuit 6\u2192 3 \u2192 4 \u2192 7 \u2192 6 to the circuit:\n1\n2 3 4\n5 6 7\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n10.\n176",
            "Now all edges are included in the circuit, so we have successfully constructed an\nEulerian circuit.\n19.2 Hamiltonian paths\nA Hamiltonian path is a path that visits each node of the graph exactly once.\nFor example, the graph\n1 2\n3\n4 5\ncontains a Hamiltonian path from node 1 to node 3:\n1 2\n3\n4 5\n1.\n2.\n3.\n4.\nIf a Hamiltonian path begins and ends at the same node, it is called a Hamil-\ntonian circuit. The graph above also has an Hamiltonian circuit that begins\nand ends at node 1:\n1 2\n3\n4 5\n1.\n2.\n3.\n4.\n5.\nExistence\nNo ef\ufb01cient method is known for testing if a graph contains a Hamiltonian path,\nand the problem is NP-hard. Still, in some special cases, we can be certain that a\ngraph contains a Hamiltonian path.\nA simple observation is that if the graph is complete, i.e., there is an edge\nbetween all pairs of nodes, it also contains a Hamiltonian path. Also stronger\nresults have been achieved:\n\u2022 Dirac\u2019s theorem: If the degree of each node is at least n/2, the graph\ncontains a Hamiltonian path.\n\u2022 Ore\u2019s theorem: If the sum of degrees of each non-adjacent pair of nodes is\nat least n, the graph contains a Hamiltonian path.\n177",
            "A common property in these theorems and other results is that they guarantee\nthe existence of a Hamiltonian path if the graph hasa large number of edges. This\nmakes sense, because the more edges the graph contains, the more possibilities\nthere is to construct a Hamiltonian path.\nConstruction\nSince there is no ef\ufb01cient way to check if a Hamiltonian path exists, it is clear\nthat there is also no method to ef\ufb01ciently construct the path, because otherwise\nwe could just try to construct the path and see whether it exists.\nA simple way to search for a Hamiltonian path is to use a backtracking\nalgorithm that goes through all possible ways to construct the path. The time\ncomplexity of such an algorithm is at least O(n!), because there are n! different\nways to choose the order of n nodes.\nA more ef\ufb01cient solution is based on dynamic programming (see Chapter\n10.5). The idea is to calculate values of a function possible(S, x), where S is a\nsubset of nodes and x is one of the nodes. The function indicates whether there is\na Hamiltonian path that visits the nodes of S and ends at node x. It is possible to\nimplement this solution in O(2nn2) time.\n19.3 De Bruijn sequences\nA De Bruijn sequence is a string that contains every string of length n exactly\nonce as a substring, for a \ufb01xed alphabet of k characters. The length of such a\nstring is kn +n \u22121 characters. For example, when n = 3 and k = 2, an example of\na De Bruijn sequence is\n0001011100.\nThe substrings of this string are all combinations of three bits: 000, 001, 010,\n011, 100, 101, 110 and 111.\nIt turns out that each De Bruijn sequence corresponds to an Eulerian path in\na graph. The idea is to construct a graph where each node contains a string of\nn \u22121 characters and each edge adds one character to the string. The following\ngraph corresponds to the above scenario:\n00 11\n01\n10\n1 1\n00\n01\n0 1\nAn Eulerian path in this graph corresponds to a string that contains all\nstrings of length n. The string contains the characters of the starting node and\nall characters of the edges. The starting node has n \u22121 characters and there are\nkn characters in the edges, so the length of the string is kn +n \u22121.\n178",
            "19.4 Knight\u2019s tours\nA knight\u2019s tour is a sequence of moves of a knight on an n \u00d7 n chessboard\nfollowing the rules of chess such that the knight visits each square exactly once.\nA knight\u2019s tour is called aclosed tour if the knight \ufb01nally returns to the starting\nsquare and otherwise it is called an open tour.\nFor example, here is an open knight\u2019s tour on a 5\u00d75 board:\n1 4 11 16 25\n12 17 2 5 10\n3 20 7 24 15\n18 13 22 9 6\n21 8 19 14 23\nA knight\u2019s tour corresponds to a Hamiltonian path in a graph whose nodes\nrepresent the squares of the board, and two nodes are connected with an edge if\na knight can move between the squares according to the rules of chess.\nA natural way to construct a knight\u2019s tour is to use backtracking. The search\ncan be made more ef\ufb01cient by using heuristics that attempt to guide the knight\nso that a complete tour will be found quickly.\nWarnsdorf\u2019s rule\nWarnsdorf\u2019s ruleis a simple and effective heuristic for \ufb01nding a knight\u2019s tour3.\nUsing the rule, it is possible to ef\ufb01ciently construct a tour even on a large board.\nThe idea is to always move the knight so that it ends up in a square where the\nnumber of possible moves is as small as possible.\nFor example, in the following situation, there are \ufb01ve possible squares to\nwhich the knight can move (squares a... e):\n1\n2\na\nb e\nc d\nIn this situation, Warnsdorf\u2019s rule moves the knight to squarea, because after\nthis choice, there is only a single possible move. The other choices would move\nthe knight to squares where there would be three moves available.\n3This heuristic was proposed in Warnsdorf\u2019s book [69] in 1823. There are also polynomial\nalgorithms for \ufb01nding knight\u2019s tours [52], but they are more complicated.\n179",
            "180",
            "Chapter 20\nFlows and cuts\nIn this chapter, we focus on the following two problems:\n\u2022 Finding a maximum \ufb02ow: What is the maximum amount of \ufb02ow we can\nsend from a node to another node?\n\u2022 Finding a minimum cut: What is a minimum-weight set of edges that\nseparates two nodes of the graph?\nThe input for both these problems is a directed, weighted graph that contains\ntwo special nodes: the source is a node with no incoming edges, and the sink is a\nnode with no outgoing edges.\nAs an example, we will use the following graph where node 1 is the source\nand node 6 is the sink:\n1\n2 3\n6\n4 5\n5\n6\n5\n4\n1\n2\n3 8\nMaximum \ufb02ow\nIn the maximum \ufb02ow problem, our task is to send as much \ufb02ow as possible\nfrom the source to the sink. The weight of each edge is a capacity that restricts\nthe \ufb02ow that can go through the edge. In each intermediate node, the incoming\nand outgoing \ufb02ow has to be equal.\nFor example, the maximum size of a \ufb02ow in the example graph is 7. The\nfollowing picture shows how we can route the \ufb02ow:\n1\n2 3\n6\n4 5\n3/5\n6/6\n5/5\n4/4\n1/1\n2/2\n3/3 1/8\n181",
            "The notation v/k means that a \ufb02ow of v units is routed through an edge whose\ncapacity is k units. The size of the \ufb02ow is 7, because the source sends 3 +4 units\nof \ufb02ow and the sink receives 5 +2 units of \ufb02ow. It is easy see that this \ufb02ow is\nmaximum, because the total capacity of the edges leading to the sink is 7.\nMinimum cut\nIn the minimum cut problem, our task is to remove a set of edges from the graph\nsuch that there will be no path from the source to the sink after the removal and\nthe total weight of the removed edges is minimum.\nThe minimum size of a cut in the example graph is 7. It suf\ufb01ces to remove\nthe edges 2 \u2192 3 and 4 \u2192 5:\n1\n2 3\n6\n4 5\n5\n6\n5\n4\n1\n2\n3 8\nAfter removing the edges, there will be no path from the source to the sink.\nThe size of the cut is 7, because the weights of the removed edges are 6 and 1.\nThe cut is minimum, because there is no valid way to remove edges from the\ngraph such that their total weight would be less than 7.\nIt is not a coincidence that the maximum size of a \ufb02ow and the minimum size\nof a cut are the same in the above example. It turns out that a maximum \ufb02ow\nand a minimum cut are always equally large, so the concepts are two sides of the\nsame coin.\nNext we will discuss the Ford\u2013Fulkerson algorithm that can be used to \ufb01nd\nthe maximum \ufb02ow and minimum cut of a graph. The algorithm also helps us to\nunderstand why they are equally large.\n20.1 Ford\u2013Fulkerson algorithm\nThe Ford\u2013Fulkerson algorithm[25] \ufb01nds the maximum \ufb02ow in a graph. The\nalgorithm begins with an empty \ufb02ow, and at each step \ufb01nds a path from the\nsource to the sink that generates more \ufb02ow. Finally, when the algorithm cannot\nincrease the \ufb02ow anymore, the maximum \ufb02ow has been found.\nThe algorithm uses a special representation of the graph where each original\nedge has a reverse edge in another direction. The weight of each edge indicates\nhow much more \ufb02ow we could route through it. At the beginning of the algorithm,\nthe weight of each original edge equals the capacity of the edge and the weight of\neach reverse edge is zero.\n182",
            "The new representation for the example graph is as follows:\n1\n2 3\n6\n4 5\n5\n0\n6\n0 5\n0\n4\n0 1\n0\n2\n0\n3 0 80\nAlgorithm description\nThe Ford\u2013Fulkerson algorithm consists of several rounds. On each round, the\nalgorithm \ufb01nds a path from the source to the sink such that each edge on the\npath has a positive weight. If there is more than one possible path available, we\ncan choose any of them.\nFor example, suppose we choose the following path:\n1\n2 3\n6\n4 5\n5\n0\n6\n0 5\n0\n4\n0 1\n0\n2\n0\n3 0 80\nAfter choosing the path, the \ufb02ow increases by x units, where x is the smallest\nedge weight on the path. In addition, the weight of each edge on the path\ndecreases by x and the weight of each reverse edge increases by x.\nIn the above path, the weights of the edges are 5, 6, 8 and 2. The smallest\nweight is 2, so the \ufb02ow increases by 2 and the new graph is as follows:\n1\n2 3\n6\n4 5\n3\n2\n4\n2 5\n0\n4\n0 1\n0\n0\n2\n3 0 62\nThe idea is that increasing the \ufb02ow decreases the amount of \ufb02ow that can\ngo through the edges in the future. On the other hand, it is possible to cancel\n\ufb02ow later using the reverse edges of the graph if it turns out that it would be\nbene\ufb01cial to route the \ufb02ow in another way.\nThe algorithm increases the \ufb02ow as long as there is a path from the source to\nthe sink through positive-weight edges. In the present example, our next path\ncan be as follows:\n183",
            "1\n2 3\n6\n4 5\n3\n2\n4\n2 5\n0\n4\n0 1\n0\n0\n2\n3 0 62\nThe minimum edge weight on this path is 3, so the path increases the \ufb02ow by\n3, and the total \ufb02ow after processing the path is 5.\nThe new graph will be as follows:\n1\n2 3\n6\n4 5\n3\n2\n1\n5 2\n3\n1\n3 1\n0\n0\n2\n0 3 62\nWe still need two more rounds before reaching the maximum \ufb02ow. For exam-\nple, we can choose the paths 1 \u2192 2 \u2192 3 \u2192 6 and 1 \u2192 4 \u2192 5 \u2192 3 \u2192 6. Both paths\nincrease the \ufb02ow by 1, and the \ufb01nal graph is as follows:\n1\n2 3\n6\n4 5\n2\n3\n0\n6 0\n5\n0\n4 0\n1\n0\n2\n0 3 71\nIt is not possible to increase the \ufb02ow anymore, because there is no path\nfrom the source to the sink with positive edge weights. Hence, the algorithm\nterminates and the maximum \ufb02ow is 7.\nFinding paths\nThe Ford\u2013Fulkerson algorithm does not specify how we should choose the paths\nthat increase the \ufb02ow. In any case, the algorithm will terminate sooner or later\nand correctly \ufb01nd the maximum \ufb02ow. However, the ef\ufb01ciency of the algorithm\ndepends on the way the paths are chosen.\nA simple way to \ufb01nd paths is to use depth-\ufb01rst search. Usually, this works\nwell, but in the worst case, each path only increases the \ufb02ow by 1 and the\nalgorithm is slow. Fortunately, we can avoid this situation by using one of the\nfollowing techniques:\n184",
            "The Edmonds\u2013Karp algorithm [18] chooses each path so that the number\nof edges on the path is as small as possible. This can be done by using breadth-\n\ufb01rst search instead of depth-\ufb01rst search for \ufb01nding paths. It can be proven that\nthis guarantees that the \ufb02ow increases quickly, and the time complexity of the\nalgorithm is O(m2n).\nThe scaling algorithm [2] uses depth-\ufb01rst search to \ufb01nd paths where each\nedge weight is at least a threshold value. Initially, the threshold value is some\nlarge number, for example the sum of all edge weights of the graph. Always when\na path cannot be found, the threshold value is divided by 2. The time complexity\nof the algorithm is O(m2 log c), where c is the initial threshold value.\nIn practice, the scaling algorithm is easier to implement, because depth-\ufb01rst\nsearch can be used for \ufb01nding paths. Both algorithms are ef\ufb01cient enough for\nproblems that typically appear in programming contests.\nMinimum cuts\nIt turns out that once the Ford\u2013Fulkerson algorithm has found a maximum \ufb02ow,\nit has also determined a minimum cut. Let A be the set of nodes that can be\nreached from the source using positive-weight edges. In the example graph, A\ncontains nodes 1, 2 and 4:\n1\n2 3\n6\n4 5\n2\n3\n0\n6 0\n5\n0\n4 0\n1\n0\n2\n0 3 71\nNow the minimum cut consists of the edges of the original graph that start at\nsome node in A, end at some node outside A, and whose capacity is fully used\nin the maximum \ufb02ow. In the above graph, such edges are 2 \u2192 3 and 4 \u2192 5, that\ncorrespond to the minimum cut 6+1 = 7.\nWhy is the \ufb02ow produced by the algorithm maximum and why is the cut\nminimum? The reason is that a graph cannot contain a \ufb02ow whose size is larger\nthan the weight of any cut of the graph. Hence, always when a \ufb02ow and a cut are\nequally large, they are a maximum \ufb02ow and a minimum cut.\nLet us consider any cut of the graph such that the source belongs to A, the\nsink belongs to B and there are some edges between the sets:\nA B\n185",
            "The size of the cut is the sum of the edges that go from A to B. This is an\nupper bound for the \ufb02ow in the graph, because the \ufb02ow has to proceed from A to\nB. Thus, the size of a maximum \ufb02ow is smaller than or equal to the size of any\ncut in the graph.\nOn the other hand, the Ford\u2013Fulkerson algorithm produces a \ufb02ow whose size\nis exactly as large as the size of a cut in the graph. Thus, the \ufb02ow has to be a\nmaximum \ufb02ow and the cut has to be a minimum cut.\n20.2 Disjoint paths\nMany graph problems can be solved by reducing them to the maximum \ufb02ow\nproblem. Our \ufb01rst example of such a problem is as follows: we are given a\ndirected graph with a source and a sink, and our task is to \ufb01nd the maximum\nnumber of disjoint paths from the source to the sink.\nEdge-disjoint paths\nWe will \ufb01rst focus on the problem of \ufb01nding the maximum number of edge-\ndisjoint paths from the source to the sink. This means that we should construct\na set of paths such that each edge appears in at most one path.\nFor example, consider the following graph:\n1\n2 3\n4 5\n6\nIn this graph, the maximum number of edge-disjoint paths is 2. We can choose\nthe paths 1 \u2192 2 \u2192 4 \u2192 3 \u2192 6 and 1 \u2192 4 \u2192 5 \u2192 6 as follows:\n1\n2 3\n4 5\n6\nIt turns out that the maximum number of edge-disjoint paths equals the\nmaximum \ufb02ow of the graph, assuming that the capacity of each edge is one. After\nthe maximum \ufb02ow has been constructed, the edge-disjoint paths can be found\ngreedily by following paths from the source to the sink.\nNode-disjoint paths\nLet us now consider another problem: \ufb01nding the maximum number of node-\ndisjoint paths from the source to the sink. In this problem, every node, except\n186",
            "for the source and sink, may appear in at most one path. The number of node-\ndisjoint paths may be smaller than the number of edge-disjoint paths.\nFor example, in the previous graph, the maximum number of node-disjoint\npaths is 1:\n1\n2 3\n4 5\n6\nWe can reduce also this problem to the maximum \ufb02ow problem. Since each\nnode can appear in at most one path, we have to limit the \ufb02ow that goes through\nthe nodes. A standard method for this is to divide each node into two nodes such\nthat the \ufb01rst node has the incoming edges of the original node, the second node\nhas the outgoing edges of the original node, and there is a new edge from the \ufb01rst\nnode to the second node.\nIn our example, the graph becomes as follows:\n1\n2 3\n4 5\n2 3\n4 5\n6\nThe maximum \ufb02ow for the graph is as follows:\n1\n2 3\n4 5\n2 3\n4 5\n6\nThus, the maximum number of node-disjoint paths from the source to the\nsink is 1.\n20.3 Maximum matchings\nThe maximum matching problem asks to \ufb01nd a maximum-size set of node\npairs in an undirected graph such that each pair is connected with an edge and\neach node belongs to at most one pair.\nThere are polynomial algorithms for \ufb01nding maximum matchings in general\ngraphs [17], but such algorithms are complex and rarely seen in programming\ncontests. However, in bipartite graphs, the maximum matching problem is much\neasier to solve, because we can reduce it to the maximum \ufb02ow problem.\n187",
            "Finding maximum matchings\nThe nodes of a bipartite graph can be always divided into two groups such that\nall edges of the graph go from the left group to the right group. For example, in\nthe following bipartite graph, the groups are {1,2,3,4} and {5,6,7,8}.\n1\n2\n3\n4\n5\n6\n7\n8\nThe size of a maximum matching of this graph is 3:\n1\n2\n3\n4\n5\n6\n7\n8\nWe can reduce the bipartite maximum matching problem to the maximum\n\ufb02ow problem by adding two new nodes to the graph: a source and a sink. We also\nadd edges from the source to each left node and from each right node to the sink.\nAfter this, the size of a maximum \ufb02ow in the graph equals the size of a maximum\nmatching in the original graph.\nFor example, the reduction for the above graph is as follows:\n1\n2\n3\n4\n5\n6\n7\n8\nThe maximum \ufb02ow of this graph is as follows:\n1\n2\n3\n4\n5\n6\n7\n8\n188",
            "Hall\u2019s theorem\nHall\u2019s theoremcan be used to \ufb01nd out whether a bipartite graph has a matching\nthat contains all left or right nodes. If the number of left and right nodes is the\nsame, Hall\u2019s theorem tells us if it is possible to construct aperfect matching\nthat contains all nodes of the graph.\nAssume that we want to \ufb01nd a matching that contains all left nodes. Let X\nbe any set of left nodes and let f (X) be the set of their neighbors. According to\nHall\u2019s theorem, a matching that contains all left nodes exists exactly when for\neach X, the condition |X| \u2264 |f (X)| holds.\nLet us study Hall\u2019s theorem in the example graph. First, letX = {1,3} which\nyields f (X) = {5,6,8}:\n1\n2\n3\n4\n5\n6\n7\n8\nThe condition of Hall\u2019s theorem holds, because|X| =2 and |f (X)| =3. Next,\nlet X = {2,4} which yields f (X) = {7}:\n1\n2\n3\n4\n5\n6\n7\n8\nIn this case, |X| =2 and |f (X)| =1, so the condition of Hall\u2019s theorem does\nnot hold. This means that it is not possible to form a perfect matching for the\ngraph. This result is not surprising, because we already know that the maximum\nmatching of the graph is 3 and not 4.\nIf the condition of Hall\u2019s theorem does not hold, the setX provides an expla-\nnation why we cannot form such a matching. Since X contains more nodes than\nf (X), there are no pairs for all nodes in X. For example, in the above graph, both\nnodes 2 and 4 should be connected with node 7 which is not possible.\nK\u0151nig\u2019s theorem\nA minimum node cover of a graph is a minimum set of nodes such that each\nedge of the graph has at least one endpoint in the set. In a general graph, \ufb01nding\na minimum node cover is a NP-hard problem. However, if the graph is bipartite,\nK\u02ddonig\u2019s theoremtells us that the size of a minimum node cover and the size\n189",
            "of a maximum matching are always equal. Thus, we can calculate the size of a\nminimum node cover using a maximum \ufb02ow algorithm.\nLet us consider the following graph with a maximum matching of size 3:\n1\n2\n3\n4\n5\n6\n7\n8\nNow K\u02ddonig\u2019s theorem tells us that the size of a minimum node cover is also 3.\nSuch a cover can be constructed as follows:\n1\n2\n3\n4\n5\n6\n7\n8\nThe nodes that do not belong to a minimum node cover form a maximum\nindependent set. This is the largest possible set of nodes such that no two\nnodes in the set are connected with an edge. Once again, \ufb01nding a maximum\nindependent set in a general graph is a NP-hard problem, but in a bipartite graph\nwe can use K\u02ddonig\u2019s theorem to solve the problem ef\ufb01ciently. In the example graph,\nthe maximum independent set is as follows:\n1\n2\n3\n4\n5\n6\n7\n8\n20.4 Path covers\nA path cover is a set of paths in a graph such that each node of the graph\nbelongs to at least one path. It turns out that in directed, acyclic graphs, we can\nreduce the problem of \ufb01nding a minimum path cover to the problem of \ufb01nding a\nmaximum \ufb02ow in another graph.\n190",
            "Node-disjoint path cover\nIn a node-disjoint path cover, each node belongs to exactly one path. As an\nexample, consider the following graph:\n1 2 3 4\n5 6 7\nA minimum node-disjoint path cover of this graph consists of three paths. For\nexample, we can choose the following paths:\n1 2 3 4\n5 6 7\nNote that one of the paths only contains node 2, so it is possible that a path\ndoes not contain any edges.\nWe can \ufb01nd a minimum node-disjoint path cover by constructing a matching\ngraph where each node of the original graph is represented by two nodes: a left\nnode and a right node. There is an edge from a left node to a right node if there\nis such an edge in the original graph. In addition, the matching graph contains a\nsource and a sink, and there are edges from the source to all left nodes and from\nall right nodes to the sink.\nA maximum matching in the resulting graph corresponds to a minimum node-\ndisjoint path cover in the original graph. For example, the following matching\ngraph for the above graph contains a maximum matching of size 4:\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\n4\n5\n6\n7\nEach edge in the maximum matching of the matching graph corresponds to\nan edge in the minimum node-disjoint path cover of the original graph. Thus, the\nsize of the minimum node-disjoint path cover is n \u2212 c, where n is the number of\nnodes in the original graph and c is the size of the maximum matching.\n191",
            "General path cover\nA general path cover is a path cover where a node can belong to more than\none path. A minimum general path cover may be smaller than a minimum\nnode-disjoint path cover, because a node can be used multiple times in paths.\nConsider again the following graph:\n1 2 3 4\n5 6 7\nThe minimum general path cover of this graph consists of two paths. For\nexample, the \ufb01rst path may be as follows:\n1 2 3 4\n5 6 7\nAnd the second path may be as follows:\n1 2 3 4\n5 6 7\nA minimum general path cover can be found almost like a minimum node-\ndisjoint path cover. It suf\ufb01ces to add some new edges to the matching graph\nso that there is an edge a \u2192 b always when there is a path from a to b in the\noriginal graph (possibly through several edges).\nThe matching graph for the above graph is as follows:\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\n4\n5\n6\n7\n192",
            "Dilworth\u2019s theorem\nAn antichain is a set of nodes of a graph such that there is no path from any\nnode to another node using the edges of the graph. Dilworth\u2019s theoremstates\nthat in a directed acyclic graph, the size of a minimum general path cover equals\nthe size of a maximum antichain.\nFor example, nodes 3 and 7 form an antichain in the following graph:\n1 2 3 4\n5 6 7\nThis is a maximum antichain, because it is not possible to construct any\nantichain that would contain three nodes. We have seen before that the size of a\nminimum general path cover of this graph consists of two paths.\n193",
            "194",
            "Part III\nAdvanced topics\n195",
            "",
            "Chapter 21\nNumber theory\nNumber theory is a branch of mathematics that studies integers. Number\ntheory is a fascinating \ufb01eld, because many questions involving integers are very\ndif\ufb01cult to solve even if they seem simple at \ufb01rst glance.\nAs an example, consider the following equation:\nx3 + y3 + z3 = 33\nIt is easy to \ufb01nd three real numbers x, y and z that satisfy the equation. For\nexample, we can choose\nx = 3,\ny =\n3\u2282\n3,\nz =\n3\u2282\n3.\nHowever, it is an open problem in number theory if there are any three integers\nx, y and z that would satisfy the equation [6].\nIn this chapter, we will focus on basic concepts and algorithms in number\ntheory. Throughout the chapter, we will assume that all numbers are integers, if\nnot otherwise stated.\n21.1 Primes and factors\nA number a is called a factor or a divisor of a number b if a divides b. If a is a\nfactor of b, we write a | b, and otherwise we write a \u2224 b. For example, the factors\nof 24 are 1, 2, 3, 4, 6, 8, 12 and 24.\nA number n > 1 is a prime if its only positive factors are 1 andn. For example,\n7, 19 and 41 are primes, but 35 is not a prime, because 5\u00b77 = 35. For every number\nn > 1, there is a unique prime factorization\nn = p\u03b11\n1 p\u03b12\n2 \u00b7\u00b7\u00b7 p\u03b1k\nk ,\nwhere p1, p2,..., pk are distinct primes and \u03b11,\u03b12,..., \u03b1k are positive numbers.\nFor example, the prime factorization for 84 is\n84 = 22 \u00b731 \u00b771.\n197",
            "The number of factors of a number n is\n\u03c4(n) =\nk\u220f\ni=1\n(\u03b1i +1),\nbecause for each prime pi, there are \u03b1i +1 ways to choose how many times it\nappears in the factor. For example, the number of factors of 84 is\u03c4(84) = 3\u00b72\u00b72 = 12.\nThe factors are 1, 2, 3, 4, 6, 7, 12, 14, 21, 28, 42 and 84.\nThe sum of factors of n is\n\u03c3(n) =\nk\u220f\ni=1\n(1+ pi +... + p\u03b1i\ni ) =\nk\u220f\ni=1\npai+1\ni \u22121\npi \u22121 ,\nwhere the latter formula is based on the geometric progression formula. For\nexample, the sum of factors of 84 is\n\u03c3(84) = 23 \u22121\n2\u22121 \u00b7 32 \u22121\n3\u22121 \u00b7 72 \u22121\n7\u22121 = 7\u00b74\u00b78 = 224.\nThe product of factors of n is\n\u00b5(n) = n\u03c4(n)/2,\nbecause we can form \u03c4(n)/2 pairs from the factors, each with product n. For\nexample, the factors of 84 produce the pairs 1 \u00b784, 2 \u00b742, 3 \u00b728, etc., and the\nproduct of the factors is \u00b5(84) = 846 = 351298031616.\nA number n is called a perfect number if n = \u03c3(n)\u2212n, i.e., n equals the sum\nof its factors between 1 and n \u22121. For example, 28 is a perfect number, because\n28 = 1+2+4+7+14.\nNumber of primes\nIt is easy to show that there is an in\ufb01nite number of primes. If the number of\nprimes would be \ufb01nite, we could construct a set P = {p1, p2,..., pn} that would\ncontain all the primes. For example, p1 = 2, p2 = 3, p3 = 5, and so on. However,\nusing P, we could form a new prime\np1 p2 \u00b7\u00b7\u00b7 pn +1\nthat is larger than all elements in P. This is a contradiction, and the number of\nprimes has to be in\ufb01nite.\nDensity of primes\nThe density of primes means how often there are primes among the numbers.\nLet \u03c0(n) denote the number of primes between 1 and n. For example, \u03c0(10) = 4,\nbecause there are 4 primes between 1 and 10: 2, 3, 5 and 7.\nIt is possible to show that\n\u03c0(n) \u2248 n\nlnn,\nwhich means that primes are quite frequent. For example, the number of primes\nbetween 1 and 106 is \u03c0(106) = 78498, and 106/ln10 6 \u2248 72382.\n198",
            "Conjectures\nThere are many conjectures involving primes. Most people think that the con-\njectures are true, but nobody has been able to prove them. For example, the\nfollowing conjectures are famous:\n\u2022 Goldbach\u2019s conjecture: Each even integer n > 2 can be represented as a\nsum n = a +b so that both a and b are primes.\n\u2022 Twin prime conjecture: There is an in\ufb01nite number of pairs of the form\n{p, p +2}, where both p and p +2 are primes.\n\u2022 Legendre\u2019s conjecture: There is always a prime between numbers n2\nand (n +1)2, where n is any positive integer.\nBasic algorithms\nIf a number n is not prime, it can be represented as a product a \u00b7b, where a \u2264 \u2282n\nor b \u2264 \u2282n, so it certainly has a factor between 2 and\u230a\u2282n\u230b. Using this observation,\nwe can both test if a number is prime and \ufb01nd the prime factorization of a number\nin O(\u2282n) time.\nThe following function prime checks if the given number n is prime. The\nfunction attempts to divide n by all numbers between 2 and \u230a\u2282n\u230b, and if none of\nthem divides n, then n is prime.\nbool prime(int n) {\nif (n < 2) return false;\nfor (int x = 2; x*x <= n; x++) {\nif (n%x == 0) return false;\n}\nreturn true;\n}\nThe following function factors constructs a vector that contains the prime factor-\nization of n. The function divides n by its prime factors, and adds them to the\nvector. The process ends when the remaining number n has no factors between 2\nand \u230a\u2282n\u230b. If n > 1, it is prime and the last factor.\nvector<int> factors(int n) {\nvector<int> f;\nfor (int x = 2; x*x <= n; x++) {\nwhile (n%x == 0) {\nf.push_back(x);\nn /= x;\n}\n}\nif (n > 1) f.push_back(n);\nreturn f;\n}\n199",
            "Note that each prime factor appears in the vector as many times as it divides\nthe number. For example, 24= 23 \u00b73, so the result of the function is [2,2,2,3].\nSieve of Eratosthenes\nThe sieve of Eratosthenes is a preprocessing algorithm that builds an array\nusing which we can ef\ufb01ciently check if a given number between 2... n is prime\nand, if it is not, \ufb01nd one prime factor of the number.\nThe algorithm builds an array sieve whose positions 2,3,..., n are used. The\nvalue sieve[k] = 0 means that k is prime, and the value sieve[k] \u0338= 0 means that\nk is not a prime and one of its prime factors is sieve[k].\nThe algorithm iterates through the numbers 2... n one by one. Always when a\nnew prime x is found, the algorithm records that the multiples of x (2x,3x,4x,... )\nare not primes, because the number x divides them.\nFor example, if n = 20, the array is as follows:\n0 0 2 0 3 0 2 3 5 0 3 0 7 5 2 0 3 0 5\n2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nThe following code implements the sieve of Eratosthenes. The code assumes\nthat each element of sieve is initially zero.\nfor (int x = 2; x <= n; x++) {\nif (sieve[x]) continue;\nfor (int u = 2*x; u <= n; u += x) {\nsieve[u] = x;\n}\n}\nThe inner loop of the algorithm is executedn/x times for each value of x. Thus,\nan upper bound for the running time of the algorithm is the harmonic sum\nn\u2211\nx=2\nn/x = n/2+n/3+n/4+\u00b7\u00b7\u00b7+ n/n = O(nlogn).\nIn fact, the algorithm is more ef\ufb01cient, because the inner loop will be executed\nonly if the number x is prime. It can be shown that the running time of the\nalgorithm is only O(nloglog n), a complexity very near to O(n).\nEuclid\u2019s algorithm\nThe greatest common divisor of numbers a and b, gcd(a,b), is the greatest\nnumber that divides both a and b, and the least common multiple of a and b,\nlcm(a,b), is the smallest number that is divisible by both a and b. For example,\ngcd(24,36) = 12 and lcm(24,36) = 72.\nThe greatest common divisor and the least common multiple are connected as\nfollows:\nlcm(a,b) = ab\ngcd(a,b)\n200",
            "Euclid\u2019s algorithm1 provides an ef\ufb01cient way to \ufb01nd the greatest common\ndivisor of two numbers. The algorithm is based on the following formula:\ngcd(a,b) =\n{\na b = 0\ngcd(b,a mod b) b \u0338= 0\nFor example,\ngcd(24,36) = gcd(36,24) = gcd(24,12) = gcd(12,0) = 12.\nThe algorithm can be implemented as follows:\nint gcd(int a, int b) {\nif (b == 0) return a;\nreturn gcd(b, a%b);\n}\nIt can be shown that Euclid\u2019s algorithm works in O(logn) time, where n =\nmin(a,b). The worst case for the algorithm is the case when a and b are consecu-\ntive Fibonacci numbers. For example,\ngcd(13,8) = gcd(8,5) = gcd(5,3) = gcd(3,2) = gcd(2,1) = gcd(1,0) = 1.\nEuler\u2019s totient function\nNumbers a and b are coprime if gcd(a,b) = 1. Euler\u2019s totient function\u03d5(n)\ngives the number of coprime numbers to n between 1 and n. For example,\n\u03d5(12) = 4, because 1, 5, 7 and 11 are coprime to 12.\nThe value of \u03d5(n) can be calculated from the prime factorization of n using\nthe formula\n\u03d5(n) =\nk\u220f\ni=1\np\u03b1i\u22121\ni (pi \u22121).\nFor example, \u03d5(12) = 21 \u00b7(2\u22121)\u00b730 \u00b7(3\u22121) = 4. Note that \u03d5(n) = n \u22121 if n is prime.\n21.2 Modular arithmetic\nIn modular arithmetic, the set of numbers is limited so that only numbers\n0,1,2,..., m \u22121 are used, where m is a constant. Each number x is represented\nby the number x mod m: the remainder after dividing x by m. For example, if\nm = 17, then 75 is represented by 75 mod 17 = 7.\nOften we can take remainders before doing calculations. In particular, the\nfollowing formulas hold:\n(x + y) mod m = (x mod m + y mod m) mod m\n(x \u2212 y) mod m = (x mod m \u2212 y mod m) mod m\n(x \u00b7 y) mod m = (x mod m \u00b7 y mod m) mod m\nxn mod m = (x mod m)n mod m\n1Euclid was a Greek mathematician who lived in about 300 BC. This is perhaps the \ufb01rst\nknown algorithm in history.\n201",
            "Modular exponentiation\nThere is often need to ef\ufb01ciently calculate the value of xn mod m. This can be\ndone in O(logn) time using the following recursion:\nxn =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n1 n = 0\nxn/2 \u00b7 xn/2 n is even\nxn\u22121 \u00b7 x n is odd\nIt is important that in the case of an even n, the value of xn/2 is calculated\nonly once. This guarantees that the time complexity of the algorithm is O(logn),\nbecause n is always halved when it is even.\nThe following function calculates the value of xn mod m:\nint modpow(int x, int n, int m) {\nif (n == 0) return 1%m;\nlong long u = modpow(x,n/2,m);\nu = (u*u)%m;\nif (n%2 == 1) u = (u*x)%m;\nreturn u;\n}\nFermat\u2019s theorem and Euler\u2019s theorem\nFermat\u2019s theoremstates that\nxm\u22121 mod m = 1\nwhen m is prime and x and m are coprime. This also yields\nxk mod m = xk mod (m\u22121) mod m.\nMore generally, Euler\u2019s theoremstates that\nx\u03d5(m) mod m = 1\nwhen x and m are coprime. Fermat\u2019s theorem follows from Euler\u2019s theorem,\nbecause if m is a prime, then \u03d5(m) = m \u22121.\nModular inverse\nThe inverse of x modulo m is a number x\u22121 such that\nxx\u22121 mod m = 1.\nFor example, if x = 6 and m = 17, then x\u22121 = 3, because 6\u00b73 mod 17 = 1.\nUsing modular inverses, we can divide numbers modulo m, because division\nby x corresponds to multiplication by x\u22121. For example, to evaluate the value\n202",
            "of 36/6 mod 17, we can use the formula 2\u00b73 mod 17, because 36 mod 17 = 2 and\n6\u22121 mod 17 = 3.\nHowever, a modular inverse does not always exist. For example, if x = 2 and\nm = 4, the equation\nxx\u22121 mod m = 1\ncannot be solved, because all multiples of 2 are even and the remainder can never\nbe 1 when m = 4. It turns out that the value of x\u22121 mod m can be calculated\nexactly when x and m are coprime.\nIf a modular inverse exists, it can be calculated using the formula\nx\u22121 = x\u03d5(m)\u22121.\nIf m is prime, the formula becomes\nx\u22121 = xm\u22122.\nFor example,\n6\u22121 mod 17 = 617\u22122 mod 17 = 3.\nThis formula allows us to ef\ufb01ciently calculate modular inverses using the\nmodular exponentation algorithm. The formula can be derived using Euler\u2019s\ntheorem. First, the modular inverse should satisfy the following equation:\nxx\u22121 mod m = 1.\nOn the other hand, according to Euler\u2019s theorem,\nx\u03d5(m) mod m = xx\u03d5(m)\u22121 mod m = 1,\nso the numbers x\u22121 and x\u03d5(m)\u22121 are equal.\nComputer arithmetic\nIn programming, unsigned integers are represented modulo 2k, where k is the\nnumber of bits of the data type. A usual consequence of this is that a number\nwraps around if it becomes too large.\nFor example, in C++, numbers of type unsigned int are represented mod-\nulo 232. The following code declares an unsigned int variable whose value is\n123456789. After this, the value will be multiplied by itself, and the result is\n1234567892 mod 232 = 2537071545.\nunsigned int x = 123456789;\ncout << x*x << \"\\n\"; // 2537071545\n203",
            "21.3 Solving equations\nDiophantine equations\nA Diophantine equation is an equation of the form\nax +by = c,\nwhere a, b and c are constants and the values of x and y should be found. Each\nnumber in the equation has to be an integer. For example, one solution for the\nequation 5x +2y = 11 is x = 3 and y = \u22122.\nWe can ef\ufb01ciently solve a Diophantine equation by using Euclid\u2019s algorithm.\nIt turns out that we can extend Euclid\u2019s algorithm so that it will \ufb01nd numbersx\nand y that satisfy the following equation:\nax +by = gcd(a,b)\nA Diophantine equation can be solved if c is divisible by gcd(a,b), and other-\nwise it cannot be solved.\nAs an example, let us \ufb01nd numbersx and y that satisfy the following equation:\n39x +15y = 12\nThe equation can be solved, because gcd(39,15) = 3 and 3 | 12. When Euclid\u2019s\nalgorithm calculates the greatest common divisor of 39 and 15, it produces the\nfollowing sequence of function calls:\ngcd(39,15) = gcd(15,9) = gcd(9,6) = gcd(6,3) = gcd(3,0) = 3\nThis corresponds to the following equations:\n39\u22122\u00b715 = 9\n15\u22121\u00b79 = 6\n9\u22121\u00b76 = 3\nUsing these equations, we can derive\n39\u00b72+15\u00b7(\u22125) = 3\nand by multiplying this by 4, the result is\n39\u00b78+15\u00b7(\u221220) = 12,\nso a solution to the equation is x = 8 and y = \u221220.\nA solution to a Diophantine equation is not unique, because we can form an\nin\ufb01nite number of solutions if we know one solution. If a pair ( x, y) is a solution,\nthen also all pairs\n(x + kb\ngcd(a,b), y\u2212 ka\ngcd(a,b))\nare solutions, where k is any integer.\n204",
            "Chinese remainder theorem\nThe Chinese remainder theorem solves a group of equations of the form\nx = a1 mod m1\nx = a2 mod m2\n\u00b7\u00b7\u00b7\nx = an mod mn\nwhere all pairs of m1,m2,..., mn are coprime.\nLet x\u22121\nm be the inverse of x modulo m, and\nXk = m1m2 \u00b7\u00b7\u00b7 mn\nmk\n.\nUsing this notation, a solution to the equations is\nx = a1 X1 X1\n\u22121\nm1 +a2 X2 X2\n\u22121\nm2 +\u00b7\u00b7\u00b7+ an Xn Xn\n\u22121\nmn .\nIn this solution, for each k = 1,2,..., n,\nak Xk Xk\n\u22121\nmk mod mk = ak,\nbecause\nXk Xk\n\u22121\nmk mod mk = 1.\nSince all other terms in the sum are divisible by mk, they have no effect on the\nremainder, and x mod mk = ak.\nFor example, a solution for\nx = 3 mod 5\nx = 4 mod 7\nx = 2 mod 3\nis\n3\u00b721\u00b71+4\u00b715\u00b71+2\u00b735\u00b72 = 263.\nOnce we have found a solution x, we can create an in\ufb01nite number of other\nsolutions, because all numbers of the form\nx +m1m2 \u00b7\u00b7\u00b7 mn\nare solutions.\n21.4 Other results\nLagrange\u2019s theorem\nLagrange\u2019s theoremstates that every positive integer can be represented as a\nsum of four squares, i.e., a2 +b2 + c2 +d2. For example, the number 123 can be\nrepresented as the sum 82 +52 +52 +32.\n205",
            "Zeckendorf\u2019s theorem\nZeckendorf\u2019s theoremstates that every positive integer has a unique repre-\nsentation as a sum of Fibonacci numbers such that no two numbers are equal or\nconsecutive Fibonacci numbers. For example, the number 74 can be represented\nas the sum 55+13+5+1.\nPythagorean triples\nA Pythagorean triple is a triple (a,b, c) that satis\ufb01es the Pythagorean theorem\na2 +b2 = c2, which means that there is a right triangle with side lengths a, b and\nc. For example, (3,4,5) is a Pythagorean triple.\nIf (a,b, c) is a Pythagorean triple, all triples of the form (ka,kb,kc) are also\nPythagorean triples where k > 1. A Pythagorean triple is primitive if a, b and\nc are coprime, and all Pythagorean triples can be constructed from primitive\ntriples using a multiplier k.\nEuclid\u2019s formulacan be used to produce all primitive Pythagorean triples.\nEach such triple is of the form\n(n2 \u2212m2,2nm,n2 +m2),\nwhere 0 < m < n, n and m are coprime and at least one of n and m is even. For\nexample, when m = 1 and n = 2, the formula produces the smallest Pythagorean\ntriple\n(22 \u221212,2\u00b72\u00b71,22 +12) = (3,4,5).\nWilson\u2019s theorem\nWilson\u2019s theoremstates that a number n is prime exactly when\n(n \u22121)! mod n = n \u22121.\nFor example, the number 11 is prime, because\n10! mod 11 = 10,\nand the number 12 is not prime, because\n11! mod 12 = 0 \u0338= 11.\nHence, Wilson\u2019s theorem can be used to \ufb01nd out whether a number is prime.\nHowever, in practice, the theorem cannot be applied to large values of n, because\nit is dif\ufb01cult to calculate values of (n \u22121)! when n is large.\n206",
            "Chapter 22\nCombinatorics\nCombinatorics studies methods for counting combinations of objects. Usually,\nthe goal is to \ufb01nd a way to count the combinations ef\ufb01ciently without generating\neach combination separately.\nAs an example, consider the problem of counting the number of ways to\nrepresent an integer n as a sum of positive integers. For example, there are 8\nrepresentations for 4:\n\u2022 1 +1+1+1\n\u2022 1 +1+2\n\u2022 1 +2+1\n\u2022 2 +1+1\n\u2022 2 +2\n\u2022 3 +1\n\u2022 1 +3\n\u2022 4\nA combinatorial problem can often be solved using a recursive function. In this\nproblem, we can de\ufb01ne a function f (n) that gives the number of representations\nfor n. For example, f (4) = 8 according to the above example. The values of the\nfunction can be recursively calculated as follows:\nf (n) =\n{\n1 n = 0\nf (0)+ f (1)+\u00b7\u00b7\u00b7+ f (n \u22121) n > 0\nThe base case is f (0) = 1, because the empty sum represents the number 0. Then,\nif n > 0, we consider all ways to choose the \ufb01rst number of the sum. If the \ufb01rst\nnumber is k, there are f (n\u2212k) representations for the remaining part of the sum.\nThus, we calculate the sum of all values of the form f (n \u2212k) where k < n.\nThe \ufb01rst values for the function are:\nf (0) = 1\nf (1) = 1\nf (2) = 2\nf (3) = 4\nf (4) = 8\nSometimes, a recursive formula can be replaced with a closed-form formula.\nIn this problem,\nf (n) = 2n\u22121,\n207",
            "which is based on the fact that there are n\u22121 possible positions for +-signs in the\nsum and we can choose any subset of them.\n22.1 Binomial coef\ufb01cients\nThe binomial coef\ufb01cient\n(n\nk\n)\nequals the number of ways we can choose a subset\nof k elements from a set of n elements. For example,\n(5\n3\n)\n= 10, because the set\n{1,2,3,4,5} has 10 subsets of 3 elements:\n{1,2,3},{1,2,4},{1,2,5},{1,3,4},{1,3,5},{1,4,5},{2,3,4},{2,3,5},{2,4,5},{3,4,5}\nFormula 1\nBinomial coef\ufb01cients can be recursively calculated as follows:\n(\nn\nk\n)\n=\n(\nn \u22121\nk \u22121\n)\n+\n(\nn \u22121\nk\n)\nThe idea is to \ufb01x an element x in the set. If x is included in the subset, we\nhave to choose k \u22121 elements from n \u22121 elements, and if x is not included in the\nsubset, we have to choose k elements from n \u22121 elements.\nThe base cases for the recursion are\n(\nn\n0\n)\n=\n(\nn\nn\n)\n= 1,\nbecause there is always exactly one way to construct an empty subset and a\nsubset that contains all the elements.\nFormula 2\nAnother way to calculate binomial coef\ufb01cients is as follows:\n(\nn\nk\n)\n= n!\nk!(n \u2212k)!.\nThere are n! permutations of n elements. We go through all permutations\nand always include the \ufb01rst k elements of the permutation in the subset. Since\nthe order of the elements in the subset and outside the subset does not matter,\nthe result is divided by k! and (n \u2212k)!\nProperties\nFor binomial coef\ufb01cients, (\nn\nk\n)\n=\n(\nn\nn \u2212k\n)\n,\n208",
            "because we actually divide a set of n elements into two subsets: the \ufb01rst contains\nk elements and the second contains n \u2212k elements.\nThe sum of binomial coef\ufb01cients is(\nn\n0\n)\n+\n(\nn\n1\n)\n+\n(\nn\n2\n)\n+... +\n(\nn\nn\n)\n= 2n.\nThe reason for the name \u201dbinomial coef\ufb01cient\u201d can be seen when the binomial\n(a +b) is raised to the nth power:\n(a +b)n =\n(\nn\n0\n)\nanb0 +\n(\nn\n1\n)\nan\u22121b1 +... +\n(\nn\nn \u22121\n)\na1bn\u22121 +\n(\nn\nn\n)\na0bn.\nBinomial coef\ufb01cients also appear in Pascal\u2019s trianglewhere each value\nequals the sum of two above values:\n1\n1 1\n1 2 1\n1 3 3 1\n1 4 6 4 1\n... ... ... ... ...\nBoxes and balls\n\u201dBoxes and balls\u201d is a useful model, where we count the ways to placek balls in n\nboxes. Let us consider three scenarios:\nScenario 1: Each box can contain at most one ball. For example, when n = 5\nand k = 2, there are 10 solutions:\nIn this scenario, the answer is directly the binomial coef\ufb01cient\n(n\nk\n)\n.\nScenario 2: A box can contain multiple balls. For example, when n = 5 and\nk = 2, there are 15 solutions:\n209",
            "The process of placing the balls in the boxes can be represented as a string\nthat consists of symbols \u201do\u201d and \u201d\u2192\u201d. Initially, assume that we are standing at\nthe leftmost box. The symbol \u201do\u201d means that we place a ball in the current box,\nand the symbol \u201d\u2192\u201d means that we move to the next box to the right.\nUsing this notation, each solution is a string that contains k times the symbol\n\u201do\u201d andn \u22121 times the symbol \u201d\u2192\u201d. For example, the upper-right solution in the\nabove picture corresponds to the string \u201d\u2192 \u2192o \u2192 o \u2192\u201d. Thus, the number of\nsolutions is\n(k+n\u22121\nk\n)\n.\nScenario 3: Each box may contain at most one ball, and in addition, no two\nadjacent boxes may both contain a ball. For example, when n = 5 and k = 2, there\nare 6 solutions:\nIn this scenario, we can assume that k balls are initially placed in boxes and\nthere is an empty box between each two adjacent boxes. The remaining task is\nto choose the positions for the remaining empty boxes. There are n \u22122k +1 such\nboxes and k +1 positions for them. Thus, using the formula of scenario 2, the\nnumber of solutions is\n(n\u2212k+1\nn\u22122k+1\n)\n.\nMultinomial coe\ufb03cients\nThe multinomial coef\ufb01cient\n(\nn\nk1,k2,..., km\n)\n= n!\nk1!k2!\u00b7\u00b7\u00b7 km!,\nequals the number of ways we can divide n elements into subsets of sizes\nk1,k2,..., km, where k1 +k2 +\u00b7\u00b7\u00b7+ km = n. Multinomial coef\ufb01cients can be seen as\na generalization of binomial cof\ufb01cients; if m = 2, the above formula corresponds\nto the binomial coef\ufb01cient formula.\n22.2 Catalan numbers\nThe Catalan number Cn equals the number of valid parenthesis expressions\nthat consist of n left parentheses and n right parentheses.\nFor example, C3 = 5, because we can construct the following parenthesis\nexpressions using three left and right parentheses:\n\u2022 ()()()\n\u2022 (())()\n\u2022 ()(())\n\u2022 ((()))\n\u2022 (()())\n210",
            "Parenthesis expressions\nWhat is exactly a valid parenthesis expression? The following rules precisely\nde\ufb01ne all valid parenthesis expressions:\n\u2022 An empty parenthesis expression is valid.\n\u2022 If an expression A is valid, then also the expression (A) is valid.\n\u2022 If expressions A and B are valid, then also the expression AB is valid.\nAnother way to characterize valid parenthesis expressions is that if we choose\nany pre\ufb01x of such an expression, it has to contain at least as many left parenthe-\nses as right parentheses. In addition, the complete expression has to contain an\nequal number of left and right parentheses.\nFormula 1\nCatalan numbers can be calculated using the formula\nCn =\nn\u22121\u2211\ni=0\nCiCn\u2212i\u22121.\nThe sum goes through the ways to divide the expression into two parts such\nthat both parts are valid expressions and the \ufb01rst part is as short as possible but\nnot empty. For any i, the \ufb01rst part contains i +1 pairs of parentheses and the\nnumber of expressions is the product of the following values:\n\u2022 Ci: the number of ways to construct an expression using the parentheses of\nthe \ufb01rst part, not counting the outermost parentheses\n\u2022 Cn\u2212i\u22121: the number of ways to construct an expression using the parenthe-\nses of the second part\nThe base case is C0 = 1, because we can construct an empty parenthesis\nexpression using zero pairs of parentheses.\nFormula 2\nCatalan numbers can also be calculated using binomial coef\ufb01cients:\nCn = 1\nn +1\n(\n2n\nn\n)\nThe formula can be explained as follows:\nThere are a total of\n(2n\nn\n)\nways to construct a (not necessarily valid) parenthesis\nexpression that contains n left parentheses and n right parentheses. Let us\ncalculate the number of such expressions that are not valid.\nIf a parenthesis expression is not valid, it has to contain a pre\ufb01x where\nthe number of right parentheses exceeds the number of left parentheses. The\n211",
            "idea is to reverse each parenthesis that belongs to such a pre\ufb01x. For example,\nthe expression ())()( contains a pre\ufb01x ()), and after reversing the pre\ufb01x, the\nexpression becomes )((()(.\nThe resulting expression consists of n +1 left parentheses and n \u22121 right\nparentheses. The number of such expressions is\n( 2n\nn+1\n)\n, which equals the number\nof non-valid parenthesis expressions. Thus, the number of valid parenthesis\nexpressions can be calculated using the formula\n(\n2n\nn\n)\n\u2212\n(\n2n\nn +1\n)\n=\n(\n2n\nn\n)\n\u2212 n\nn +1\n(\n2n\nn\n)\n= 1\nn +1\n(\n2n\nn\n)\n.\nCounting trees\nCatalan numbers are also related to trees:\n\u2022 there are Cn binary trees of n nodes\n\u2022 there are Cn\u22121 rooted trees of n nodes\nFor example, for C3 = 5, the binary trees are\nand the rooted trees are\n22.3 Inclusion-exclusion\nInclusion-exclusion is a technique that can be used for counting the size of a\nunion of sets when the sizes of the intersections are known, and vice versa. A\nsimple example of the technique is the formula\n|A \u222aB| = |A|+| B|\u2212| A \u2229B|,\nwhere A and B are sets and |X| denotes the size of X. The formula can be\nillustrated as follows:\nA BA \u2229B\n212",
            "Our goal is to calculate the size of the union A \u222aB that corresponds to the\narea of the region that belongs to at least one circle. The picture shows that we\ncan calculate the area of A \u222aB by \ufb01rst summing the areas of A and B and then\nsubtracting the area of A \u2229B.\nThe same idea can be applied when the number of sets is larger. When there\nare three sets, the inclusion-exclusion formula is\n|A \u222aB \u222aC| = |A|+| B|+| C|\u2212| A \u2229B|\u2212| A \u2229C|\u2212| B \u2229C|+| A \u2229B \u2229C|\nand the corresponding picture is\nA B\nC\nA \u2229B\nA \u2229C B \u2229C\nA \u2229B \u2229C\nIn the general case, the size of the union X1 \u222a X2 \u222a \u00b7\u00b7\u00b7 \u222aXn can be calcu-\nlated by going through all possible intersections that contain some of the sets\nX1, X2,..., Xn. If the intersection contains an odd number of sets, its size is added\nto the answer, and otherwise its size is subtracted from the answer.\nNote that there are similar formulas for calculating the size of an intersection\nfrom the sizes of unions. For example,\n|A \u2229B| = |A|+| B|\u2212| A \u222aB|\nand\n|A \u2229B \u2229C| = |A|+| B|+| C|\u2212| A \u222aB|\u2212| A \u222aC|\u2212| B \u222aC|+| A \u222aB \u222aC|.\nDerangements\nAs an example, let us count the number ofderangements of elements {1,2,..., n},\ni.e., permutations where no element remains in its original place. For example,\nwhen n = 3, there are two derangements: (2,3,1) and (3,1,2).\nOne approach for solving the problem is to use inclusion-exclusion. Let Xk be\nthe set of permutations that contain the element k at position k. For example,\nwhen n = 3, the sets are as follows:\nX1 = {(1,2,3),(1,3,2)}\nX2 = {(1,2,3),(3,2,1)}\nX3 = {(1,2,3),(2,1,3)}\nUsing these sets, the number of derangements equals\nn!\u2212| X1 \u222a X2 \u222a\u00b7\u00b7\u00b7\u222a Xn|,\n213",
            "so it suf\ufb01ces to calculate the size of the union. Using inclusion-exclusion, this\nreduces to calculating sizes of intersections which can be done ef\ufb01ciently. For\nexample, when n = 3, the size of |X1 \u222a X2 \u222a X3| is\n|X1|+| X2|+| X3|\u2212| X1 \u2229 X2|\u2212| X1 \u2229 X3|\u2212| X2 \u2229 X3|+| X1 \u2229 X2 \u2229 X3|\n= 2+2+2\u22121\u22121\u22121+1\n= 4,\nso the number of solutions is 3!\u22124 = 2.\nIt turns out that the problem can also be solved without using inclusion-\nexclusion. Let f (n) denote the number of derangements for {1,2,..., n}. We can\nuse the following recursive formula:\nf (n) =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n0 n = 1\n1 n = 2\n(n \u22121)(f (n \u22122)+ f (n \u22121)) n > 2\nThe formula can be derived by considering the possibilities how the element 1\nchanges in the derangement. There are n \u22121 ways to choose an element x that\nreplaces the element 1. In each such choice, there are two options:\nOption 1: We also replace the element x with the element 1. After this, the\nremaining task is to construct a derangement of n \u22122 elements.\nOption 2: We replace the element x with some other element than 1. Now we\nhave to construct a derangement of n \u22121 element, because we cannot replace the\nelement x with the element 1, and all other elements must be changed.\n22.4 Burnside\u2019s lemma\nBurnside\u2019s lemmacan be used to count the number of combinations so that\nonly one representative is counted for each group of symmetric combinations.\nBurnside\u2019s lemma states that the number of combinations is\nn\u2211\nk=1\nc(k)\nn ,\nwhere there are n ways to change the position of a combination, and there are\nc(k) combinations that remain unchanged when the kth way is applied.\nAs an example, let us calculate the number of necklaces of n pearls, where\neach pearl has m possible colors. Two necklaces are symmetric if they are similar\nafter rotating them. For example, the necklace\nhas the following symmetric necklaces:\n214",
            "There are n ways to change the position of a necklace, because we can rotate it\n0,1,..., n\u22121 steps clockwise. If the number of steps is 0, all mn necklaces remain\nthe same, and if the number of steps is 1, only the m necklaces where each pearl\nhas the same color remain the same.\nMore generally, when the number of steps is k, a total of\nmgcd(k,n)\nnecklaces remain the same, where gcd(k,n) is the greatest common divisor of k\nand n. The reason for this is that blocks of pearls of size gcd(k,n) will replace\neach other. Thus, according to Burnside\u2019s lemma, the number of necklaces is\nn\u22121\u2211\ni=0\nmgcd(i,n)\nn .\nFor example, the number of necklaces of length 4 with 3 colors is\n34 +3+32 +3\n4 = 24.\n22.5 Cayley\u2019s formula\nCayley\u2019s formulastates that there are nn\u22122 labeled trees that contain n nodes.\nThe nodes are labeled 1,2,..., n, and two trees are different if either their struc-\nture or labeling is different.\nFor example, when n = 4, the number of labeled trees is 44\u22122 = 16:\n1\n2 3 4\n2\n1 3 4\n3\n1 2 4\n4\n1 2 3\n1 2 3 4 1 2 4 3 1 3 2 4\n1 3 4 2 1 4 2 3 1 4 3 2\n2 1 3 4 2 1 4 3 2 3 1 4\n2 4 1 3 3 1 2 4 3 2 1 4\nNext we will see how Cayley\u2019s formula can be derived using Pr\u00fcfer codes.\n215",
            "Pr\u00fcfer code\nA Pr\u00fcfer code is a sequence of n \u22122 numbers that describes a labeled tree. The\ncode is constructed by following a process that removes n\u22122 leaves from the tree.\nAt each step, the leaf with the smallest label is removed, and the label of its only\nneighbor is added to the code.\nFor example, let us calculate the Pr\u00fcfer code of the following graph:\n1 2\n3 4\n5\nFirst we remove node 1 and add node 4 to the code:\n2\n3 4\n5\nThen we remove node 3 and add node 4 to the code:\n2\n4\n5\nFinally we remove node 4 and add node 2 to the code:\n2\n5\nThus, the Pr\u00fcfer code of the graph is [4,4,2].\nWe can construct a Pr\u00fcfer code for any tree, and more importantly, the original\ntree can be reconstructed from a Pr\u00fcfer code. Hence, the number of labeled trees\nof n nodes equals nn\u22122, the number of Pr\u00fcfer codes of size n.\n216",
            "Chapter 23\nMatrices\nA matrix is a mathematical concept that corresponds to a two-dimensional array\nin programming. For example,\nA =\n\uf8ee\n\uf8f0\n6 13 7 4\n7 0 8 2\n9 5 4 18\n\uf8f9\n\uf8fb\nis a matrix of size 3\u00d74, i.e., it has 3 rows and 4 columns. The notation [i, j] refers\nto the element in row i and column j in a matrix. For example, in the above\nmatrix, A[2,3] = 8 and A[3,1] = 9.\nA special case of a matrix is a vector that is a one-dimensional matrix of size\nn \u00d71. For example,\nV =\n\uf8ee\n\uf8f0\n4\n7\n5\n\uf8f9\n\uf8fb\nis a vector that contains three elements.\nThe transpose AT of a matrix A is obtained when the rows and columns of\nA are swapped, i.e., AT[i, j] = A[ j, i]:\nAT =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n6 7 9\n13 0 5\n7 8 4\n4 2 18\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nA matrix is a square matrix if it has the same number of rows and columns.\nFor example, the following matrix is a square matrix:\nS =\n\uf8ee\n\uf8f0\n3 12 4\n5 9 15\n0 2 4\n\uf8f9\n\uf8fb\n23.1 Operations\nThe sum A +B of matrices A and B is de\ufb01ned if the matrices are of the same\nsize. The result is a matrix where each element is the sum of the corresponding\nelements in A and B.\n217",
            "For example,\n[6 1 4\n3 9 2\n]\n+\n[4 9 3\n8 1 3\n]\n=\n[6+4 1 +9 4 +3\n3+8 9 +1 2 +3\n]\n=\n[10 10 7\n11 10 5\n]\n.\nMultiplying a matrix A by a value x means that each element of A is multi-\nplied by x. For example,\n2\u00b7\n[6 1 4\n3 9 2\n]\n=\n[2\u00b76 2 \u00b71 2 \u00b74\n2\u00b73 2 \u00b79 2 \u00b72\n]\n=\n[12 2 8\n6 18 4\n]\n.\nMatrix multiplication\nThe product AB of matrices A and B is de\ufb01ned if A is of size a \u00d7n and B is of\nsize n \u00d7b, i.e., the width of A equals the height of B. The result is a matrix of\nsize a \u00d7b whose elements are calculated using the formula\nAB[i, j] =\nn\u2211\nk=1\nA[i,k]\u00b7B[k, j].\nThe idea is that each element of AB is a sum of products of elements of A and\nB according to the following picture:\nA AB\nB\nFor example,\n\uf8ee\n\uf8f0\n1 4\n3 9\n8 6\n\uf8f9\n\uf8fb\u00b7\n[1 6\n2 9\n]\n=\n\uf8ee\n\uf8f0\n1\u00b71+4\u00b72 1 \u00b76+4\u00b79\n3\u00b71+9\u00b72 3 \u00b76+9\u00b79\n8\u00b71+6\u00b72 8 \u00b76+6\u00b79\n\uf8f9\n\uf8fb=\n\uf8ee\n\uf8f0\n9 42\n21 99\n20 102\n\uf8f9\n\uf8fb.\nMatrix multiplication is associative, so A(BC) = (AB)C holds, but it is not\ncommutative, so AB = BA does not usually hold.\nAn identity matrix is a square matrix where each element on the diagonal\nis 1 and all other elements are 0. For example, the following matrix is the 3\u00d73\nidentity matrix:\nI =\n\uf8ee\n\uf8f0\n1 0 0\n0 1 0\n0 0 1\n\uf8f9\n\uf8fb\n218",
            "Multiplying a matrix by an identity matrix does not change it. For example,\n\uf8ee\n\uf8f0\n1 0 0\n0 1 0\n0 0 1\n\uf8f9\n\uf8fb\u00b7\n\uf8ee\n\uf8f0\n1 4\n3 9\n8 6\n\uf8f9\n\uf8fb=\n\uf8ee\n\uf8f0\n1 4\n3 9\n8 6\n\uf8f9\n\uf8fb and\n\uf8ee\n\uf8f0\n1 4\n3 9\n8 6\n\uf8f9\n\uf8fb\u00b7\n[1 0\n0 1\n]\n=\n\uf8ee\n\uf8f0\n1 4\n3 9\n8 6\n\uf8f9\n\uf8fb.\nUsing a straightforward algorithm, we can calculate the product of two n \u00d7n\nmatrices in O(n3) time. There are also more ef\ufb01cient algorithms for matrix\nmultiplication1, but they are mostly of theoretical interest and such algorithms\nare not necessary in competitive programming.\nMatrix power\nThe power Ak of a matrix A is de\ufb01ned if A is a square matrix. The de\ufb01nition is\nbased on matrix multiplication:\nAk = A \u00b7 A \u00b7 A \u00b7\u00b7\u00b7 A\ued19 \ued18\ued17 \ued1a\nk times\nFor example,\n[2 5\n1 4\n]3\n=\n[2 5\n1 4\n]\n\u00b7\n[2 5\n1 4\n]\n\u00b7\n[2 5\n1 4\n]\n=\n[48 165\n33 114\n]\n.\nIn addition, A0 is an identity matrix. For example,\n[2 5\n1 4\n]0\n=\n[1 0\n0 1\n]\n.\nThe matrix Ak can be ef\ufb01ciently calculated in O(n3 logk) time using the\nalgorithm in Chapter 21.2. For example,\n[2 5\n1 4\n]8\n=\n[2 5\n1 4\n]4\n\u00b7\n[2 5\n1 4\n]4\n.\nDeterminant\nThe determinant det(A) of a matrix A is de\ufb01ned if A is a square matrix. If\nA is of size 1 \u00d71, then det(A) = A[1,1]. The determinant of a larger matrix is\ncalculated recursively using the formula\ndet(A) =\nn\u2211\nj=1\nA[1, j]C[1, j],\nwhere C[i, j] is the cofactor of A at [i, j]. The cofactor is calculated using the\nformula\nC[i, j] = (\u22121)i+j det(M[i, j]),\n1The \ufb01rst such algorithm was Strassen\u2019s algorithm, published in 1969 [ 63], whose time\ncomplexity is O(n2.80735); the best current algorithm [27] works in O(n2.37286) time.\n219",
            "where M[i, j] is obtained by removing row i and column j from A. Due to the\ncoef\ufb01cient (\u22121)i+j in the cofactor, every other determinant is positive and negative.\nFor example,\ndet(\n[3 4\n1 6\n]\n) = 3\u00b76\u22124\u00b71 = 14\nand\ndet(\n\uf8ee\n\uf8f0\n2 4 3\n5 1 6\n7 2 4\n\uf8f9\n\uf8fb) = 2\u00b7det(\n[1 6\n2 4\n]\n)\u22124\u00b7det(\n[5 6\n7 4\n]\n)+3\u00b7det(\n[5 1\n7 2\n]\n) = 81.\nThe determinant of A tells us whether there is an inverse matrix A\u22121 such\nthat A\u00b7A\u22121 = I, where I is an identity matrix. It turns out that A\u22121 exists exactly\nwhen det(A) \u0338= 0, and it can be calculated using the formula\nA\u22121[i, j] = C[ j, i]\ndet (A).\nFor example,\n\uf8ee\n\uf8f0\n2 4 3\n5 1 6\n7 2 4\n\uf8f9\n\uf8fb\n\ued19 \ued18\ued17 \ued1a\nA\n\u00b7 1\n81\n\uf8ee\n\uf8f0\n\u22128 \u221210 21\n22 \u221213 3\n3 24 \u221218\n\uf8f9\n\uf8fb\n\ued19 \ued18\ued17 \ued1a\nA\u22121\n=\n\uf8ee\n\uf8f0\n1 0 0\n0 1 0\n0 0 1\n\uf8f9\n\uf8fb\n\ued19 \ued18\ued17 \ued1a\nI\n.\n23.2 Linear recurrences\nA linear recurrence is a function f (n) whose initial values aref (0), f (1),..., f (k\u2212\n1) and larger values are calculated recursively using the formula\nf (n) = c1 f (n \u22121)+ c2 f (n \u22122)+... + ck f (n \u2212k),\nwhere c1, c2,..., ck are constant coef\ufb01cients.\nDynamic programming can be used to calculate any value of f (n) in O(kn)\ntime by calculating all values of f (0), f (1),..., f (n) one after another. However,\nif k is small, it is possible to calculate f (n) much more ef\ufb01ciently in O(k3 logn)\ntime using matrix operations.\nFibonacci numbers\nA simple example of a linear recurrence is the following function that de\ufb01nes the\nFibonacci numbers:\nf (0) = 0\nf (1) = 1\nf (n) = f (n \u22121)+ f (n \u22122)\nIn this case, k = 2 and c1 = c2 = 1.\n220",
            "To ef\ufb01ciently calculate Fibonacci numbers, we represent the Fibonacci formula\nas a square matrix X of size 2\u00d72, for which the following holds:\nX \u00b7\n[ f (i)\nf (i +1)\n]\n=\n[f (i +1)\nf (i +2)\n]\nThus, values f (i) and f (i +1) are given as \u201dinput\u201d forX, and X calculates values\nf (i +1) and f (i +2) from them. It turns out that such a matrix is\nX =\n[0 1\n1 1\n]\n.\nFor example,\n[0 1\n1 1\n]\n\u00b7\n[f (5)\nf (6)\n]\n=\n[0 1\n1 1\n]\n\u00b7\n[5\n8\n]\n=\n[ 8\n13\n]\n=\n[f (6)\nf (7)\n]\n.\nThus, we can calculate f (n) using the formula\n[ f (n)\nf (n +1)\n]\n= X n \u00b7\n[f (0)\nf (1)\n]\n=\n[0 1\n1 1\n]n\n\u00b7\n[0\n1\n]\n.\nThe value of X n can be calculated in O(logn) time, so the value of f (n) can also\nbe calculated in O(logn) time.\nGeneral case\nLet us now consider the general case where f (n) is any linear recurrence. Again,\nour goal is to construct a matrix X for which\nX \u00b7\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nf (i)\nf (i +1)\n...\nf (i +k \u22121)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nf (i +1)\nf (i +2)\n...\nf (i +k)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\nSuch a matrix is\nX =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 1 0 0 \u00b7\u00b7\u00b7 0\n0 0 1 0 \u00b7\u00b7\u00b7 0\n0 0 0 1 \u00b7\u00b7\u00b7 0\n... ... ... ... ... ...\n0 0 0 0 \u00b7\u00b7\u00b7 1\nck ck\u22121 ck\u22122 ck\u22123 \u00b7\u00b7\u00b7 c1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nIn the \ufb01rst k \u22121 rows, each element is 0 except that one element is 1. These rows\nreplace f (i) with f (i +1), f (i +1) with f (i +2), and so on. The last row contains\nthe coef\ufb01cients of the recurrence to calculate the new value f (i +k).\nNow, f (n) can be calculated in O(k3 logn) time using the formula\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nf (n)\nf (n +1)\n...\nf (n +k \u22121)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb= X n \u00b7\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nf (0)\nf (1)\n...\nf (k \u22121)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb.\n221",
            "23.3 Graphs and matrices\nCounting paths\nThe powers of an adjacency matrix of a graph have an interesting property. When\nV is an adjacency matrix of an unweighted graph, the matrix V n contains the\nnumbers of paths of n edges between the nodes in the graph.\nFor example, for the graph\n1\n4\n2 3\n5 6\nthe adjacency matrix is\nV =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 0 0 1 0 0\n1 0 0 0 1 1\n0 1 0 0 0 0\n0 1 0 0 0 0\n0 0 0 0 0 0\n0 0 1 0 1 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nNow, for example, the matrix\nV4 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 0 1 1 1 0\n2 0 0 0 2 2\n0 2 0 0 0 0\n0 2 0 0 0 0\n0 0 0 0 0 0\n0 0 1 1 1 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ncontains the numbers of paths of 4 edges between the nodes. For example,\nV4[2,5] = 2, because there are two paths of 4 edges from node 2 to node 5:\n2 \u2192 1 \u2192 4 \u2192 2 \u2192 5 and 2 \u2192 6 \u2192 3 \u2192 2 \u2192 5.\nShortest paths\nUsing a similar idea in a weighted graph, we can calculate for each pair of nodes\nthe minimum length of a path between them that contains exactly n edges. To\ncalculate this, we have to de\ufb01ne matrix multiplication in a new way, so that we\ndo not calculate the numbers of paths but minimize the lengths of paths.\n222",
            "As an example, consider the following graph:\n1\n4\n2 3\n5 6\n4 1\n2 4\n1 2 3\n2\nLet us construct an adjacency matrix where \u221e means that an edge does not\nexist, and other values correspond to edge weights. The matrix is\nV =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221e \u221e \u221e 4 \u221e \u221e\n2 \u221e \u221e \u221e 1 2\n\u221e 4 \u221e \u221e \u221e \u221e\n\u221e 1 \u221e \u221e \u221e \u221e\n\u221e \u221e \u221e \u221e \u221e \u221e\n\u221e \u221e 3 \u221e 2 \u221e\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nInstead of the formula\nAB[i, j] =\nn\u2211\nk=1\nA[i,k]\u00b7B[k, j]\nwe now use the formula\nAB[i, j] =\nn\nmin\nk=1\nA[i,k]+B[k, j]\nfor matrix multiplication, so we calculate a minimum instead of a sum, and a\nsum of elements instead of a product. After this modi\ufb01cation, matrix powers\ncorrespond to shortest paths in the graph.\nFor example, as\nV4 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221e \u221e 10 11 9 \u221e\n9 \u221e \u221e \u221e 8 9\n\u221e 11 \u221e \u221e \u221e \u221e\n\u221e 8 \u221e \u221e \u221e \u221e\n\u221e \u221e \u221e \u221e \u221e \u221e\n\u221e \u221e 12 13 11 \u221e\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\nwe can conclude that the minimum length of a path of 4 edges from node 2 to\nnode 5 is 8. Such a path is 2 \u2192 1 \u2192 4 \u2192 2 \u2192 5.\nKirchho\ufb00\u2019s theorem\nKirchhoff\u2019s theoremprovides a way to calculate the number of spanning trees\nof a graph as a determinant of a special matrix. For example, the graph\n1 2\n3 4\n223",
            "has three spanning trees:\n1 2\n3 4\n1 2\n3 4\n1 2\n3 4\nTo calculate the number of spanning trees, we construct a Laplacean matrix L,\nwhere L[i, i] is the degree of node i and L[i, j] = \u22121 if there is an edge between\nnodes i and j, and otherwise L[i, j] = 0. The Laplacean matrix for the above\ngraph is as follows:\nL =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n3 \u22121 \u22121 \u22121\n\u22121 1 0 0\n\u22121 0 2 \u22121\n\u22121 0 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nIt can be shown that the number of spanning trees equals the determinant of\na matrix that is obtained when we remove any row and any column from L. For\nexample, if we remove the \ufb01rst row and column, the result is\ndet(\n\uf8ee\n\uf8f0\n1 0 0\n0 2 \u22121\n0 \u22121 2\n\uf8f9\n\uf8fb) = 3.\nThe determinant is always the same, regardless of which row and column we\nremove from L.\nNote that Cayley\u2019s formula in Chapter 22.5 is a special case of Kirchhoff\u2019s\ntheorem, because in a complete graph of n nodes\ndet(\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nn \u22121 \u22121 \u00b7\u00b7\u00b7 \u2212 1\n\u22121 n \u22121 \u00b7\u00b7\u00b7 \u2212 1\n... ... ... ...\n\u22121 \u22121 \u00b7\u00b7\u00b7 n \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb) = nn\u22122.\n224",
            "Chapter 24\nProbability\nA probability is a real number between 0 and 1 that indicates how probable\nan event is. If an event is certain to happen, its probability is 1, and if an event\nis impossible, its probability is 0. The probability of an event is denoted P(\u00b7\u00b7\u00b7 )\nwhere the three dots describe the event.\nFor example, when throwing a dice, the outcome is an integer between 1 and\n6, and the probability of each outcome is 1/6. For example, we can calculate the\nfollowing probabilities:\n\u2022 P(\u201dthe outcome is 4\u201d)= 1/6\n\u2022 P(\u201dthe outcome is not 6\u201d)= 5/6\n\u2022 P(\u201dthe outcome is even\u201d)= 1/2\n24.1 Calculation\nTo calculate the probability of an event, we can either use combinatorics or\nsimulate the process that generates the event. As an example, let us calculate\nthe probability of drawing three cards with the same value from a shuf\ufb02ed deck\nof cards (for example, \u26608, \u26638 and \u26668).\nMethod 1\nWe can calculate the probability using the formula\nnumber of desired outcomes\ntotal number of outcomes .\nIn this problem, the desired outcomes are those in which the value of each\ncard is the same. There are 13\n(4\n3\n)\nsuch outcomes, because there are 13 possibilities\nfor the value of the cards and\n(4\n3\n)\nways to choose 3 suits from 4 possible suits.\nThere are a total of\n(52\n3\n)\noutcomes, because we choose 3 cards from 52 cards.\nThus, the probability of the event is\n13\n(4\n3\n)\n(52\n3\n) = 1\n425.\n225",
            "Method 2\nAnother way to calculate the probability is to simulate the process that generates\nthe event. In this example, we draw three cards, so the process consists of three\nsteps. We require that each step of the process is successful.\nDrawing the \ufb01rst card certainly succeeds, because there are no restrictions.\nThe second step succeeds with probability 3/51, because there are 51 cards left\nand 3 of them have the same value as the \ufb01rst card. In a similar way, the third\nstep succeeds with probability 2/50.\nThe probability that the entire process succeeds is\n1\u00b7 3\n51 \u00b7 2\n50 = 1\n425.\n24.2 Events\nAn event in probability theory can be represented as a set\nA \u2282 X,\nwhere X contains all possible outcomes and A is a subset of outcomes. For\nexample, when drawing a dice, the outcomes are\nX = {1,2,3,4,5,6}.\nNow, for example, the event \u201dthe outcome is even\u201d corresponds to the set\nA = {2,4,6}.\nEach outcome x is assigned a probability p(x). Then, the probability P(A)\nof an event A can be calculated as a sum of probabilities of outcomes using the\nformula\nP(A) =\n\u2211\nx\u2208A\np(x).\nFor example, when throwing a dice, p(x) = 1/6 for each outcome x, so the proba-\nbility of the event \u201dthe outcome is even\u201d is\np(2)+ p(4)+ p(6) = 1/2.\nThe total probability of the outcomes in X must be 1, i.e., P(X) = 1.\nSince the events in probability theory are sets, we can manipulate them using\nstandard set operations:\n\u2022 The complement \u00afA means \u201dA does not happen\u201d. For example, when\nthrowing a dice, the complement of A = {2,4,6} is \u00afA = {1,3,5}.\n\u2022 The union A \u222a B means \u201dA or B happen\u201d. For example, the union of\nA = {2,5} and B = {4,5,6} is A \u222aB = {2,4,5,6}.\n\u2022 The intersection A \u2229B means \u201dA and B happen\u201d. For example, the inter-\nsection of A = {2,5} and B = {4,5,6} is A \u2229B = {5}.\n226",
            "Complement\nThe probability of the complement \u00afA is calculated using the formula\nP( \u00afA) = 1\u2212P(A).\nSometimes, we can solve a problem easily using complements by solving the\nopposite problem. For example, the probability of getting at least one six when\nthrowing a dice ten times is\n1\u2212(5/6)10.\nHere 5/6 is the probability that the outcome of a single throw is not six, and\n(5/6)10 is the probability that none of the ten throws is a six. The complement of\nthis is the answer to the problem.\nUnion\nThe probability of the union A \u222aB is calculated using the formula\nP(A \u222aB) = P(A)+P(B)\u2212P(A \u2229B).\nFor example, when throwing a dice, the union of the events\nA = \u201dthe outcome is even\u201d\nand\nB = \u201dthe outcome is less than 4\u201d\nis\nA \u222aB = \u201dthe outcome is even or less than 4\u201d,\nand its probability is\nP(A \u222aB) = P(A)+P(B)\u2212P(A \u2229B) = 1/2+1/2\u22121/6 = 5/6.\nIf the events A and B are disjoint, i.e., A \u2229B is empty, the probability of the\nevent A \u222aB is simply\nP(A \u222aB) = P(A)+P(B).\nConditional probability\nThe conditional probability\nP(A|B) = P(A \u2229B)\nP(B)\nis the probability of A assuming that B happens. Hence, when calculating the\nprobability of A, we only consider the outcomes that also belong to B.\nUsing the previous sets,\nP(A|B) = 1/3,\nbecause the outcomes of B are {1,2,3}, and one of them is even. This is the\nprobability of an even outcome if we know that the outcome is between 1... 3.\n227",
            "Intersection\nUsing conditional probability, the probability of the intersection A \u2229B can be\ncalculated using the formula\nP(A \u2229B) = P(A)P(B|A).\nEvents A and B are independent if\nP(A|B) = P(A) and P(B|A) = P(B),\nwhich means that the fact that B happens does not change the probability of A,\nand vice versa. In this case, the probability of the intersection is\nP(A \u2229B) = P(A)P(B).\nFor example, when drawing a card from a deck, the events\nA = \u201dthe suit is clubs\u201d\nand\nB = \u201dthe value is four\u201d\nare independent. Hence the event\nA \u2229B = \u201dthe card is the four of clubs\u201d\nhappens with probability\nP(A \u2229B) = P(A)P(B) = 1/4\u00b71/13 = 1/52.\n24.3 Random variables\nA random variable is a value that is generated by a random process. For\nexample, when throwing two dice, a possible random variable is\nX = \u201dthe sum of the outcomes\u201d.\nFor example, if the outcomes are [4,6] (meaning that we \ufb01rst throw a four and\nthen a six), then the value of X is 10.\nWe denote P(X = x) the probability that the value of a random variable X\nis x. For example, when throwing two dice, P(X = 10) = 3/36, because the total\nnumber of outcomes is 36 and there are three possible ways to obtain the sum 10:\n[4,6], [5,5] and [6,4].\n228",
            "Expected value\nThe expected value E[X] indicates the average value of a random variable X.\nThe expected value can be calculated as the sum\n\u2211\nx\nP(X = x)x,\nwhere x goes through all possible values of X.\nFor example, when throwing a dice, the expected outcome is\n1/6\u00b71+1/6\u00b72+1/6\u00b73+1/6\u00b74+1/6\u00b75+1/6\u00b76 = 7/2.\nA useful property of expected values is linearity. It means that the sum\nE[X1 + X2 +\u00b7\u00b7\u00b7+ Xn] always equals the sum E[X1] + E[X2] +\u00b7\u00b7\u00b7+ E[Xn]. This\nformula holds even if random variables depend on each other.\nFor example, when throwing two dice, the expected sum is\nE[X1 + X2] = E[X1]+E[X2] = 7/2+7/2 = 7.\nLet us now consider a problem where n balls are randomly placed in n boxes,\nand our task is to calculate the expected number of empty boxes. Each ball has\nan equal probability to be placed in any of the boxes. For example, if n = 2, the\npossibilities are as follows:\nIn this case, the expected number of empty boxes is\n0+0+1+1\n4 = 1\n2.\nIn the general case, the probability that a single box is empty is\n(n \u22121\nn\n)n\n,\nbecause no ball should be placed in it. Hence, using linearity, the expected\nnumber of empty boxes is\nn \u00b7\n(n \u22121\nn\n)n\n.\nDistributions\nThe distribution of a random variable X shows the probability of each value\nthat X may have. The distribution consists of values P(X = x). For example,\nwhen throwing two dice, the distribution for their sum is:\nx 2 3 4 5 6 7 8 9 10 11 12\nP(X = x) 1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36\n229",
            "In a uniform distribution, the random variable X has n possible values\na,a+1,..., b and the probability of each value is 1/n. For example, when throwing\na dice, a = 1, b = 6 and P(X = x) = 1/6 for each value x.\nThe expected value of X in a uniform distribution is\nE[X] = a +b\n2 .\nIn a binomial distribution, n attempts are made and the probability that\na single attempt succeeds is p. The random variable X counts the number of\nsuccessful attempts, and the probability of a value x is\nP(X = x) = px(1\u2212 p)n\u2212x\n(\nn\nx\n)\n,\nwhere px and (1\u2212 p)n\u2212x correspond to successful and unsuccessful attemps, and(n\nx\n)\nis the number of ways we can choose the order of the attempts.\nFor example, when throwing a dice ten times, the probability of throwing a\nsix exactly three times is (1/6)3(5/6)7(10\n3\n)\n.\nThe expected value of X in a binomial distribution is\nE[X] = pn.\nIn a geometric distribution, the probability that an attempt succeeds is p,\nand we continue until the \ufb01rst success happens. The random variable X counts\nthe number of attempts needed, and the probability of a value x is\nP(X = x) = (1\u2212 p)x\u22121 p,\nwhere (1\u2212 p)x\u22121 corresponds to the unsuccessful attemps and p corresponds to\nthe \ufb01rst successful attempt.\nFor example, if we throw a dice until we throw a six, the probability that the\nnumber of throws is exactly 4 is (5/6)31/6.\nThe expected value of X in a geometric distribution is\nE[X] = 1\np.\n24.4 Markov chains\nA Markov chain is a random process that consists of states and transitions\nbetween them. For each state, we know the probabilities for moving to other\nstates. A Markov chain can be represented as a graph whose nodes are states\nand edges are transitions.\nAs an example, consider a problem where we are in \ufb02oor 1 in an n \ufb02oor\nbuilding. At each step, we randomly walk either one \ufb02oor up or one \ufb02oor down,\nexcept that we always walk one \ufb02oor up from \ufb02oor 1 and one \ufb02oor down from\n\ufb02oor n. What is the probability of being in \ufb02oor m after k steps?\nIn this problem, each \ufb02oor of the building corresponds to a state in a Markov\nchain. For example, if n = 5, the graph is as follows:\n230",
            "1 2 3 4 5\n1 1/2 1/2 1/2\n11/21/21/2\nThe probability distribution of a Markov chain is a vector [ p1, p2,..., pn],\nwhere pk is the probability that the current state is k. The formula p1 + p2 +\u00b7\u00b7\u00b7+\npn = 1 always holds.\nIn the above scenario, the initial distribution is [1,0,0,0,0], because we always\nbegin in \ufb02oor 1. The next distribution is [0,1,0,0,0], because we can only move\nfrom \ufb02oor 1 to \ufb02oor 2. After this, we can either move one \ufb02oor up or one \ufb02oor\ndown, so the next distribution is [1/2,0,1/2,0,0], and so on.\nAn ef\ufb01cient way to simulate the walk in a Markov chain is to use dynamic\nprogramming. The idea is to maintain the probability distribution, and at each\nstep go through all possibilities how we can move. Using this method, we can\nsimulate a walk of m steps in O(n2m) time.\nThe transitions of a Markov chain can also be represented as a matrix that\nupdates the probability distribution. In the above scenario, the matrix is\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 1/2 0 0 0\n1 0 1/2 0 0\n0 1/2 0 1/2 0\n0 0 1/2 0 1\n0 0 0 1/2 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nWhen we multiply a probability distribution by this matrix, we get the new\ndistribution after moving one step. For example, we can move from the distribu-\ntion [1,0,0,0,0] to the distribution [0,1,0,0,0] as follows:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 1/2 0 0 0\n1 0 1/2 0 0\n0 1/2 0 1/2 0\n0 0 1/2 0 1\n0 0 0 1/2 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n1\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nBy calculating matrix powers ef\ufb01ciently, we can calculate the distribution\nafter m steps in O(n3 logm) time.\n24.5 Randomized algorithms\nSometimes we can use randomness for solving a problem, even if the problem is\nnot related to probabilities. A randomized algorithm is an algorithm that is\nbased on randomness.\nA Monte Carlo algorithm is a randomized algorithm that may sometimes\ngive a wrong answer. For such an algorithm to be useful, the probability of a\nwrong answer should be small.\n231",
            "A Las Vegas algorithm is a randomized algorithm that always gives the\ncorrect answer, but its running time varies randomly. The goal is to design an\nalgorithm that is ef\ufb01cient with high probability.\nNext we will go through three example problems that can be solved using\nrandomness.\nOrder statistics\nThe kth order statistic of an array is the element at position k after sorting the\narray in increasing order. It is easy to calculate any order statistic in O(nlogn)\ntime by \ufb01rst sorting the array, but is it really needed to sort the entire array just\nto \ufb01nd one element?\nIt turns out that we can \ufb01nd order statistics using a randomized algorithm\nwithout sorting the array. The algorithm, called quickselect1, is a Las Vegas\nalgorithm: its running time is usually O(n) but O(n2) in the worst case.\nThe algorithm chooses a random element x of the array, and moves elements\nsmaller than x to the left part of the array, and all other elements to the right\npart of the array. This takes O(n) time when there are n elements. Assume that\nthe left part contains a elements and the right part contains b elements. If a = k,\nelement x is the kth order statistic. Otherwise, if a > k, we recursively \ufb01nd the\nkth order statistic for the left part, and if a < k, we recursively \ufb01nd the rth order\nstatistic for the right part where r = k \u2212a. The search continues in a similar way,\nuntil the element has been found.\nWhen each element x is randomly chosen, the size of the array about halves\nat each step, so the time complexity for \ufb01nding the kth order statistic is about\nn +n/2+n/4+n/8+\u00b7\u00b7\u00b7 <2n = O(n).\nThe worst case of the algorithm requires stillO(n2) time, because it is possible\nthat x is always chosen in such a way that it is one of the smallest or largest\nelements in the array and O(n) steps are needed. However, the probability for\nthis is so small that this never happens in practice.\nVerifying matrix multiplication\nOur next problem is to verify if AB = C holds when A, B and C are matrices of\nsize n \u00d7 n. Of course, we can solve the problem by calculating the product AB\nagain (in O(n3) time using the basic algorithm), but one could hope that verifying\nthe answer would by easier than to calculate it from scratch.\nIt turns out that we can solve the problem using a Monte Carlo algorithm2\nwhose time complexity is only O(n2). The idea is simple: we choose a random\nvector X of n elements, and calculate the matrices ABX and CX . If ABX = CX ,\nwe report that AB = C, and otherwise we report that AB \u0338= C.\n1In 1961, C. A. R. Hoare published two algorithms that are ef\ufb01cient on average: quicksort\n[36] for sorting arrays and quickselect [37] for \ufb01nding order statistics.\n2R. M. Freivalds published this algorithm in 1977 [26], and it is sometimes called Freivalds\u2019\nalgorithm.\n232",
            "The time complexity of the algorithm is O(n2), because we can calculate\nthe matrices ABX and CX in O(n2) time. We can calculate the matrix ABX\nef\ufb01ciently by using the representation A(BX ), so only two multiplications of n\u00d7n\nand n \u00d71 size matrices are needed.\nThe drawback of the algorithm is that there is a small chance that the\nalgorithm makes a mistake when it reports that AB = C. For example,\n[6 8\n1 3\n]\n\u0338=\n[8 7\n3 2\n]\n,\nbut [6 8\n1 3\n][3\n6\n]\n=\n[8 7\n3 2\n][3\n6\n]\n.\nHowever, in practice, the probability that the algorithm makes a mistake is\nsmall, and we can decrease the probability by verifying the result using multiple\nrandom vectors X before reporting that AB = C.\nGraph coloring\nGiven a graph that contains n nodes and m edges, our task is to \ufb01nd a way to\ncolor the nodes of the graph using two colors so that for at least m/2 edges, the\nendpoints have different colors. For example, in the graph\n1 2\n3 4\n5\na valid coloring is as follows:\n1 2\n3 4\n5\nThe above graph contains 7 edges, and for 5 of them, the endpoints have different\ncolors, so the coloring is valid.\nThe problem can be solved using a Las Vegas algorithm that generates random\ncolorings until a valid coloring has been found. In a random coloring, the color of\neach node is independently chosen so that the probability of both colors is 1/2.\nIn a random coloring, the probability that the endpoints of a single edge have\ndifferent colors is 1/2. Hence, the expected number of edges whose endpoints\nhave different colors is m/2. Since it is expected that a random coloring is valid,\nwe will quickly \ufb01nd a valid coloring in practice.\n233",
            "234",
            "Chapter 25\nGame theory\nIn this chapter, we will focus on two-player games that do not contain random\nelements. Our goal is to \ufb01nd a strategy that we can follow to win the game no\nmatter what the opponent does, if such a strategy exists.\nIt turns out that there is a general strategy for such games, and we can\nanalyze the games using the nim theory. First, we will analyze simple games\nwhere players remove sticks from heaps, and after this, we will generalize the\nstrategy used in those games to other games.\n25.1 Game states\nLet us consider a game where there is initially a heap of n sticks. Players A and\nB move alternately, and player A begins. On each move, the player has to remove\n1, 2 or 3 sticks from the heap, and the player who removes the last stick wins the\ngame.\nFor example, if n = 10, the game may proceed as follows:\n\u2022 Player A removes 2 sticks (8 sticks left).\n\u2022 Player B removes 3 sticks (5 sticks left).\n\u2022 Player A removes 1 stick (4 sticks left).\n\u2022 Player B removes 2 sticks (2 sticks left).\n\u2022 Player A removes 2 sticks and wins.\nThis game consists of states 0,1,2,..., n, where the number of the state corre-\nsponds to the number of sticks left.\nWinning and losing states\nA winning state is a state where the player will win the game if they play\noptimally, and alosing state is a state where the player will lose the game if the\nopponent plays optimally. It turns out that we can classify all states of a game so\nthat each state is either a winning state or a losing state.\nIn the above game, state 0 is clearly a losing state, because the player cannot\nmake any moves. States 1, 2 and 3 are winning states, because we can remove 1,\n235",
            "2 or 3 sticks and win the game. State 4, in turn, is a losing state, because any\nmove leads to a state that is a winning state for the opponent.\nMore generally, if there is a move that leads from the current state to a losing\nstate, the current state is a winning state, and otherwise the current state is a\nlosing state. Using this observation, we can classify all states of a game starting\nwith losing states where there are no possible moves.\nThe states 0... 15 of the above game can be classi\ufb01ed as follows (W denotes a\nwinning state and L denotes a losing state):\nL W W W L W W W L W W W L W W W\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nIt is easy to analyze this game: a state k is a losing state if k is divisible by\n4, and otherwise it is a winning state. An optimal way to play the game is to\nalways choose a move after which the number of sticks in the heap is divisible by\n4. Finally, there are no sticks left and the opponent has lost.\nOf course, this strategy requires that the number of sticks is not divisible by\n4 when it is our move. If it is, there is nothing we can do, and the opponent will\nwin the game if they play optimally.\nState graph\nLet us now consider another stick game, where in each state k, it is allowed to\nremove any number x of sticks such that x is smaller than k and divides k. For\nexample, in state 8 we may remove 1, 2 or 4 sticks, but in state 7 the only allowed\nmove is to remove 1 stick.\nThe following picture shows the states 1 ... 9 of the game as a state graph,\nwhose nodes are the states and edges are the moves between them:\n1 2\n3\n4\n5\n6\n7\n8\n9\nThe \ufb01nal state in this game is always state 1, which is a losing state, because\nthere are no valid moves. The classi\ufb01cation of states 1 ... 9 is as follows:\nL W L W L W L W L\n1 2 3 4 5 6 7 8 9\nSurprisingly, in this game, all even-numbered states are winning states, and\nall odd-numbered states are losing states.\n236",
            "25.2 Nim game\nThe nim game is a simple game that has an important role in game theory,\nbecause many other games can be played using the same strategy. First, we focus\non nim, and then we generalize the strategy to other games.\nThere are n heaps in nim, and each heap contains some number of sticks.\nThe players move alternately, and on each turn, the player chooses a heap that\nstill contains sticks and removes any number of sticks from it. The winner is the\nplayer who removes the last stick.\nThe states in nim are of the form [x1, x2,..., xn], where xk denotes the number\nof sticks in heap k. For example, [10,12,5] is a game where there are three heaps\nwith 10, 12 and 5 sticks. The state [0,0,..., 0] is a losing state, because it is not\npossible to remove any sticks, and this is always the \ufb01nal state.\nAnalysis\nIt turns out that we can easily classify any nim state by calculating thenim sum\ns = x1 \u2295x2 \u2295\u00b7\u00b7\u00b7\u2295 xn, where \u2295 is the xor operation1. The states whose nim sum is\n0 are losing states, and all other states are winning states. For example, the nim\nsum of [10,12,5] is 10\u229512\u22955 = 3, so the state is a winning state.\nBut how is the nim sum related to the nim game? We can explain this by\nlooking at how the nim sum changes when the nim state changes.\nLosing states: The \ufb01nal state [0,0,..., 0] is a losing state, and its nim sum is 0,\nas expected. In other losing states, any move leads to a winning state, because\nwhen a single value xk changes, the nim sum also changes, so the nim sum is\ndifferent from 0 after the move.\nWinning states: We can move to a losing state if there is any heap k for which\nxk \u2295s < xk. In this case, we can remove sticks from heap k so that it will contain\nxk \u2295s sticks, which will lead to a losing state. There is always such a heap, where\nxk has a one bit at the position of the leftmost one bit of s.\nAs an example, consider the state [10 ,12,5]. This state is a winning state,\nbecause its nim sum is 3. Thus, there has to be a move which leads to a losing\nstate. Next we will \ufb01nd out such a move.\nThe nim sum of the state is as follows:\n10 1010\n12 1100\n5 0101\n3 0011\nIn this case, the heap with 10 sticks is the only heap that has a one bit at the\nposition of the leftmost one bit of the nim sum:\n10 1010\n12 1100\n5 0101\n3 0011\n1The optimal strategy for nim was published in 1901 by C. L. Bouton [10].\n237",
            "The new size of the heap has to be 10 \u22953 = 9, so we will remove just one stick.\nAfter this, the state will be [9,12,5], which is a losing state:\n9 1001\n12 1100\n5 0101\n0 0000\nMis\u00e8re game\nIn a mis\u00e8re game, the goal of the game is opposite, so the player who removes\nthe last stick loses the game. It turns out that the mis\u00e8re nim game can be\noptimally played almost like the standard nim game.\nThe idea is to \ufb01rst play the mis\u00e8re game like the standard game, but change\nthe strategy at the end of the game. The new strategy will be introduced in a\nsituation where each heap would contain at most one stick after the next move.\nIn the standard game, we should choose a move after which there is an even\nnumber of heaps with one stick. However, in the mis\u00e8re game, we choose a move\nso that there is an odd number of heaps with one stick.\nThis strategy works because a state where the strategy changes always\nappears in the game, and this state is a winning state, because it contains exactly\none heap that has more than one stick so the nim sum is not 0.\n25.3 Sprague\u2013Grundy theorem\nThe Sprague\u2013Grundy theorem2 generalizes the strategy used in nim to all\ngames that ful\ufb01l the following requirements:\n\u2022 There are two players who move alternately.\n\u2022 The game consists of states, and the possible moves in a state do not depend\non whose turn it is.\n\u2022 The game ends when a player cannot make a move.\n\u2022 The game surely ends sooner or later.\n\u2022 The players have complete information about the states and allowed moves,\nand there is no randomness in the game.\nThe idea is to calculate for each game state a Grundy number that corresponds\nto the number of sticks in a nim heap. When we know the Grundy numbers of all\nstates, we can play the game like the nim game.\nGrundy numbers\nThe Grundy number of a game state is\nmex({g1, g2,..., gn}),\n2The theorem was independently discovered by R. Sprague [61] and P. M. Grundy [31].\n238",
            "where g1, g2,..., gn are the Grundy numbers of the states to which we can move,\nand the mex function gives the smallest nonnegative number that is not in the\nset. For example, mex({0,1,3}) = 2. If there are no possible moves in a state, its\nGrundy number is 0, because mex(\u2208) = 0.\nFor example, in the state graph\nthe Grundy numbers are as follows:\n0 1 0\n2 0 2\nThe Grundy number of a losing state is 0, and the Grundy number of a winning\nstate is a positive number.\nThe Grundy number of a state corresponds to the number of sticks in a nim\nheap. If the Grundy number is 0, we can only move to states whose Grundy\nnumbers are positive, and if the Grundy number is x > 0, we can move to states\nwhose Grundy numbers include all numbers 0,1,..., x \u22121.\nAs an example, consider a game where the players move a \ufb01gure in a maze.\nEach square in the maze is either \ufb02oor or wall. On each turn, the player has to\nmove the \ufb01gure some number of steps left or up. The winner of the game is the\nplayer who makes the last move.\nThe following picture shows a possible initial state of the game, where @\ndenotes the \ufb01gure and * denotes a square where it can move.\n@****\n*\n*\nThe states of the game are all \ufb02oor squares of the maze. In the above maze,\nthe Grundy numbers are as follows:\n0 1 0 1\n0 1 2\n0 2 1 0\n3 0 4 1\n0 4 1 3 2\n239",
            "Thus, each state of the maze game corresponds to a heap in the nim game. For\nexample, the Grundy number for the lower-right square is 2, so it is a winning\nstate. We can reach a losing state and win the game by moving either four steps\nleft or two steps up.\nNote that unlike in the original nim game, it may be possible to move to a\nstate whose Grundy number is larger than the Grundy number of the current\nstate. However, the opponent can always choose a move that cancels such a move,\nso it is not possible to escape from a losing state.\nSubgames\nNext we will assume that our game consists of subgames, and on each turn, the\nplayer \ufb01rst chooses a subgame and then a move in the subgame. The game ends\nwhen it is not possible to make any move in any subgame.\nIn this case, the Grundy number of a game is the nim sum of the Grundy\nnumbers of the subgames. The game can be played like a nim game by calculating\nall Grundy numbers for subgames and then their nim sum.\nAs an example, consider a game that consists of three mazes. In this game,\non each turn, the player chooses one of the mazes and then moves the \ufb01gure in\nthe maze. Assume that the initial state of the game is as follows:\n@ @ @\nThe Grundy numbers for the mazes are as follows:\n0 1 0 1\n0 1 2\n0 2 1 0\n3 0 4 1\n0 4 1 3 2\n0 1 2 3\n1 0 0 1\n2 0 1 2\n3 1 2 0\n4 0 2 5 3\n0 1 2 3 4\n1 0\n2 1\n3 2\n4 0 1 2 3\nIn the initial state, the nim sum of the Grundy numbers is 2\u22953\u22953 = 2, so the\n\ufb01rst player can win the game. One optimal move is to move two steps up in the\n\ufb01rst maze, which produces the nim sum 0 \u22953\u22953 = 0.\nGrundy\u2019s game\nSometimes a move in a game divides the game into subgames that are indepen-\ndent of each other. In this case, the Grundy number of the game is\nmex({g1, g2,..., gn}),\n240",
            "where n is the number of possible moves and\ngk = ak,1 \u2295ak,2 \u2295... \u2295ak,m,\nwhere move k generates subgames with Grundy numbers ak,1,ak,2,..., ak,m.\nAn example of such a game is Grundy\u2019s game. Initially, there is a single\nheap that contains n sticks. On each turn, the player chooses a heap and divides\nit into two nonempty heaps such that the heaps are of different size. The player\nwho makes the last move wins the game.\nLet f (n) be the Grundy number of a heap that contains n sticks. The Grundy\nnumber can be calculated by going through all ways to divide the heap into two\nheaps. For example, when n = 8, the possibilities are 1+7, 2+6 and 3+5, so\nf (8) = mex({f (1)\u2295 f (7), f (2)\u2295 f (6), f (3)\u2295 f (5)}).\nIn this game, the value of f (n) is based on the values of f (1),..., f (n \u22121). The\nbase cases are f (1) = f (2) = 0, because it is not possible to divide the heaps of 1\nand 2 sticks. The \ufb01rst Grundy numbers are:\nf (1) = 0\nf (2) = 0\nf (3) = 1\nf (4) = 0\nf (5) = 2\nf (6) = 1\nf (7) = 0\nf (8) = 2\nThe Grundy number for n = 8 is 2, so it is possible to win the game. The winning\nmove is to create heaps 1+7, because f (1)\u2295 f (7) = 0.\n241",
            "242",
            "Chapter 26\nString algorithms\nThis chapter deals with ef\ufb01cient algorithms for string processing. Many string\nproblems can be easily solved inO(n2) time, but the challenge is to \ufb01nd algorithms\nthat work in O(n) or O(nlogn) time.\nFor example, a fundamental string processing problem is thepattern match-\ning problem: given a string of length n and a pattern of length m, our task is to\n\ufb01nd the occurrences of the pattern in the string. For example, the pattern ABC\noccurs two times in the string ABABCBABC.\nThe pattern matching problem can be easily solved in O(nm) time by a brute\nforce algorithm that tests all positions where the pattern may occur in the string.\nHowever, in this chapter, we will see that there are more ef\ufb01cient algorithms that\nrequire only O(n +m) time.\n26.1 String terminology\nThroughout the chapter, we assume that zero-based indexing is used in strings.\nThus, a string s of length n consists of characters s[0],s[1],..., s[n \u22121]. The set of\ncharacters that may appear in strings is called an alphabet. For example, the\nalphabet {A,B,..., Z} consists of the capital letters of English.\nA substring is a sequence of consecutive characters in a string. We use the\nnotation s[a... b] to refer to a substring of s that begins at position a and ends\nat position b. A string of length n has n(n +1)/2 substrings. For example, the\nsubstrings of ABCD are A, B, C, D, AB, BC, CD, ABC, BCD and ABCD.\nA subsequence is a sequence of (not necessarily consecutive) characters in a\nstring in their original order. A string of length n has 2n \u22121 subsequences. For\nexample, the subsequences of ABCD are A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD,\nACD, BCD and ABCD.\nA pre\ufb01x is a substring that starts at the beginning of a string, and a suf\ufb01x\nis a substring that ends at the end of a string. For example, the pre\ufb01xes of ABCD\nare A, AB, ABC and ABCD, and the suf\ufb01xes of ABCD are D, CD, BCD and ABCD.\nA rotation can be generated by moving the characters of a string one by one\nfrom the beginning to the end (or vice versa). For example, the rotations of ABCD\nare ABCD, BCDA, CDAB and DABC.\n243",
            "A period is a pre\ufb01x of a string such that the string can be constructed by\nrepeating the period. The last repetition may be partial and contain only a pre\ufb01x\nof the period. For example, the shortest period of ABCABCA is ABC.\nA border is a string that is both a pre\ufb01x and a suf\ufb01x of a string. For example,\nthe borders of ABACABA are A, ABA and ABACABA.\nStrings are compared using the lexicographical order (which corresponds\nto the alphabetical order). It means that x < y if either x \u0338= y and x is a pre\ufb01x of y,\nor there is a position k such that x[i] = y[i] when i < k and x[k] < y[k].\n26.2 Trie structure\nA trie is a rooted tree that maintains a set of strings. Each string in the set\nis stored as a chain of characters that starts at the root. If two strings have a\ncommon pre\ufb01x, they also have a common chain in the tree.\nFor example, consider the following trie:\n* *\n*\n*\nC T\nA\nN\nA D\nL Y\nH\nE\nR\nE\nThis trie corresponds to the set {CANAL,CANDY,THE,THERE}. The character * in a\nnode means that a string in the set ends at the node. Such a character is needed,\nbecause a string may be a pre\ufb01x of another string. For example, in the above trie,\nTHE is a pre\ufb01x of THERE.\nWe can check inO(n) time whether a trie contains a string of lengthn, because\nwe can follow the chain that starts at the root node. We can also add a string of\nlength n to the trie in O(n) time by \ufb01rst following the chain and then adding new\nnodes to the trie if necessary.\nUsing a trie, we can \ufb01nd the longest pre\ufb01x of a given string such that the\npre\ufb01x belongs to the set. Moreover, by storing additional information in each\nnode, we can calculate the number of strings that belong to the set and have a\ngiven string as a pre\ufb01x.\nA trie can be stored in an array\nint trie[N][A];\n244",
            "where N is the maximum number of nodes (the maximum total length of the\nstrings in the set) and A is the size of the alphabet. The nodes of a trie are\nnumbered 0,1,2,... so that the number of the root is 0, and trie[s][c] is the next\nnode in the chain when we move from node s using character c.\n26.3 String hashing\nString hashing is a technique that allows us to ef\ufb01ciently check whether two\nstrings are equal1. The idea in string hashing is to compare hash values of strings\ninstead of their individual characters.\nCalculating hash values\nA hash value of a string is a number that is calculated from the characters of\nthe string. If two strings are the same, their hash values are also the same, which\nmakes it possible to compare strings based on their hash values.\nA usual way to implement string hashing is polynomial hashing, which\nmeans that the hash value of a string s of length n is\n(s[0]An\u22121 +s[1]An\u22122 +\u00b7\u00b7\u00b7+ s[n \u22121]A0) mod B,\nwhere s[0],s[1],..., s[n\u22121] are interpreted as the codes of the characters of s, and\nA and B are pre-chosen constants.\nFor example, the codes of the characters of ALLEY are:\nA L L E Y\n65 76 76 69 89\nThus, if A = 3 and B = 97, the hash value of ALLEY is\n(65\u00b734 +76\u00b733 +76\u00b732 +69\u00b731 +89\u00b730) mod 97 = 52.\nPreprocessing\nUsing polynomial hashing, we can calculate the hash value of any substring of a\nstring s in O(1) time after an O(n) time preprocessing. The idea is to construct\nan array h such that h[k] contains the hash value of the pre\ufb01x s[0... k]. The array\nvalues can be recursively calculated as follows:\nh[0] = s[0]\nh[k] = (h[k \u22121]A +s[k]) mod B\nIn addition, we construct an array p where p[k] = Ak mod B:\np[0] = 1\np[k] = (p[k \u22121]A) mod B.\n1The technique was popularized by the Karp\u2013Rabin pattern matching algorithm [42].\n245",
            "Constructing these arrays takes O(n) time. After this, the hash value of any\nsubstring s[a... b] can be calculated in O(1) time using the formula\n(h[b]\u2212h[a \u22121]p[b \u2212a +1]) mod B\nassuming that a > 0. If a = 0, the hash value is simply h[b].\nUsing hash values\nWe can ef\ufb01ciently compare strings using hash values. Instead of comparing the\nindividual characters of the strings, the idea is to compare their hash values. If\nthe hash values are equal, the strings are probably equal, and if the hash values\nare different, the strings are certainly different.\nUsing hashing, we can often make a brute force algorithm ef\ufb01cient. As an\nexample, consider the pattern matching problem: given a string s and a pattern\np, \ufb01nd the positions where p occurs in s. A brute force algorithm goes through\nall positions where p may occur and compares the strings character by character.\nThe time complexity of such an algorithm is O(n2).\nWe can make the brute force algorithm more ef\ufb01cient by using hashing,\nbecause the algorithm compares substrings of strings. Using hashing, each\ncomparison only takes O(1) time, because only hash values of substrings are\ncompared. This results in an algorithm with time complexity O(n), which is the\nbest possible time complexity for this problem.\nBy combining hashing and binary search, it is also possible to \ufb01nd out the\nlexicographic order of two strings in logarithmic time. This can be done by\ncalculating the length of the common pre\ufb01x of the strings using binary search.\nOnce we know the length of the common pre\ufb01x, we can just check the next\ncharacter after the pre\ufb01x, because this determines the order of the strings.\nCollisions and parameters\nAn evident risk when comparing hash values is a collision, which means that\ntwo strings have different contents but equal hash values. In this case, an\nalgorithm that relies on the hash values concludes that the strings are equal, but\nin reality they are not, and the algorithm may give incorrect results.\nCollisions are always possible, because the number of different strings is\nlarger than the number of different hash values. However, the probability of a\ncollision is small if the constants A and B are carefully chosen. A usual way is to\nchoose random constants near 109, for example as follows:\nA = 911382323\nB = 972663749\nUsing such constants, the long long type can be used when calculating hash\nvalues, because the products AB and BB will \ufb01t in long long. But is it enough to\nhave about 109 different hash values?\nLet us consider three scenarios where hashing can be used:\n246",
            "Scenario 1: Strings x and y are compared with each other. The probability of\na collision is 1/B assuming that all hash values are equally probable.\nScenario 2: A string x is compared with strings y1, y2,..., yn. The probability\nof one or more collisions is\n1\u2212(1\u2212 1\nB)n.\nScenario 3: All pairs of strings x1, x2,..., xn are compared with each other.\nThe probability of one or more collisions is\n1\u2212 B \u00b7(B \u22121)\u00b7(B \u22122)\u00b7\u00b7\u00b7 (B \u2212n +1)\nBn .\nThe following table shows the collision probabilities when n = 106 and the\nvalue of B varies:\nconstant B scenario 1 scenario 2 scenario 3\n103 0.001000 1 .000000 1 .000000\n106 0.000001 0 .632121 1 .000000\n109 0.000000 0 .001000 1 .000000\n1012 0.000000 0 .000000 0 .393469\n1015 0.000000 0 .000000 0 .000500\n1018 0.000000 0 .000000 0 .000001\nThe table shows that in scenario 1, the probability of a collision is negligible\nwhen B \u2248 109. In scenario 2, a collision is possible but the probability is still\nquite small. However, in scenario 3 the situation is very different: a collision will\nalmost always happen when B \u2248 109.\nThe phenomenon in scenario 3 is known as the birthday paradox: if there\nare n people in a room, the probability that some two people have the same\nbirthday is large even if n is quite small. In hashing, correspondingly, when all\nhash values are compared with each other, the probability that some two hash\nvalues are equal is large.\nWe can make the probability of a collision smaller by calculating multiple\nhash values using different parameters. It is unlikely that a collision would\noccur in all hash values at the same time. For example, two hash values with\nparameter B \u2248 109 correspond to one hash value with parameter B \u2248 1018, which\nmakes the probability of a collision very small.\nSome people use constants B = 232 and B = 264, which is convenient, because\noperations with 32 and 64 bit integers are calculated modulo 232 and 264. How-\never, this is not a good choice, because it is possible to construct inputs that\nalways generate collisions when constants of the form 2x are used [51].\n26.4 Z-algorithm\nThe Z-array z of a string s of length n contains for each k = 0,1,..., n \u22121 the\nlength of the longest substring of s that begins at position k and is a pre\ufb01x of\n247",
            "s. Thus, z[k] = p tells us that s[0... p \u22121] equals s[k... k + p \u22121]. Many string\nprocessing problems can be ef\ufb01ciently solved using the Z-array.\nFor example, the Z-array of ACBACDACBACBACDA is as follows:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 7 0 0 2 0 0 1\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nIn this case, for example, z[6] = 5, because the substring ACBAC of length 5 is a\npre\ufb01x of s, but the substring ACBACB of length 6 is not a pre\ufb01x of s.\nAlgorithm description\nNext we describe an algorithm, called the Z-algorithm2, that ef\ufb01ciently con-\nstructs the Z-array in O(n) time. The algorithm calculates the Z-array values\nfrom left to right by both using information already stored in the Z-array and\ncomparing substrings character by character.\nTo ef\ufb01ciently calculate the Z-array values, the algorithm maintains a range\n[x, y] such that s[x... y] is a pre\ufb01x of s and y is as large as possible. Since we\nknow that s[0... y\u2212x] and s[x... y] are equal, we can use this information when\ncalculating Z-values for positions x +1, x +2,..., y.\nAt each position k, we \ufb01rst check the value of z[k \u2212 x]. If k +z[k \u2212 x] < y, we\nknow that z[k] = z[k \u2212 x]. However, if k +z[k \u2212 x] \u2265 y, s[0... y\u2212k] equals s[k... y],\nand to determine the value of z[k] we need to compare the substrings character\nby character. Still, the algorithm works in O(n) time, because we start comparing\nat positions y\u2212k +1 and y+1.\nFor example, let us construct the following Z-array:\nA C B A C D A C B A C B A C D A\n\u2013 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAfter calculating the value z[6] = 5, the current [x, y] range is [6,10]:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 ? ? ? ? ? ? ? ? ?\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nNow we can calculate subsequent Z-array values ef\ufb01ciently, because we know\nthat s[0... 4] and s[6... 10] are equal. First, since z[1] = z[2] = 0, we immediately\nknow that also z[7] = z[8] = 0:\n2The Z-algorithm was presented in [32] as the simplest known method for linear-time pattern\nmatching, and the original idea was attributed to [50].\n248",
            "A C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nThen, since z[3] = 2, we know that z[9] \u2265 2:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nHowever, we have no information about the string after position 10, so we\nneed to compare the substrings character by character:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nIt turns out that z[9] = 7, so the new [x, y] range is [9,15]:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 7 ? ? ? ? ? ?\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nAfter this, all the remaining Z-array values can be determined by using the\ninformation already stored in the Z-array:\nA C B A C D A C B A C B A C D A\n\u2013 0 0 2 0 0 5 0 0 7 0 0 2 0 0 1\nx y\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n249",
            "Using the Z-array\nIt is often a matter of taste whether to use string hashing or the Z-algorithm.\nUnlike hashing, the Z-algorithm always works and there is no risk for collisions.\nOn the other hand, the Z-algorithm is more dif\ufb01cult to implement and some\nproblems can only be solved using hashing.\nAs an example, consider again the pattern matching problem, where our task\nis to \ufb01nd the occurrences of a pattern p in a string s. We already solved this\nproblem ef\ufb01ciently using string hashing, but the Z-algorithm provides another\nway to solve the problem.\nA usual idea in string processing is to construct a string that consists of mul-\ntiple strings separated by special characters. In this problem, we can construct a\nstring p#s, where p and s are separated by a special character # that does not\noccur in the strings. The Z-array of p#s tells us the positions where p occurs in s,\nbecause such positions contain the length of p.\nFor example, if s =HATTIVATTI and p =ATT, the Z-array is as follows:\nA T T # H A T T I V A T T I\n\u2013 0 0 0 0 3 0 0 0 0 3 0 0 0\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\nThe positions 5 and 10 contain the value 3, which means that the pattern ATT\noccurs in the corresponding positions of HATTIVATTI.\nThe time complexity of the resulting algorithm is linear, because it suf\ufb01ces to\nconstruct the Z-array and go through its values.\nImplementation\nHere is a short implementation of the Z-algorithm that returns a vector that\ncorresponds to the Z-array.\nvector<int> z(string s) {\nint n = s.size();\nvector<int> z(n);\nint x = 0, y = 0;\nfor (int i = 1; i < n; i++) {\nz[i] = max(0,min(z[i-x],y-i+1));\nwhile (i+z[i] < n && s[z[i]] == s[i+z[i]]) {\nx = i; y = i+z[i]; z[i]++;\n}\n}\nreturn z;\n}\n250",
            "Chapter 27\nSquare root algorithms\nA square root algorithm is an algorithm that has a square root in its time\ncomplexity. A square root can be seen as a \u201dpoor man\u2019s logarithm\u201d: the complexity\nO(\u2282n) is better than O(n) but worse than O(logn). In any case, many square\nroot algorithms are fast and usable in practice.\nAs an example, consider the problem of creating a data structure that sup-\nports two operations on an array: modifying an element at a given position and\ncalculating the sum of elements in the given range. We have previously solved the\nproblem using binary indexed and segment trees, that support both operations in\nO(logn) time. However, now we will solve the problem in another way using a\nsquare root structure that allows us to modify elements inO(1) time and calculate\nsums in O(\u2282n) time.\nThe idea is to divide the array intoblocks of size \u2282n so that each block contains\nthe sum of elements inside the block. For example, an array of 16 elements will\nbe divided into blocks of 4 elements as follows:\n5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 2\n21 17 20 13\nIn this structure, it is easy to modify array elements, because it is only needed\nto update the sum of a single block after each modi\ufb01cation, which can be done in\nO(1) time. For example, the following picture shows how the value of an element\nand the sum of the corresponding block change:\n5 8 6 3 2 5 2 6 7 1 7 5 6 2 3 2\n21 15 20 13\nThen, to calculate the sum of elements in a range, we divide the range into\nthree parts such that the sum consists of values of single elements and sums of\nblocks between them:\n5 8 6 3 2 5 2 6 7 1 7 5 6 2 3 2\n21 15 20 13\n251",
            "Since the number of single elements is O(\u2282n) and the number of blocks is\nalso O(\u2282n), the sum query takes O(\u2282n) time. The purpose of the block size \u2282n is\nthat it balances two things: the array is divided into \u2282n blocks, each of which\ncontains \u2282n elements.\nIn practice, it is not necessary to use the exact value of \u2282n as a parameter,\nand instead we may use parameters k and n/k where k is different from \u2282n.\nThe optimal parameter depends on the problem and input. For example, if an\nalgorithm often goes through the blocks but rarely inspects single elements inside\nthe blocks, it may be a good idea to divide the array into k < \u2282n blocks, each of\nwhich contains n/k > \u2282n elements.\n27.1 Combining algorithms\nIn this section we discuss two square root algorithms that are based on combining\ntwo algorithms into one algorithm. In both cases, we could use either of the\nalgorithms without the other and solve the problem in O(n2) time. However, by\ncombining the algorithms, the running time is only O(n\u2282n).\nCase processing\nSuppose that we are given a two-dimensional grid that contains n cells. Each\ncell is assigned a letter, and our task is to \ufb01nd two cells with the same letter\nwhose distance is minimum, where the distance between cells (x1, y1) and (x2, y2)\nis |x1 \u2212 x2|+| y1 \u2212 y2|. For example, consider the following grid:\nA\nB\nC\nA\nC\nD\nE\nF\nB\nA\nG\nB\nD\nF\nE\nA\nIn this case, the minimum distance is 2 between the two \u2019E\u2019 letters.\nWe can solve the problem by considering each letter separately. Using this\napproach, the new problem is to calculate the minimum distance between two\ncells with a \ufb01xed letter c. We focus on two algorithms for this:\nAlgorithm 1: Go through all pairs of cells with letter c, and calculate the\nminimum distance between such cells. This will take O(k2) time where k is the\nnumber of cells with letter c.\nAlgorithm 2: Perform a breadth-\ufb01rst search that simultaneously starts at\neach cell with letter c. The minimum distance between two cells with letter c\nwill be calculated in O(n) time.\nOne way to solve the problem is to choose either of the algorithms and use\nit for all letters. If we use Algorithm 1, the running time is O(n2), because all\ncells may contain the same letter, and in this casek = n. Also if we use Algorithm\n2, the running time is O(n2), because all cells may have different letters, and in\nthis case n searches are needed.\n252",
            "However, we can combine the two algorithms and use different algorithms for\ndifferent letters depending on how many times each letter appears in the grid.\nAssume that a letter c appears k times. If k \u2264 \u2282n, we use Algorithm 1, and if\nk > \u2282n, we use Algorithm 2. It turns out that by doing this, the total running\ntime of the algorithm is only O(n\u2282n).\nFirst, suppose that we use Algorithm 1 for a letter c. Since c appears at most\u2282n times in the grid, we compare each cell with letter c O(\u2282n) times with other\ncells. Thus, the time used for processing all such cells is O(n\u2282n). Then, suppose\nthat we use Algorithm 2 for a letter c. There are at most \u2282n such letters, so\nprocessing those letters also takes O(n\u2282n) time.\nBatch processing\nOur next problem also deals with a two-dimensional grid that contains n cells.\nInitially, each cell except one is white. We performn\u22121 operations, each of which\n\ufb01rst calculates the minimum distance from a given white cell to a black cell, and\nthen paints the white cell black.\nFor example, consider the following operation:\n*\nFirst, we calculate the minimum distance from the white cell marked with *\nto a black cell. The minimum distance is 2, because we can move two steps left to\na black cell. Then, we paint the white cell black:\nConsider the following two algorithms:\nAlgorithm 1: Use breadth-\ufb01rst search to calculate for each white cell the\ndistance to the nearest black cell. This takes O(n) time, and after the search, we\ncan \ufb01nd the minimum distance from any white cell to a black cell in O(1) time.\nAlgorithm 2: Maintain a list of cells that have been painted black, go through\nthis list at each operation and then add a new cell to the list. An operation takes\nO(k) time where k is the length of the list.\nWe combine the above algorithms by dividing the operations into O(\u2282n)\nbatches, each of which consists of O(\u2282n) operations. At the beginning of each\nbatch, we perform Algorithm 1. Then, we use Algorithm 2 to process the opera-\ntions in the batch. We clear the list of Algorithm 2 between the batches. At each\n253",
            "operation, the minimum distance to a black cell is either the distance calculated\nby Algorithm 1 or the distance calculated by Algorithm 2.\nThe resulting algorithm works in O(n\u2282n) time. First, Algorithm 1 is per-\nformed O(\u2282n) times, and each search works in O(n) time. Second, when using\nAlgorithm 2 in a batch, the list contains O(\u2282n) cells (because we clear the list\nbetween the batches) and each operation takes O(\u2282n) time.\n27.2 Integer partitions\nSome square root algorithms are based on the following observation: if a positive\ninteger n is represented as a sum of positive integers, such a sum always contains\nat most O(\u2282n) distinct numbers. The reason for this is that to construct a sum\nthat contains a maximum number of distinct numbers, we should choose small\nnumbers. If we choose the numbers 1,2,..., k, the resulting sum is\nk(k +1)\n2 .\nThus, the maximum amount of distinct numbers is k = O(\u2282n). Next we will\ndiscuss two problems that can be solved ef\ufb01ciently using this observation.\nKnapsack\nSuppose that we are given a list of integer weights whose sum is n. Our task is to\n\ufb01nd out all sums that can be formed using a subset of the weights. For example,\nif the weights are {1,3,3}, the possible sums are as follows:\n\u2022 0 (empty set)\n\u2022 1\n\u2022 3\n\u2022 1 +3 = 4\n\u2022 3 +3 = 6\n\u2022 1 +3+3 = 7\nUsing the standard knapsack approach (see Chapter 7.4), the problem can be\nsolved as follows: we de\ufb01ne a function possible(x,k) whose value is 1 if the sum\nx can be formed using the \ufb01rst k weights, and 0 otherwise. Since the sum of the\nweights is n, there are at most n weights and all values of the function can be\ncalculated in O(n2) time using dynamic programming.\nHowever, we can make the algorithm more ef\ufb01cient by using the fact that\nthere are at most O(\u2282n) distinct weights. Thus, we can process the weights in\ngroups that consists of similar weights. We can process each group in O(n) time,\nwhich yields an O(n\u2282n) time algorithm.\nThe idea is to use an array that records the sums of weights that can be formed\nusing the groups processed so far. The array contains n elements: element k is 1\nif the sum k can be formed and 0 otherwise. To process a group of weights, we\nscan the array from left to right and record the new sums of weights that can be\nformed using this group and the previous groups.\n254",
            "String construction\nGiven a strings of length n and a set of stringsD whose total length ism, consider\nthe problem of counting the number of ways s can be formed as a concatenation\nof strings in D. For example, if s = ABAB and D = {A,B,AB}, there are 4 ways:\n\u2022 A +B +A +B\n\u2022 AB +A +B\n\u2022 A +B +AB\n\u2022 AB +AB\nWe can solve the problem using dynamic programming: Let count(k) denote\nthe number of ways to construct the pre\ufb01x s[0... k] using the strings in D. Now\ncount(n \u22121) gives the answer to the problem, and we can solve the problem in\nO(n2) time using a trie structure.\nHowever, we can solve the problem more ef\ufb01ciently by using string hashing\nand the fact that there are at most O(\u2282m) distinct string lengths in D. First, we\nconstruct a set H that contains all hash values of the strings in D. Then, when\ncalculating a value of count(k), we go through all values of p such that there is a\nstring of length p in D, calculate the hash value of s[k \u2212 p +1... k] and check if it\nbelongs to H. Since there are at most O(\u2282m) distinct string lengths, this results\nin an algorithm whose running time is O(n\u2282m).\n27.3 Mo\u2019s algorithm\nMo\u2019s algorithm1 can be used in many problems that require processing range\nqueries in a static array, i.e., the array values do not change between the queries.\nIn each query, we are given a range [a,b], and we should calculate a value based\non the array elements between positions a and b. Since the array is static, the\nqueries can be processed in any order, and Mo\u2019s algorithm processes the queries\nin a special order which guarantees that the algorithm works ef\ufb01ciently.\nMo\u2019s algorithm maintains an active range of the array, and the answer to\na query concerning the active range is known at each moment. The algorithm\nprocesses the queries one by one, and always moves the endpoints of the active\nrange by inserting and removing elements. The time complexity of the algorithm\nis O(n\u2282nf (n)) where the array contains n elements, there are n queries and each\ninsertion and removal of an element takes O(f (n)) time.\nThe trick in Mo\u2019s algorithm is the order in which the queries are processed:\nThe array is divided into blocks of k = O(\u2282n) elements, and a query [ a1,b1] is\nprocessed before a query [a2,b2] if either\n\u2022 \u230aa1/k\u230b < \u230aa2/k\u230b or\n\u2022 \u230aa1/k\u230b = \u230aa2/k\u230b and b1 < b2.\n1According to [12], this algorithm is named after Mo Tao, a Chinese competitive programmer,\nbut the technique has appeared earlier in the literature [44].\n255",
            "Thus, all queries whose left endpoints are in a certain block are processed\none after another sorted according to their right endpoints. Using this order, the\nalgorithm only performs O(n\u2282n) operations, because the left endpoint moves\nO(n) times O(\u2282n) steps, and the right endpoint moves O(\u2282n) times O(n) steps.\nThus, both endpoints move a total of O(n\u2282n) steps during the algorithm.\nExample\nAs an example, consider a problem where we are given a set of queries, each of\nthem corresponding to a range in an array, and our task is to calculate for each\nquery the number of distinct elements in the range.\nIn Mo\u2019s algorithm, the queries are always sorted in the same way, but it\ndepends on the problem how the answer to the query is maintained. In this\nproblem, we can maintain an array count where count[x] indicates the number\nof times an element x occurs in the active range.\nWhen we move from one query to another query, the active range changes.\nFor example, if the current range is\n4 2 5 4 2 4 3 3 4\nand the next range is\n4 2 5 4 2 4 3 3 4\nthere will be three steps: the left endpoint moves one step to the right, and the\nright endpoint moves two steps to the right.\nAfter each step, the array count needs to be updated. After adding an element\nx, we increase the value of count[x] by 1, and if count[x] = 1 after this, we also\nincrease the answer to the query by 1. Similarly, after removing an elementx, we\ndecrease the value of count[x] by 1, and if count[x] = 0 after this, we also decrease\nthe answer to the query by 1.\nIn this problem, the time needed to perform each step is O(1), so the total\ntime complexity of the algorithm is O(n\u2282n).\n256",
            "Chapter 28\nSegment trees revisited\nA segment tree is a versatile data structure that can be used to solve a large num-\nber of algorithm problems. However, there are many topics related to segment\ntrees that we have not touched yet. Now is time to discuss some more advanced\nvariants of segment trees.\nSo far, we have implemented the operations of a segment tree by walking\nfrom bottom to top in the tree. For example, we have calculated range sums as\nfollows (Chapter 9.3):\nint sum(int a, int b) {\na += n; b += n;\nint s = 0;\nwhile (a <= b) {\nif (a%2 == 1) s += tree[a++];\nif (b%2 == 0) s += tree[b--];\na /= 2; b /= 2;\n}\nreturn s;\n}\nHowever, in more advanced segment trees, it is often necessary to implement\nthe operations in another way, from top to bottom . Using this approach, the\nfunction becomes as follows:\nint sum(int a, int b, int k, int x, int y) {\nif (b < x || a > y) return 0;\nif (a <= x && y <= b) return tree[k];\nint d = (x+y)/2;\nreturn sum(a,b,2*k,x,d) + sum(a,b,2*k+1,d+1,y);\n}\nNow we can calculate any value ofsumq(a,b) (the sum of array values in range\n[a,b]) as follows:\nint s = sum(a, b, 1, 0, n-1);\n257",
            "The parameter k indicates the current position in tree. Initially k equals 1,\nbecause we begin at the root of the tree. The range [x, y] corresponds to k and is\ninitially [0,n \u22121]. When calculating the sum, if [x, y] is outside [a,b], the sum is\n0, and if [x, y] is completely inside [a,b], the sum can be found in tree. If [x, y] is\npartially inside [a,b], the search continues recursively to the left and right half\nof [x, y]. The left half is [x,d] and the right half is [d +1, y] where d = \u230ax+y\n2 \u230b.\nThe following picture shows how the search proceeds when calculating the\nvalue of sumq(a,b). The gray nodes indicate nodes where the recursion stops and\nthe sum can be found in tree.\n5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 2\n13 9 9 8 8 12 8 5\n22 17 20 13\n39 33\n72\na b\nAlso in this implementation, operations take O(logn) time, because the total\nnumber of visited nodes is O(logn).\n28.1 Lazy propagation\nUsing lazy propagation, we can build a segment tree that supports both range\nupdates and range queries in O(logn) time. The idea is to perform updates and\nqueries from top to bottom and perform updateslazily so that they are propagated\ndown the tree only when it is necessary.\nIn a lazy segment tree, nodes contain two types of information. Like in an\nordinary segment tree, each node contains the sum or some other value related\nto the corresponding subarray. In addition, the node may contain information\nrelated to lazy updates, which has not been propagated to its children.\nThere are two types of range updates: each array value in the range is\neither increased by some value or assigned some value. Both operations can be\nimplemented using similar ideas, and it is even possible to construct a tree that\nsupports both operations at the same time.\nLazy segment trees\nLet us consider an example where our goal is to construct a segment tree that sup-\nports two operations: increasing each value in [a,b] by a constant and calculating\n258",
            "the sum of values in [a,b].\nWe will construct a tree where each node has two values s/z: s denotes the\nsum of values in the range, andz denotes the value of a lazy update, which means\nthat all values in the range should be increased by z. In the following tree, z = 0\nin all nodes, so there are no ongoing lazy updates.\n5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 2\n13/0 9/0 9/0 8/0 8/0 12/0 8/0 5/0\n22/0 17/0 20/0 13/0\n39/0 33/0\n72/0\nWhen the elements in [a,b] are increased by u, we walk from the root towards\nthe leaves and modify the nodes of the tree as follows: If the range [x, y] of a node\nis completely inside [a,b], we increase the z value of the node by u and stop. If\n[x, y] only partially belongs to [a,b], we increase the s value of the node by hu,\nwhere h is the size of the intersection of [a,b] and [x, y], and continue our walk\nrecursively in the tree.\nFor example, the following picture shows the tree after increasing the ele-\nments in [a,b] by 2:\n5 8 6 3 2 9 2 6 7 1 7 5 6 2 3 2\n13/0 9/0 11/0 8/2 8/0 12/0 8/2 5/0\n22/0 23/0 20/2 17/0\n45/0 45/0\n90/0\na b\nWe also calculate the sum of elements in a range [ a,b] by walking in the\ntree from top to bottom. If the range [x, y] of a node completely belongs to [a,b],\nwe add the s value of the node to the sum. Otherwise, we continue the search\nrecursively downwards in the tree.\n259",
            "Both in updates and queries, the value of a lazy update is always propagated\nto the children of the node before processing the node. The idea is that updates\nwill be propagated downwards only when it is necessary, which guarantees that\nthe operations are always ef\ufb01cient.\nThe following picture shows how the tree changes when we calculate the\nvalue of suma(a,b). The rectangle shows the nodes whose values change, because\na lazy update is propagated downwards.\n5 8 6 3 2 9 2 6 7 1 7 5 6 2 3 2\n13/0 9/0 11/0 8/2 8/2 12/2 8/2 5/0\n22/0 23/0 28/0 17/0\n45/0 45/0\n90/0\na b\nNote that sometimes it is needed to combine lazy updates. This happens when\na node that already has a lazy update is assigned another lazy update. When\ncalculating sums, it is easy to combine lazy updates, because the combination of\nupdates z1 and z2 corresponds to an update z1 + z2.\nPolynomial updates\nLazy updates can be generalized so that it is possible to update ranges using\npolynomials of the form\np(u) = tkuk + tk\u22121uk\u22121 +\u00b7\u00b7\u00b7+ t0.\nIn this case, the update for a value at position i in [a,b] is p(i \u2212 a). For\nexample, adding the polynomial p(u) = u +1 to [a,b] means that the value at\nposition a increases by 1, the value at position a +1 increases by 2, and so on.\nTo support polynomial updates, each node is assigned k +2 values, where k\nequals the degree of the polynomial. The value s is the sum of the elements in\nthe range, and the values z0, z1,..., zk are the coef\ufb01cients of a polynomial that\ncorresponds to a lazy update.\nNow, the sum of values in a range [x, y] equals\ns +\ny\u2212x\u2211\nu=0\nzkuk + zk\u22121uk\u22121 +\u00b7\u00b7\u00b7+ z0.\n260",
            "The value of such a sum can be ef\ufb01ciently calculated using sum formulas.\nFor example, the term z0 corresponds to the sum (y\u2212x +1)z0, and the term z1u\ncorresponds to the sum\nz1(0+1+\u00b7\u00b7\u00b7+ y\u2212 x) = z1\n(y\u2212 x)(y\u2212 x +1)\n2 .\nWhen propagating an update in the tree, the indices of p(u) change, because\nin each range [x, y], the values are calculated for u = 0,1,..., y\u2212x. However, this\nis not a problem, because p\u2032(u) = p(u +h) is a polynomial of equal degree as p(u).\nFor example, if p(u) = t2u2 + t1u \u2212 t0, then\np\u2032(u) = t2(u +h)2 + t1(u +h)\u2212 t0 = t2u2 +(2ht2 + t1)u + t2h2 + t1h \u2212 t0.\n28.2 Dynamic trees\nAn ordinary segment tree is static, which means that each node has a \ufb01xed\nposition in the array and the tree requires a \ufb01xed amount of memory. In a\ndynamic segment tree, memory is allocated only for nodes that are actually\naccessed during the algorithm, which can save a large amount of memory.\nThe nodes of a dynamic tree can be represented as structs:\nstruct node {\nint value;\nint x, y;\nnode *left, *right;\nnode(int v, int x, int y) : value(v), x(x), y(y) {}\n};\nHere value is the value of the node, [x,y] is the corresponding range, and left\nand right point to the left and right subtree.\nAfter this, nodes can be created as follows:\n// create new node\nnode *x = new node(0, 0, 15);\n// change value\nx->value = 5;\nSparse segment trees\nA dynamic segment tree is useful when the underlying array is sparse, i.e., the\nrange [0,n \u22121] of allowed indices is large, but most array values are zeros. While\nan ordinary segment tree uses O(n) memory, a dynamic segment tree only uses\nO(klogn) memory, where k is the number of operations performed.\nA sparse segment tree initially has only one node [0 ,n \u22121] whose value\nis zero, which means that every array value is zero. After updates, new nodes\nare dynamically added to the tree. For example, if n = 16 and the elements in\npositions 3 and 10 have been modi\ufb01ed, the tree contains the following nodes:\n261",
            "[0,15]\n[0,7]\n[0,3]\n[2,3]\n[3]\n[8,15]\n[8,11]\n[10,11]\n[10]\nAny path from the root node to a leaf containsO(logn) nodes, so each operation\nadds at most O(logn) new nodes to the tree. Thus, after k operations, the tree\ncontains at most O(klogn) nodes.\nNote that if we know all elements to be updated at the beginning of the\nalgorithm, a dynamic segment tree is not necessary, because we can use an\nordinary segment tree with index compression (Chapter 9.4). However, this is\nnot possible when the indices are generated during the algorithm.\nPersistent segment trees\nUsing a dynamic implementation, it is also possible to create a persistent\nsegment tree that stores the modi\ufb01cation history of the tree. In such an im-\nplementation, we can ef\ufb01ciently access all versions of the tree that have existed\nduring the algorithm.\nWhen the modi\ufb01cation history is available, we can perform queries in any\nprevious tree like in an ordinary segment tree, because the full structure of each\ntree is stored. We can also create new trees based on previous trees and modify\nthem independently.\nConsider the following sequence of updates, where red nodes change and\nother nodes remain the same:\nstep 1 step 2 step 3\nAfter each update, most nodes of the tree remain the same, so an ef\ufb01cient way\nto store the modi\ufb01cation history is to represent each tree in the history as a\n262",
            "combination of new nodes and subtrees of previous trees. In this example, the\nmodi\ufb01cation history can be stored as follows:\nstep 1 step 2 step 3\nThe structure of each previous tree can be reconstructed by following the\npointers starting at the corresponding root node. Since each operation adds only\nO(logn) new nodes to the tree, it is possible to store the full modi\ufb01cation history\nof the tree.\n28.3 Data structures\nInstead of single values, nodes in a segment tree can also contain data structures\nthat maintain information about the corresponding ranges. In such a tree, the\noperations take O(f (n)logn) time, where f (n) is the time needed for processing a\nsingle node during an operation.\nAs an example, consider a segment tree that supports queries of the form\n\u201dhow many times does an element x appear in the range [a,b]?\u201d For example, the\nelement 1 appears three times in the following range:\n3 1 2 3 1 1 1 2\nTo support such queries, we build a segment tree where each node is assigned\na data structure that can be asked how many times any element x appears in the\ncorresponding range. Using this tree, the answer to a query can be calculated by\ncombining the results from the nodes that belong to the range.\nFor example, the following segment tree corresponds to the above array:\n3\n1\n1\n1\n2\n1\n3\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1 3\n1 1\n2 3\n1 1\n1\n2\n1 2\n1 1\n1 2 3\n1 1 2\n1 2\n3 1\n1 2 3\n4 2 2\n263",
            "We can build the tree so that each node contains a map structure. In this case,\nthe time needed for processing each node is O(logn), so the total time complexity\nof a query isO(log2 n). The tree uses O(nlogn) memory, because there areO(logn)\nlevels and each level contains O(n) elements.\n28.4 Two-dimensionality\nA two-dimensional segment tree supports queries related to rectangular sub-\narrays of a two-dimensional array. Such a tree can be implemented as nested\nsegment trees: a big tree corresponds to the rows of the array, and each node\ncontains a small tree that corresponds to a column.\nFor example, in the array\n8 5 3 8\n3 9 7 1\n8 7 5 2\n7 6 1 6\nthe sum of any subarray can be calculated from the following segment tree:\n7 6 1 6\n13 7\n20\n8 7 5 2\n15 7\n22\n3 9 7 1\n12 8\n20\n8 5 3 8\n13 11\n24\n1513 6 8\n28 14\n42\n111410 9\n25 19\n44\n26271617\n53 33\n86\nThe operations of a two-dimensional segment tree takeO(log2 n) time, because\nthe big tree and each small tree consist of O(logn) levels. The tree requires O(n2)\nmemory, because each small tree contains O(n) values.\n264",
            "Chapter 29\nGeometry\nIn geometric problems, it is often challenging to \ufb01nd a way to approach the\nproblem so that the solution to the problem can be conveniently implemented\nand the number of special cases is small.\nAs an example, consider a problem where we are given the vertices of a\nquadrilateral (a polygon that has four vertices), and our task is to calculate its\narea. For example, a possible input for the problem is as follows:\nOne way to approach the problem is to divide the quadrilateral into two triangles\nby a straight line between two opposite vertices:\nAfter this, it suf\ufb01ces to sum the areas of the triangles. The area of a triangle can\nbe calculated, for example, using Heron\u2019s formula\n\u221a\ns(s \u2212a)(s \u2212b)(s \u2212 c),\nwhere a, b and c are the lengths of the triangle\u2019s sides ands = (a +b + c)/2.\nThis is a possible way to solve the problem, but there is one pitfall: how to\ndivide the quadrilateral into triangles? It turns out that sometimes we cannot\njust pick two arbitrary opposite vertices. For example, in the following situation,\nthe division line is outside the quadrilateral:\n265",
            "However, another way to draw the line works:\nIt is clear for a human which of the lines is the correct choice, but the situation is\ndif\ufb01cult for a computer.\nHowever, it turns out that we can solve the problem using another method\nthat is more convenient to a programmer. Namely, there is a general formula\nx1 y2 \u2212 x2 y1 + x2 y3 \u2212 x3 y2 + x3 y4 \u2212 x4 y3 + x4 y1 \u2212 x1 y4,\nthat calculates the area of a quadrilateral whose vertices are ( x1, y1), ( x2, y2),\n(x3, y3) and (x4, y4). This formula is easy to implement, there are no special cases,\nand we can even generalize the formula to all polygons.\n29.1 Complex numbers\nA complex number is a number of the form x+ yi, where i =\n\u2282\n\u22121 is the imagi-\nnary unit. A geometric interpretation of a complex number is that it represents\na two-dimensional point (x, y) or a vector from the origin to a point (x, y).\nFor example, 4+2i corresponds to the following point and vector:\n(4,2)\nThe C++ complex number class complex is useful when solving geometric\nproblems. Using the class we can represent points and vectors as complex\nnumbers, and the class contains tools that are useful in geometry.\nIn the following code, C is the type of a coordinate and P is the type of a point\nor a vector. In addition, the code de\ufb01nes macros X and Y that can be used to refer\nto x and y coordinates.\ntypedef long long C;\ntypedef complex<C> P;\n#define X real()\n#define Y imag()\n266",
            "For example, the following code de\ufb01nes a point p = (4,2) and prints its x and\ny coordinates:\nP p = {4,2};\ncout << p.X << \" \" << p.Y << \"\\n\"; // 4 2\nThe following code de\ufb01nes vectors v = (3,1) and u = (2,2), and after that\ncalculates the sum s = v +u.\nP v = {3,1};\nP u = {2,2};\nP s = v+u;\ncout << s.X << \" \" << s.Y << \"\\n\"; // 5 3\nIn practice, an appropriate coordinate type is usually long long (integer) or\nlong double (real number). It is a good idea to use integer whenever possible,\nbecause calculations with integers are exact. If real numbers are needed, preci-\nsion errors should be taken into account when comparing numbers. A safe way\nto check if real numbers a and b are equal is to compare them using |a \u2212b| <\u03f5,\nwhere \u03f5 is a small number (for example, \u03f5 = 10\u22129).\nFunctions\nIn the following examples, the coordinate type is long double.\nThe function abs(v) calculates the length |v| of a vector v = (x, y) using the\nformula\n\u221a\nx2 + y2. The function can also be used for calculating the distance\nbetween points (x1, y1) and (x2, y2), because that distance equals the length of the\nvector (x2 \u2212 x1, y2 \u2212 y1).\nThe following code calculates the distance between points (4,2) and (3,\u22121):\nP a = {4,2};\nP b = {3,-1};\ncout << abs(b-a) << \"\\n\"; // 3.16228\nThe function arg(v) calculates the angle of a vector v = (x, y) with respect to\nthe x axis. The function gives the angle in radians, wherer radians equals 180r/\u03c0\ndegrees. The angle of a vector that points to the right is 0, and angles decrease\nclockwise and increase counterclockwise.\nThe function polar(s,a) constructs a vector whose length is s and that points\nto an angle a. A vector can be rotated by an angle a by multiplying it by a vector\nwith length 1 and angle a.\nThe following code calculates the angle of the vector (4 ,2), rotates it 1/2\nradians counterclockwise, and then calculates the angle again:\nP v = {4,2};\ncout << arg(v) << \"\\n\"; // 0.463648\nv *= polar(1.0,0.5);\ncout << arg(v) << \"\\n\"; // 0.963648\n267",
            "29.2 Points and lines\nThe cross product a\u00d7b of vectors a = (x1, y1) and b = (x2, y2) is calculated using\nthe formula x1 y2 \u2212 x2 y1. The cross product tells us whether b turns left (positive\nvalue), does not turn (zero) or turns right (negative value) when it is placed\ndirectly after a.\nThe following picture illustrates the above cases:\na\nb\na \u00d7b = 6\na\nb\na \u00d7b = 0\na\nb\na \u00d7b = \u22128\nFor example, in the \ufb01rst casea = (4,2) and b = (1,2). The following code calculates\nthe cross product using the class complex:\nP a = {4,2};\nP b = {1,2};\nC p = (conj(a)*b).Y; // 6\nThe above code works, because the function conj negates the y coordinate of\na vector, and when the vectors (x1,\u2212y1) and (x2, y2) are multiplied together, the y\ncoordinate of the result is x1 y2 \u2212 x2 y1.\nPoint location\nCross products can be used to test whether a point is located on the left or right\nside of a line. Assume that the line goes through points s1 and s2, we are looking\nfrom s1 to s2 and the point is p.\nFor example, in the following picture, p is on the left side of the line:\ns1\ns2\np\nThe cross product (p \u2212s1)\u00d7(p \u2212s2) tells us the location of the point p. If the\ncross product is positive, p is located on the left side, and if the cross product is\nnegative, p is located on the right side. Finally, if the cross product is zero, points\ns1, s2 and p are on the same line.\n268",
            "Line segment intersection\nNext we consider the problem of testing whether two line segments ab and cd\nintersect. The possible cases are:\nCase 1: The line segments are on the same line and they overlap each other.\nIn this case, there is an in\ufb01nite number of intersection points. For example, in\nthe following picture, all points between c and b are intersection points:\na\nd\nc\nb\nIn this case, we can use cross products to check if all points are on the same\nline. After this, we can sort the points and check whether the line segments\noverlap each other.\nCase 2: The line segments have a common vertex that is the only intersection\npoint. For example, in the following picture the intersection point is b = c:\na\nb = c\nd\nThis case is easy to check, because there are only four possibilities for the\nintersection point: a = c, a = d, b = c and b = d.\nCase 3: There is exactly one intersection point that is not a vertex of any line\nsegment. In the following picture, the point p is the intersection point:\nc\nd\na\nb\np\nIn this case, the line segments intersect exactly when both points c and d are\non different sides of a line through a and b, and points a and b are on different\nsides of a line through c and d. We can use cross products to check this.\nPoint distance from a line\nAnother feature of cross products is that the area of a triangle can be calculated\nusing the formula\n|(a \u2212 c)\u00d7(b \u2212 c)|\n2 ,\n269",
            "where a, b and c are the vertices of the triangle. Using this fact, we can derive\na formula for calculating the shortest distance between a point and a line. For\nexample, in the following picture d is the shortest distance between the point p\nand the line that is de\ufb01ned by the points s1 and s2:\ns1\ns2\np\nd\nThe area of the triangle whose vertices are s1, s2 and p can be calculated\nin two ways: it is both 1\n2 |s2 \u2212 s1|d and 1\n2 ((s1 \u2212 p) \u00d7(s2 \u2212 p)). Thus, the shortest\ndistance is\nd = (s1 \u2212 p)\u00d7(s2 \u2212 p)\n|s2 \u2212s1| .\nPoint inside a polygon\nLet us now consider the problem of testing whether a point is located inside or\noutside a polygon. For example, in the following picture point a is inside the\npolygon and point b is outside the polygon.\na\nb\nA convenient way to solve the problem is to send a ray from the point to an\narbitrary direction and calculate the number of times it touches the boundary\nof the polygon. If the number is odd, the point is inside the polygon, and if the\nnumber is even, the point is outside the polygon.\nFor example, we could send the following rays:\na\nb\nThe rays from a touch 1 and 3 times the boundary of the polygon, so a is\ninside the polygon. Correspondingly, the rays from b touch 0 and 2 times the\nboundary of the polygon, so b is outside the polygon.\n270",
            "29.3 Polygon area\nA general formula for calculating the area of a polygon, sometimes called the\nshoelace formula, is as follows:\n1\n2|\nn\u22121\u2211\ni=1\n(pi \u00d7 pi+1)| =1\n2|\nn\u22121\u2211\ni=1\n(xi yi+1 \u2212 xi+1 yi)|,\nHere the vertices are p1 = (x1, y1), p2 = (x2, y2), ... , pn = (xn, yn) in such an order\nthat pi and pi+1 are adjacent vertices on the boundary of the polygon, and the\n\ufb01rst and last vertex is the same, i.e., p1 = pn.\nFor example, the area of the polygon\n(4,1)\n(7,3)\n(5,5)\n(2,4)\n(4,3)\nis\n|(2\u00b75\u22125\u00b74)+(5\u00b73\u22127\u00b75)+(7\u00b71\u22124\u00b73)+(4\u00b73\u22124\u00b71)+(4\u00b74\u22122\u00b73)|\n2 = 17/2.\nThe idea of the formula is to go through trapezoids whose one side is a side of\nthe polygon, and another side lies on the horizontal line y = 0. For example:\n(4,1)\n(7,3)\n(5,5)\n(2,4)\n(4,3)\nThe area of such a trapezoid is\n(xi+1 \u2212 xi) yi + yi+1\n2 ,\nwhere the vertices of the polygon are pi and pi+1. If xi+1 > xi, the area is positive,\nand if xi+1 < xi, the area is negative.\nThe area of the polygon is the sum of areas of all such trapezoids, which yields\nthe formula\n|\nn\u22121\u2211\ni=1\n(xi+1 \u2212 xi) yi + yi+1\n2 | =1\n2|\nn\u22121\u2211\ni=1\n(xi yi+1 \u2212 xi+1 yi)|.\nNote that the absolute value of the sum is taken, because the value of the\nsum may be positive or negative, depending on whether we walk clockwise or\ncounterclockwise along the boundary of the polygon.\n271",
            "Pick\u2019s theorem\nPick\u2019s theoremprovides another way to calculate the area of a polygon provided\nthat all vertices of the polygon have integer coordinates. According to Pick\u2019s\ntheorem, the area of the polygon is\na +b/2\u22121,\nwhere a is the number of integer points inside the polygon and b is the number\nof integer points on the boundary of the polygon.\nFor example, the area of the polygon\n(4,1)\n(7,3)\n(5,5)\n(2,4)\n(4,3)\nis 6+7/2\u22121 = 17/2.\n29.4 Distance functions\nA distance function de\ufb01nes the distance between two points. The usual dis-\ntance function is the Euclidean distance where the distance between points\n(x1, y1) and (x2, y2) is \u221a\n(x2 \u2212 x1)2 +(y2 \u2212 y1)2.\nAn alternative distance function is the Manhattan distance where the distance\nbetween points (x1, y1) and (x2, y2) is\n|x1 \u2212 x2|+| y1 \u2212 y2|.\nFor example, consider the following picture:\n(2,1)\n(5,2)\n(2,1)\n(5,2)\nEuclidean distance Manhattan distance\nThe Euclidean distance between the points is\n\u221a\n(5\u22122)2 +(2\u22121)2 =\n\u2282\n10\nand the Manhattan distance is\n|5\u22122|+| 2\u22121| =4.\nThe following picture shows regions that are within a distance of 1 from the\ncenter point, using the Euclidean and Manhattan distances:\n272",
            "Euclidean distance Manhattan distance\nRotating coordinates\nSome problems are easier to solve if Manhattan distances are used instead of\nEuclidean distances. As an example, consider a problem where we are given n\npoints in the two-dimensional plane and our task is to calculate the maximum\nManhattan distance between any two points.\nFor example, consider the following set of points:\nA\nC\nB\nD\nThe maximum Manhattan distance is 5 between points B and C:\nA\nC\nB\nD\nA useful technique related to Manhattan distances is to rotate all coordinates\n45 degrees so that a point (x, y) becomes (x + y, y\u2212x). For example, after rotating\nthe above points, the result is:\nA\nC\nB\nD\nAnd the maximum distance is as follows:\n273",
            "A\nC\nB\nD\nConsider two points p1 = (x1, y1) and p2 = (x2, y2) whose rotated coordinates\nare p\u2032\n1 = (x\u2032\n1, y\u2032\n1) and p\u2032\n2 = (x\u2032\n2, y\u2032\n2). Now there are two ways to express the Manhat-\ntan distance between p1 and p2:\n|x1 \u2212 x2|+| y1 \u2212 y2| =max(|x\u2032\n1 \u2212 x\u2032\n2|,|y\u2032\n1 \u2212 y\u2032\n2|)\nFor example, if p1 = (1,0) and p2 = (3,3), the rotated coordinates are p\u2032\n1 =\n(1,\u22121) and p\u2032\n2 = (6,0) and the Manhattan distance is\n|1\u22123|+| 0\u22123| =max(|1\u22126|,|\u2212 1\u22120|) = 5.\nThe rotated coordinates provide a simple way to operate with Manhattan\ndistances, because we can consider x and y coordinates separately. To maximize\nthe Manhattan distance between two points, we should \ufb01nd two points whose\nrotated coordinates maximize the value of\nmax(|x\u2032\n1 \u2212 x\u2032\n2|,|y\u2032\n1 \u2212 y\u2032\n2|).\nThis is easy, because either the horizontal or vertical difference of the rotated\ncoordinates has to be maximum.\n274",
            "Chapter 30\nSweep line algorithms\nMany geometric problems can be solved using sweep line algorithms. The idea\nin such algorithms is to represent an instance of the problem as a set of events\nthat correspond to points in the plane. The events are processed in increasing\norder according to their x or y coordinates.\nAs an example, consider the following problem: There is a company that has\nn employees, and we know for each employee their arrival and leaving times on\na certain day. Our task is to calculate the maximum number of employees that\nwere in the of\ufb01ce at the same time.\nThe problem can be solved by modeling the situation so that each employee\nis assigned two events that correspond to their arrival and leaving times. After\nsorting the events, we go through them and keep track of the number of people\nin the of\ufb01ce. For example, the table\nperson arrival time leaving time\nJohn 10 15\nMaria 6 12\nPeter 14 16\nLisa 5 13\ncorresponds to the following events:\nJohn\nMaria\nPeter\nLisa\nWe go through the events from left to right and maintain a counter. Always when\na person arrives, we increase the value of the counter by one, and when a person\nleaves, we decrease the value of the counter by one. The answer to the problem is\nthe maximum value of the counter during the algorithm.\nIn the example, the events are processed as follows:\n275",
            "John\nMaria\nPeter\nLisa\n+ \u2212+ \u2212 + \u2212+ \u2212\n3 12 2 2 01 1\nThe symbols + and \u2212 indicate whether the value of the counter increases or\ndecreases, and the value of the counter is shown below. The maximum value of\nthe counter is 3 between John\u2019s arrival and Maria\u2019s leaving.\nThe running time of the algorithm is O(nlogn), because sorting the events\ntakes O(nlogn) time and the rest of the algorithm takes O(n) time.\n30.1 Intersection points\nGiven a set of n line segments, each of them being either horizontal or vertical,\nconsider the problem of counting the total number of intersection points. For\nexample, when the line segments are\nthere are three intersection points:\nIt is easy to solve the problem in O(n2) time, because we can go through all\npossible pairs of line segments and check if they intersect. However, we can solve\nthe problem more ef\ufb01ciently in O(nlogn) time using a sweep line algorithm and\na range query data structure.\nThe idea is to process the endpoints of the line segments from left to right\nand focus on three types of events:\n(1) horizontal segment begins\n(2) horizontal segment ends\n(3) vertical segment\n276",
            "The following events correspond to the example:\n1 2\n1 2\n1 23 3\nWe go through the events from left to right and use a data structure that\nmaintains a set of y coordinates where there is an active horizontal segment. At\nevent 1, we add the y coordinate of the segment to the set, and at event 2, we\nremove the y coordinate from the set.\nIntersection points are calculated at event 3. When there is a vertical segment\nbetween points y1 and y2, we count the number of active horizontal segments\nwhose y coordinate is between y1 and y2, and add this number to the total number\nof intersection points.\nTo store y coordinates of horizontal segments, we can use a binary indexed\nor segment tree, possibly with index compression. When such structures are\nused, processing each event takes O(logn) time, so the total running time of the\nalgorithm is O(nlogn).\n30.2 Closest pair problem\nGiven a set of n points, our next problem is to \ufb01nd two points whose Euclidean\ndistance is minimum. For example, if the points are\nwe should \ufb01nd the following points:\nThis is another example of a problem that can be solved in O(nlogn) time\nusing a sweep line algorithm1. We go through the points from left to right and\nmaintain a value d: the minimum distance between two points seen so far. At\n1Besides this approach, there is also an O(nlogn) time divide-and-conquer algorithm [56] that\ndivides the points into two sets and recursively solves the problem for both sets.\n277",
            "each point, we \ufb01nd the nearest point to the left. If the distance is less than d, it\nis the new minimum distance and we update the value of d.\nIf the current point is (x, y) and there is a point to the left within a distance of\nless than d, the x coordinate of such a point must be between [x \u2212d, x] and the y\ncoordinate must be between [y\u2212d, y+d]. Thus, it suf\ufb01ces to only consider points\nthat are located in those ranges, which makes the algorithm ef\ufb01cient.\nFor example, in the following picture, the region marked with dashed lines\ncontains the points that can be within a distance of d from the active point:\nd\nd\nThe ef\ufb01ciency of the algorithm is based on the fact that the region always\ncontains only O(1) points. We can go through those points in O(logn) time by\nmaintaining a set of points whose x coordinate is between [x \u2212d, x], in increasing\norder according to their y coordinates.\nThe time complexity of the algorithm is O(nlogn), because we go through n\npoints and \ufb01nd for each point the nearest point to the left in O(logn) time.\n30.3 Convex hull problem\nA convex hull is the smallest convex polygon that contains all points of a given\nset. Convexity means that a line segment between any two vertices of the polygon\nis completely inside the polygon.\nFor example, for the points\nthe convex hull is as follows:\n278",
            "Andrew\u2019s algorithm[3] provides an easy way to construct the convex hull\nfor a set of points in O(nlogn) time. The algorithm \ufb01rst locates the leftmost\nand rightmost points, and then constructs the convex hull in two parts: \ufb01rst the\nupper hull and then the lower hull. Both parts are similar, so we can focus on\nconstructing the upper hull.\nFirst, we sort the points primarily according to x coordinates and secondarily\naccording to y coordinates. After this, we go through the points and add each\npoint to the hull. Always after adding a point to the hull, we make sure that\nthe last line segment in the hull does not turn left. As long as it turns left, we\nrepeatedly remove the second last point from the hull.\nThe following pictures show how Andrew\u2019s algorithm works:\n1 2 3 4\n5 6 7 8\n9 10 11 12\n13 14 15 16\n17 18 19 20\n279",
            "280",
            "Bibliography\n[1] A. V. Aho, J. E. Hopcroft and J. Ullman. Data Structures and Algorithms ,\nAddison-Wesley, 1983.\n[2] R. K. Ahuja and J. B. Orlin. Distance directed augmenting path algorithms\nfor maximum \ufb02ow and parametric maximum \ufb02ow problems. Naval Research\nLogistics, 38(3):413\u2013430, 1991.\n[3] A. M. Andrew. Another ef\ufb01cient algorithm for convex hulls in two dimensions.\nInformation Processing Letters, 9(5):216\u2013219, 1979.\n[4] B. Aspvall, M. F. Plass and R. E. Tarjan. A linear-time algorithm for testing\nthe truth of certain quanti\ufb01ed boolean formulas. Information Processing\nLetters, 8(3):121\u2013123, 1979.\n[5] R. Bellman. On a routing problem. Quarterly of Applied Mathematics ,\n16(1):87\u201390, 1958.\n[6] M. Beck, E. Pine, W. Tarrat and K. Y. Jensen. New integer representations\nas the sum of three cubes. Mathematics of Computation, 76(259):1683\u20131690,\n2007.\n[7] M. A. Bender and M. Farach-Colton. The LCA problem revisited. In Latin\nAmerican Symposium on Theoretical Informatics, 88\u201394, 2000.\n[8] J. Bentley. Programming Pearls. Addison-Wesley, 1999 (2nd edition).\n[9] J. Bentley and D. Wood. An optimal worst case algorithm for reporting inter-\nsections of rectangles. IEEE Transactions on Computers, C-29(7):571\u2013577,\n1980.\n[10] C. L. Bouton. Nim, a game with a complete mathematical theory.Annals of\nMathematics, 3(1/4):35\u201339, 1901.\n[11] Croatian Open Competition in Informatics, http://hsin.hr/coci/\n[12] Codeforces: On \u201dMo\u2019s algorithm\u201d, http://codeforces.com/blog/entry/\n20032\n[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein. Introduction to\nAlgorithms, MIT Press, 2009 (3rd edition).\n281",
            "[14] E. W. Dijkstra. A note on two problems in connexion with graphs. Nu-\nmerische Mathematik, 1(1):269\u2013271, 1959.\n[15] K. Diks et al. Looking for a Challenge? The Ultimate Problem Set from\nthe University of Warsaw Programming Competitions, University of Warsaw,\n2012.\n[16] M. Dima and R. Ceterchi. Ef\ufb01cient range minimum queries using binary\nindexed trees. Olympiad in Informatics, 9(1):39\u201344, 2015.\n[17] J. Edmonds. Paths, trees, and \ufb02owers.Canadian Journal of Mathematics,\n17(3):449\u2013467, 1965.\n[18] J. Edmonds and R. M. Karp. Theoretical improvements in algorithmic ef\ufb01-\nciency for network \ufb02ow problems. Journal of the ACM, 19(2):248\u2013264, 1972.\n[19] S. Even, A. Itai and A. Shamir. On the complexity of time table and multi-\ncommodity \ufb02ow problems. 16th Annual Symposium on Foundations of Com-\nputer Science, 184\u2013193, 1975.\n[20] D. Fanding. A faster algorithm for shortest-path \u2013 SPFA.Journal of South-\nwest Jiaotong University, 2, 1994.\n[21] P. M. Fenwick. A new data structure for cumulative frequency tables.Soft-\nware: Practice and Experience, 24(3):327\u2013336, 1994.\n[22] J. Fischer and V. Heun. Theoretical and practical improvements on the\nRMQ-problem, with applications to LCA and LCE. In Annual Symposium on\nCombinatorial Pattern Matching, 36\u201348, 2006.\n[23] R. W. Floyd Algorithm 97: shortest path. Communications of the ACM ,\n5(6):345, 1962.\n[24] L. R. Ford. Network \ufb02ow theory. RAND Corporation, Santa Monica, Califor-\nnia, 1956.\n[25] L. R. Ford and D. R. Fulkerson. Maximal \ufb02ow through a network. Canadian\nJournal of Mathematics, 8(3):399\u2013404, 1956.\n[26] R. Freivalds. Probabilistic machines can use less running time. In IFIP\ncongress, 839\u2013842, 1977.\n[27] F. Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings\nof the 39th International Symposium on Symbolic and Algebraic Computation,\n296\u2013303, 2014.\n[28] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to\nthe Theory of NP-Completeness, W. H. Freeman and Company, 1979.\n[29] Google Code Jam Statistics (2017), https://www.go-hero.net/jam/17\n282",
            "[30] A. Gr\u00f8nlund and S. Pettie. Threesomes, degenerates, and love triangles.\nIn Proceedings of the 55th Annual Symposium on Foundations of Computer\nScience, 621\u2013630, 2014.\n[31] P. M. Grundy. Mathematics and games. Eureka, 2(5):6\u20138, 1939.\n[32] D. Gus\ufb01eld. Algorithms on Strings, Trees and Sequences: Computer Science\nand Computational Biology, Cambridge University Press, 1997.\n[33] S. Halim and F. Halim. Competitive Programming 3: The New Lower Bound\nof Programming Contests, 2013.\n[34] M. Held and R. M. Karp. A dynamic programming approach to sequencing\nproblems. Journal of the Society for Industrial and Applied Mathematics ,\n10(1):196\u2013210, 1962.\n[35] C. Hierholzer and C. Wiener. \u00dcber die M\u00f6glichkeit, einen Linienzug ohne\nWiederholung und ohne Unterbrechung zu umfahren. Mathematische An-\nnalen, 6(1), 30\u201332, 1873.\n[36] C. A. R. Hoare. Algorithm 64: Quicksort. Communications of the ACM ,\n4(7):321, 1961.\n[37] C. A. R. Hoare. Algorithm 65: Find. Communications of the ACM, 4(7):321\u2013\n322, 1961.\n[38] J. E. Hopcroft and J. D. Ullman. A linear list merging algorithm. Technical\nreport, Cornell University, 1971.\n[39] E. Horowitz and S. Sahni. Computing partitions with applications to the\nknapsack problem. Journal of the ACM, 21(2):277\u2013292, 1974.\n[40] D. A. Huffman. A method for the construction of minimum-redundancy\ncodes. Proceedings of the IRE, 40(9):1098\u20131101, 1952.\n[41] The International Olympiad in Informatics Syllabus, https://people.ksp.\nsk/~misof/ioi-syllabus/\n[42] R. M. Karp and M. O. Rabin. Ef\ufb01cient randomized pattern-matching algo-\nrithms. IBM Journal of Research and Development, 31(2):249\u2013260, 1987.\n[43] P. W. Kasteleyn. The statistics of dimers on a lattice: I. The number of dimer\narrangements on a quadratic lattice. Physica, 27(12):1209\u20131225, 1961.\n[44] C. Kent, G. M. Landau and M. Ziv-Ukelson. On the complexity of sparse\nexon assembly. Journal of Computational Biology, 13(5):1013\u20131027, 2006.\n[45] J. Kleinberg and \u00c9. Tardos. Algorithm Design, Pearson, 2005.\n[46] D. E. Knuth. The Art of Computer Programming. Volume 2: Seminumerical\nAlgorithms, Addison\u2013Wesley, 1998 (3rd edition).\n283",
            "[47] D. E. Knuth. The Art of Computer Programming. Volume 3: Sorting and\nSearching, Addison\u2013Wesley, 1998 (2nd edition).\n[48] J. B. Kruskal. On the shortest spanning subtree of a graph and the travel-\ning salesman problem. Proceedings of the American Mathematical Society,\n7(1):48\u201350, 1956.\n[49] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions,\nand reversals. Soviet physics doklady, 10(8):707\u2013710, 1966.\n[50] M. G. Main and R. J. Lorentz. An O(nlogn) algorithm for \ufb01nding all repeti-\ntions in a string. Journal of Algorithms, 5(3):422\u2013432, 1984.\n[51] J. Pachocki and J. Radoszewski. Where to use and how not to use polynomial\nstring hashing. Olympiads in Informatics, 7(1):90\u2013100, 2013.\n[52] I. Parberry. An ef\ufb01cient algorithm for the Knight\u2019s tour problem.Discrete\nApplied Mathematics, 73(3):251\u2013260, 1997.\n[53] D. Pearson. A polynomial-time algorithm for the change-making problem.\nOperations Research Letters, 33(3):231\u2013234, 2005.\n[54] R. C. Prim. Shortest connection networks and some generalizations. Bell\nSystem Technical Journal, 36(6):1389\u20131401, 1957.\n[55] 27-Queens Puzzle: Massively Parallel Enumeration and Solution Counting.\nhttps://github.com/preusser/q27\n[56] M. I. Shamos and D. Hoey. Closest-point problems. InProceedings of the 16th\nAnnual Symposium on Foundations of Computer Science, 151\u2013162, 1975.\n[57] M. Sharir. A strong-connectivity algorithm and its applications in data \ufb02ow\nanalysis. Computers & Mathematics with Applications, 7(1):67\u201372, 1981.\n[58] S. S. Skiena. The Algorithm Design Manual, Springer, 2008 (2nd edition).\n[59] S. S. Skiena and M. A. Revilla. Programming Challenges: The Programming\nContest Training Manual, Springer, 2003.\n[60] SZKOpu\u0142, https://szkopul.edu.pl/\n[61] R. Sprague. \u00dcber mathematische Kampfspiele. Tohoku Mathematical Jour-\nnal, 41:438\u2013444, 1935.\n[62] P. Sta\u00b4nczyk. Algorytmika praktyczna w konkursach Informatycznych, MSc\nthesis, University of Warsaw, 2006.\n[63] V. Strassen. Gaussian elimination is not optimal. Numerische Mathematik,\n13(4):354\u2013356, 1969.\n[64] R. E. Tarjan. Ef\ufb01ciency of a good but not linear set union algorithm.Journal\nof the ACM, 22(2):215\u2013225, 1975.\n284",
            "[65] R. E. Tarjan. Applications of path compression on balanced trees. Journal of\nthe ACM, 26(4):690\u2013715, 1979.\n[66] R. E. Tarjan and U. Vishkin. Finding biconnected componemts and comput-\ning tree functions in logarithmic parallel time. In Proceedings of the 25th\nAnnual Symposium on Foundations of Computer Science, 12\u201320, 1984.\n[67] H. N. V. Temperley and M. E. Fisher. Dimer problem in statistical mechanics\n\u2013 an exact result. Philosophical Magazine, 6(68):1061\u20131063, 1961.\n[68] USA Computing Olympiad, http://www.usaco.org/\n[69] H. C. von Warnsdorf.Des R\u00f6sselsprunges einfachste und allgemeinste L\u00f6sung.\nSchmalkalden, 1823.\n[70] S. Warshall. A theorem on boolean matrices.Journal of the ACM, 9(1):11\u201312,\n1962.\n285",
            "286",
            "Index\n2SAT problem, 160\n2SUM problem, 78\n3SAT problem, 162\n3SUM problem, 79\nadjacency list, 113\nadjacency matrix, 114\nalphabet, 243\namortized analysis, 77\nancestor, 163\nand operation, 96\nAndrew\u2019s algorithm, 279\nantichain, 193\narithmetic progression, 10\nbacktracking, 50\nBellman\u2013Ford algorithm, 123\nbinary code, 62\nbinary indexed tree, 86\nbinary search, 31\nbinary tree, 139\nBinet\u2019s formula, 14\nbinomial coef\ufb01cient, 208\nbinomial distribution, 230\nbipartite graph, 112, 122\nbirthday paradox, 247\nbit representation, 95\nbit shift, 97\nbitset, 41\nborder, 244\nbreadth-\ufb01rst search, 119\nbubble sort, 25\nBurnside\u2019s lemma, 214\nCatalan number, 210\nCayley\u2019s formula, 215\nchild, 133\nChinese remainder theorem, 205\nclosest pair, 277\ncodeword, 62\ncofactor, 219\ncollision, 246\ncoloring, 112, 233\ncombinatorics, 207\ncomparison function, 31\ncomparison operator, 30\ncomplement, 12\ncomplete graph, 111\ncomplex, 266\ncomplex number, 266\ncomplexity classes, 20\ncomponent, 110\ncomponent graph, 157\nconditional probability, 227\nconjuction, 13\nconnected graph, 110, 121\nconstant factor, 21\nconstant-time algorithm, 20\ncoprime, 201\ncounting sort, 28\ncross product, 268\ncubic algorithm, 20\ncut, 182\ncycle, 109, 121, 149, 155\ncycle detection, 155\ndata compression, 62\ndata structure, 35\nDe Bruijn sequence, 178\ndegree, 111\ndepth-\ufb01rst search, 117\ndeque, 42\nderangement, 213\ndeterminant, 219\ndiameter, 135\n287",
            "difference, 12\ndifference array, 93\nDijkstra\u2019s algorithm, 126, 153\nDilworth\u2019s theorem, 193\nDiophantine equation, 204\nDirac\u2019s theorem, 177\ndirected graph, 110\ndisjunction, 13\ndistance function, 272\ndistribution, 229\ndivisibility, 197\ndivisor, 197\ndynamic array, 35\ndynamic programming, 65\ndynamic segment tree, 261\nedge, 109\nedge list, 115\nedit distance, 74\nEdmonds\u2013Karp algorithm, 184\nequivalence, 13\nEuclid\u2019s algorithm, 200\nEuclid\u2019s formula, 206\nEuclidean distance, 272\nEuler tour technique, 168\nEuler\u2019s theorem, 202\nEuler\u2019s totient function, 201\nEulerian circuit, 174\nEulerian path, 173\nexpected value, 229\nextended Euclid\u2019s algorithm, 204\nfactor, 197\nfactorial, 14\nFaulhaber\u2019s formula, 10\nFenwick tree, 86\nFermat\u2019s theorem, 202\nFibonacci number, 14, 206, 220\n\ufb02oating point number, 7\n\ufb02ow, 181\nFloyd\u2019s algorithm, 156\nFloyd\u2013Warshall algorithm, 129\nFord\u2013Fulkerson algorithm, 182\nFreivalds\u2019 algoritm, 232\nfunctional graph, 154\ngeometric distribution, 230\ngeometric progression, 11\ngeometry, 265\nGoldbach\u2019s conjecture, 199\ngraph, 109\ngreatest common divisor, 200\ngreedy algorithm, 57\nGrundy number, 238\nGrundy\u2019s game, 241\nHall\u2019s theorem, 189\nHamiltonian circuit, 177\nHamiltonian path, 177\nHamming distance, 100\nharmonic sum, 11, 200\nhash value, 245\nhashing, 245\nheap, 43\nHeron\u2019s formula, 265\nheuristic, 179\nHierholzer\u2019s algorithm, 175\nHuffman coding, 63\nidentity matrix, 218\nimplication, 13\nin-order, 139\ninclusion-exclusion, 212\nindegree, 111\nindependence, 228\nindependent set, 190\nindex compression, 93\ninput and output, 4\ninteger, 6\nintersection, 12\nintersection point, 276\ninverse matrix, 220\ninversion, 26\niterator, 39\nK\u02ddonig\u2019s theorem, 189\nKadane\u2019s algorithm, 23\nKirchhoff\u2019s theorem, 223\nknapsack, 72\nknight\u2019s tour, 179\nKosaraju\u2019s algorithm, 158\nKruskal\u2019s algorithm, 142\nLagrange\u2019s theorem, 205\nLaplacean matrix, 224\n288",
            "Las Vegas algorithm, 231\nlazy propagation, 258\nlazy segment tree, 258\nleaf, 133\nleast common multiple, 200\nLegendre\u2019s conjecture, 199\nLevenshtein distance, 74\nlexicographical order, 244\nline segment intersection, 269\nlinear algorithm, 20\nlinear recurrence, 220\nlogarithm, 14\nlogarithmic algorithm, 20\nlogic, 13\nlongest increasing subsequence, 70\nlosing state, 235\nlowest common ancestor, 167\nmacro, 9\nManhattan distance, 272\nmap, 38\nMarkov chain, 230\nmatching, 187\nmatrix, 217\nmatrix multiplication, 218, 232\nmatrix power, 219\nmaximum \ufb02ow, 181\nmaximum independent set, 190\nmaximum matching, 187\nmaximum query, 83\nmaximum spanning tree, 142\nmaximum subarray sum, 21\nmeet in the middle, 54\nmemoization, 67\nmerge sort, 27\nmex function, 238\nminimum cut, 182, 185\nminimum node cover, 189\nminimum query, 83\nminimum spanning tree, 141\nmis\u00e8re game, 238\nMo\u2019s algorithm, 255\nmodular arithmetic, 6, 201\nmodular inverse, 202\nMonte Carlo algorithm, 231\nmultinomial coef\ufb01cient, 210\nnatural logarithm, 15\nnearest smaller elements, 79\nnegation, 13\nnegative cycle, 125\nneighbor, 111\nnext_permutation, 49\nnim game, 237\nnim sum, 237\nnode, 109\nnode cover, 189\nnot operation, 97\nNP-hard problem, 20\nnumber theory, 197\nor operation, 96\norder statistic, 232\nOre\u2019s theorem, 177\noutdegree, 111\npair, 30\nparent, 133\nparenthesis expression, 211\nPascal\u2019s triangle, 209\npath, 109\npath cover, 190\npattern matching, 243\nperfect matching, 189\nperfect number, 198\nperiod, 243\npermutation, 49\npersistent segment tree, 262\nPick\u2019s theorem, 272\npoint, 266\npolynomial algorithm, 20\npolynomial hashing, 245\npost-order, 139\nPr\u00fcfer code, 216\npre-order, 139\npredicate, 13\npre\ufb01x, 243\npre\ufb01x sum array, 84\nPrim\u2019s algorithm, 147\nprime, 197\nprime decomposition, 197\npriority queue, 43\nprobability, 225\nprogramming language, 3\n289",
            "Pythagorean triple, 206\nquadratic algorithm, 20\nquanti\ufb01er, 13\nqueen problem, 50\nqueue, 43\nquickselect, 232\nquicksort, 232\nrandom variable, 228\nrandom_shuffle, 39\nrandomized algorithm, 231\nrange query, 83\nregular graph, 111\nremainder, 6\nreverse, 39\nroot, 133\nrooted tree, 133\nrotation, 243\nscaling algorithm, 185\nsegment tree, 89, 257\nset, 12, 37\nset theory, 12\nshoelace formula, 271\nshortest path, 123\nsieve of Eratosthenes, 200\nsimple graph, 112\nsliding window, 81\nsliding window minimum, 81\nsort, 29, 39\nsorting, 25\nspanning tree, 141, 223\nsparse segment tree, 261\nsparse table, 85\nSPFA algorithm, 126\nSprague\u2013Grundy theorem, 238\nsquare matrix, 217\nsquare root algorithm, 251\nstack, 42\nstring, 36, 243\nstring hashing, 245\nstrongly connected component, 157\nstrongly connected graph, 157\nsubsequence, 243\nsubset, 12, 47\nsubstring, 243\nsubtree, 133\nsuccessor graph, 154\nsuf\ufb01x, 243\nsum query, 83\nsweep line, 275\ntime complexity, 17\ntopological sorting, 149\ntranspose, 217\ntree, 110, 133\ntree query, 163\ntree traversal array, 164\ntrie, 244\ntuple, 30\ntypedef, 8\ntwin prime, 199\ntwo pointers method, 77\ntwo-dimensional segment tree, 264\nuniform distribution, 230\nunion, 12\nunion-\ufb01nd structure, 145\nuniversal set, 12\nvector, 35, 217, 266\nWarnsdorf\u2019s rule, 179\nweighted graph, 111\nWilson\u2019s theorem, 206\nwinning state, 235\nxor operation, 97\nZ-algorithm, 247\nZ-array, 247\nZeckendorf\u2019s theorem, 206\n290"
        ]
    }
}