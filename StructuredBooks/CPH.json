{
    "contents": {
        "0": [
            "Competitive Programmers HandbookAntti LaaksonenDraft August 19, 2019",
            "ii",
            "ContentsPreface ixI Basic techniques 11 Introduction 31.1 Programming languages . . . . . . . . . . . . . . . . . . . . . . . . . 31.2 Input and output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41.3 Working with numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 61.4 Shortening code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81.5 Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.6 Contests and resources . . . . . . . . . . . . . . . . . . . . . . . . . . 152 Time complexity 172.1 Calculation rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172.2 Complexity classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202.3 Estimating efciency . . . . . . . . . . . . . . . . . . . . . . . . . . . 212.4 Maximum subarray sum . . . . . . . . . . . . . . . . . . . . . . . . . 213 Sorting 253.1 Sorting theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253.2 Sorting in C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293.3 Binary search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314 Data structures 354.1 Dynamic arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354.2 Set structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374.3 Map structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384.4 Iterators and ranges . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394.5 Other structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414.6 Comparison to sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . 445 Complete search 475.1 Generating subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475.2 Generating permutations . . . . . . . . . . . . . . . . . . . . . . . . . 495.3 Backtracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505.4 Pruning the search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515.5 Meet in the middle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54iii",
            "6 Greedy algorithms 576.1 Coin problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576.2 Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586.3 Tasks and deadlines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606.4 Minimizing sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616.5 Data compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627 Dynamic programming 657.1 Coin problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657.2 Longest increasing subsequence . . . . . . . . . . . . . . . . . . . . . 707.3 Paths in a grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717.4 Knapsack problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727.5 Edit distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747.6 Counting tilings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 758 Amortized analysis 778.1 Two pointers method . . . . . . . . . . . . . . . . . . . . . . . . . . . 778.2 Nearest smaller elements . . . . . . . . . . . . . . . . . . . . . . . . . 798.3 Sliding window minimum . . . . . . . . . . . . . . . . . . . . . . . . . 819 Range queries 839.1 Static array queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849.2 Binary indexed tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869.3 Segment tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 899.4 Additional techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 9310 Bit manipulation 9510.1 Bit representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9510.2 Bit operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9610.3 Representing sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9810.4 Bit optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10010.5 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . 102II Graph algorithms 10711 Basics of graphs 10911.1 Graph terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10911.2 Graph representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 11312 Graph traversal 11712.1 Depth-rst search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11712.2 Breadth-rst search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11912.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121iv",
            "13 Shortest paths 12313.1 BellmanFord algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 12313.2 Dijkstras algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12613.3 FloydWarshall algorithm . . . . . . . . . . . . . . . . . . . . . . . . 12914 Tree algorithms 13314.1 Tree traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13414.2 Diameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13514.3 All longest paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13714.4 Binary trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13915 Spanning trees 14115.1 Kruskals algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14215.2 Union-nd structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14515.3 Prims algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14716 Directed graphs 14916.1 Topological sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14916.2 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . 15116.3 Successor paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15416.4 Cycle detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15517 Strong connectivity 15717.1 Kosarajus algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15817.2 2SAT problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16018 Tree queries 16318.1 Finding ancestors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16318.2 Subtrees and paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16418.3 Lowest common ancestor . . . . . . . . . . . . . . . . . . . . . . . . . 16718.4 Ofine algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17019 Paths and circuits 17319.1 Eulerian paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17319.2 Hamiltonian paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17719.3 De Bruijn sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17819.4 Knights tours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17920 Flows and cuts 18120.1 FordFulkerson algorithm . . . . . . . . . . . . . . . . . . . . . . . . 18220.2 Disjoint paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18620.3 Maximum matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . 18720.4 Path covers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190v",
            "III Advanced topics 19521 Number theory 19721.1 Primes and factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19721.2 Modular arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20121.3 Solving equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20421.4 Other results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20522 Combinatorics 20722.1 Binomial coefcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20822.2 Catalan numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21022.3 Inclusion-exclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21222.4 Burnsides lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21422.5 Cayleys formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21523 Matrices 21723.1 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21723.2 Linear recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22023.3 Graphs and matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 22224 Probability 22524.1 Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22524.2 Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22624.3 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22824.4 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23024.5 Randomized algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 23125 Game theory 23525.1 Game states . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23525.2 Nim game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23725.3 SpragueGrundy theorem . . . . . . . . . . . . . . . . . . . . . . . . 23826 String algorithms 24326.1 String terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24326.2 Trie structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24426.3 String hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24526.4 Z-algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24727 Square root algorithms 25127.1 Combining algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 25227.2 Integer partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25427.3 Mos algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25528 Segment trees revisited 25728.1 Lazy propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25828.2 Dynamic trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26128.3 Data structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26328.4 Two-dimensionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264vi",
            "29 Geometry 26529.1 Complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26629.2 Points and lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26829.3 Polygon area . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27129.4 Distance functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27230 Sweep line algorithms 27530.1 Intersection points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27630.2 Closest pair problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27730.3 Convex hull problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278Bibliography 281vii",
            "viii",
            "PrefaceThe purpose of this book is to give you a thorough introduction to competitiveprogramming. It is assumed that you already know the basics of programming,but no previous background in competitive programming is needed.The book is especially intended for students who want to learn algorithmsand possibly participate in the International Olympiad in Informatics (IOI) or inthe International Collegiate Programming Contest (ICPC). Of course, the book isalso suitable for anybody else interested in competitive programming.It takes a long time to become a good competitive programmer, but it is alsoan opportunity to learn a lot. You can be sure that you will get a good generalunderstanding of algorithms if you spend time reading the book, solving problemsand taking part in contests.The book is under continuous development. You can always send feedback onthe book to ahslaaks@cs.helsinki.fi.Helsinki, August 2019Antti Laaksonenix",
            "x",
            "Part IBasic techniques1",
            "",
            "Chapter 1IntroductionCompetitive programming combines two topics: (1) the design of algorithms and(2) the implementation of algorithms.The design of algorithms consists of problem solving and mathematicalthinking. Skills for analyzing problems and solving them creatively are needed.An algorithm for solving a problem has to be both correct and efcient, and thecore of the problem is often about inventing an efcient algorithm.Theoretical knowledge of algorithms is important to competitive programmers.Typically, a solution to a problem is a combination of well-known techniques andnew insights. The techniques that appear in competitive programming also formthe basis for the scientic research of algorithms.The implementation of algorithms requires good programming skills. Incompetitive programming, the solutions are graded by testing an implementedalgorithm using a set of test cases. Thus, it is not enough that the idea of thealgorithm is correct, but the implementation also has to be correct.A good coding style in contests is straightforward and concise. Programsshould be written quickly, because there is not much time available. Unlike intraditional software engineering, the programs are short (usually at most a fewhundred lines of code), and they do not need to be maintained after the contest.1.1 Programming languagesAt the moment, the most popular programming languages used in contests areC++, Python and Java. For example, in Google Code Jam 2017, among the best3,000 participants, 79 % used C++, 16 % used Python and 8 % used Java [ 29].Some participants also used several languages.Many people think that C++ is the best choice for a competitive programmer,and C++ is nearly always available in contest systems. The benets of using C++are that it is a very efcient language and its standard library contains a largecollection of data structures and algorithms.On the other hand, it is good to master several languages and understandtheir strengths. For example, if large integers are needed in the problem, Pythoncan be a good choice, because it contains built-in operations for calculating with3",
            "large integers. Still, most problems in programming contests are set so that usinga specic programming language is not an unfair advantage.All example programs in this book are written in C++, and the standardlibrarys data structures and algorithms are often used. The programs follow theC++11 standard, which can be used in most contests nowadays. If you cannotprogram in C++ yet, now is a good time to start learning.C++ code templateA typical C++ code template for competitive programming looks like this:#include <bits/stdc++.h>using namespace std;int main() {// solution comes here}The #include line at the beginning of the code is a feature of the g++ compilerthat allows us to include the entire standard library. Thus, it is not needed toseparately include libraries such as iostream, vector and algorithm, but ratherthey are available automatically.The using line declares that the classes and functions of the standard librarycan be used directly in the code. Without the using line we would have to write,for example, std::cout, but now it sufces to write cout.The code can be compiled using the following command:g++ -std=c++11 -O2 -Wall test.cpp -o testThis command produces a binary le test from the source code test.cpp. Thecompiler follows the C++11 standard (-std=c++11), optimizes the code (-O2) andshows warnings about possible errors (-Wall).1.2 Input and outputIn most contests, standard streams are used for reading input and writing output.In C++, the standard streams are cin for input and cout for output. In addition,the C functions scanf and printf can be used.The input for the program usually consists of numbers and strings that areseparated with spaces and newlines. They can be read from the cin stream asfollows:int a, b;string x;cin >> a >> b >> x;4",
            "This kind of code always works, assuming that there is at least one space ornewline between each element in the input. For example, the above code canread both of the following inputs:123 456 monkey123 456monkeyThe cout stream is used for output as follows:int a = 123, b = 456;string x = \"monkey\";cout << a << \" \" << b << \" \" << x << \"\\n\";Input and output is sometimes a bottleneck in the program. The followinglines at the beginning of the code make input and output more efcient:ios::sync_with_stdio(0);cin.tie(0);Note that the newline \"\\n\" works faster than endl, because endl alwayscauses a ush operation.The C functions scanf and printf are an alternative to the C++ standardstreams. They are usually a bit faster, but they are also more difcult to use. Thefollowing code reads two integers from the input:int a, b;scanf(\"%d %d\", &a, &b);The following code prints two integers:int a = 123, b = 456;printf(\"%d %d\\n\", a, b);Sometimes the program should read a whole line from the input, possiblycontaining spaces. This can be accomplished by using the getline function:string s;getline(cin, s);If the amount of data is unknown, the following loop is useful:while (cin >> x) {// code}This loop reads elements from the input one after another, until there is no moredata available in the input.5",
            "In some contest systems, les are used for input and output. An easy solutionfor this is to write the code as usual using standard streams, but add the followinglines to the beginning of the code:freopen(\"input.txt\", \"r\", stdin);freopen(\"output.txt\", \"w\", stdout);After this, the program reads the input from the le input.txt and writes theoutput to the le output.txt.1.3 Working with numbersIntegersThe most used integer type in competitive programming is int, which is a 32-bittype with a value range of 231 ... 231 1 or about 2109 ... 2109. If the typeint is not enough, the 64-bit type long long can be used. It has a value range of263 ... 263 1 or about 91018 ... 91018.The following code denes a long long variable:long long x = 123456789123456789LL;The sufx LL means that the type of the number is long long.A common mistake when using the type long long is that the type int is stillused somewhere in the code. For example, the following code contains a subtleerror:int a = 123456789;long long b = a*a;cout << b << \"\\n\"; // -1757895751Even though the variable b is of type long long, both numbers in the expres-sion a*a are of type int and the result is also of type int. Because of this, thevariable b will contain a wrong result. The problem can be solved by changingthe type of a to long long or by changing the expression to (long long)a*a.Usually contest problems are set so that the type long long is enough. Still,it is good to know that the g++ compiler also provides a 128-bit type __int128_twith a value range of 2127 ... 2127 1 or about 1038 ... 1038. However, this typeis not available in all contest systems.Modular arithmeticWe denote by x mod m the remainder when x is divided by m. For example,17 mod 5 = 2, because 17 = 35+2.Sometimes, the answer to a problem is a very large number but it is enoughto output it modulo m, i.e., the remainder when the answer is divided by m (for6",
            "example, modulo 109 +7). The idea is that even if the actual answer is verylarge, it sufces to use the types int and long long.An important property of the remainder is that in addition, subtraction andmultiplication, the remainder can be taken before the operation:(a +b) mod m = (a mod m +b mod m) mod m(a b) mod m = (a mod m b mod m) mod m(a b) mod m = (a mod m b mod m) mod mThus, we can take the remainder after every operation and the numbers willnever become too large.For example, the following code calculates n!, the factorial of n, modulo m:long long x = 1;for (int i = 2; i <= n; i++) {x = (x*i)%m;}cout << x%m << \"\\n\";Usually we want the remainder to always be between 0... m 1. However, inC++ and other languages, the remainder of a negative number is either zero ornegative. An easy way to make sure there are no negative remainders is to rstcalculate the remainder as usual and then add m if the result is negative:x = x%m;if (x < 0) x += m;However, this is only needed when there are subtractions in the code and theremainder may become negative.Floating point numbersThe usual oating point types in competitive programming are the 64-bit doubleand, as an extension in the g++ compiler, the 80-bit long double. In most cases,double is enough, but long double is more accurate.The required precision of the answer is usually given in the problem statement.An easy way to output the answer is to use the printf function and give thenumber of decimal places in the formatting string. For example, the followingcode prints the value of x with 9 decimal places:printf(\"%.9f\\n\", x);A difculty when using oating point numbers is that some numbers cannotbe represented accurately as oating point numbers, and there will be roundingerrors. For example, the result of the following code is surprising:double x = 0.3*3+0.1;printf(\"%.20f\\n\", x); // 0.999999999999999888987",
            "Due to a rounding error, the value ofx is a bit smaller than 1, while the correctvalue would be 1.It is risky to compare oating point numbers with the == operator, because itis possible that the values should be equal but they are not because of precisionerrors. A better way to compare oating point numbers is to assume that twonumbers are equal if the difference between them is less than , where  is asmall number.In practice, the numbers can be compared as follows ( = 109):if (abs(a-b) < 1e-9) {// a and b are equal}Note that while oating point numbers are inaccurate, integers up to a certainlimit can still be represented accurately. For example, using double, it is possibleto accurately represent all integers whose absolute value is at most 253.1.4 Shortening codeShort code is ideal in competitive programming, because programs should bewritten as fast as possible. Because of this, competitive programmers often deneshorter names for datatypes and other parts of code.Type namesUsing the command typedef it is possible to give a shorter name to a datatype.For example, the name long long is long, so we can dene a shorter name ll:typedef long long ll;After this, the codelong long a = 123456789;long long b = 987654321;cout << a*b << \"\\n\";can be shortened as follows:ll a = 123456789;ll b = 987654321;cout << a*b << \"\\n\";The command typedef can also be used with more complex types. For example,the following code gives the name vi for a vector of integers and the name pi fora pair that contains two integers.typedef vector<int> vi;typedef pair<int,int> pi;8",
            "MacrosAnother way to shorten code is to dene macros. A macro means that certainstrings in the code will be changed before the compilation. In C++, macros aredened using the #define keyword.For example, we can dene the following macros:#define F first#define S second#define PB push_back#define MP make_pairAfter this, the codev.push_back(make_pair(y1,x1));v.push_back(make_pair(y2,x2));int d = v[i].first+v[i].second;can be shortened as follows:v.PB(MP(y1,x1));v.PB(MP(y2,x2));int d = v[i].F+v[i].S;A macro can also have parameters which makes it possible to shorten loopsand other structures. For example, we can dene the following macro:#define REP(i,a,b) for (int i = a; i <= b; i++)After this, the codefor (int i = 1; i <= n; i++) {search(i);}can be shortened as follows:REP(i,1,n) {search(i);}Sometimes macros cause bugs that may be difcult to detect. For example,consider the following macro that calculates the square of a number:#define SQ(a) a*aThis macro does not always work as expected. For example, the codecout << SQ(3+3) << \"\\n\";9",
            "corresponds to the codecout << 3+3*3+3 << \"\\n\"; // 15A better version of the macro is as follows:#define SQ(a) (a)*(a)Now the codecout << SQ(3+3) << \"\\n\";corresponds to the codecout << (3+3)*(3+3) << \"\\n\"; // 361.5 MathematicsMathematics plays an important role in competitive programming, and it isnot possible to become a successful competitive programmer without havinggood mathematical skills. This section discusses some important mathematicalconcepts and formulas that are needed later in the book.Sum formulasEach sum of the formnx=1xk = 1k +2k +3k +... +nk,where k is a positive integer, has a closed-form formula that is a polynomial ofdegree k +1. For example1,nx=1x = 1+2+3+... +n = n(n +1)2andnx=1x2 = 12 +22 +32 +... +n2 = n(n +1)(2n +1)6 .An arithmetic progression is a sequence of numbers where the differencebetween any two consecutive numbers is constant. For example,3,7,11,151 There is even a general formula for such sums, called Faulhabers formula, but it is toocomplex to be presented here.10",
            "is an arithmetic progression with constant 4. The sum of an arithmetic progres-sion can be calculated using the formulaa ++ b  n numbers= n(a +b)2where a is the rst number, b is the last number and n is the amount of numbers.For example,3+7+11+15 = 4(3+15)2 = 36.The formula is based on the fact that the sum consists of n numbers and thevalue of each number is (a +b)/2 on average.A geometric progression is a sequence of numbers where the ratio betweenany two consecutive numbers is constant. For example,3,6,12,24is a geometric progression with constant 2. The sum of a geometric progressioncan be calculated using the formulaa +ak +ak2 ++ b = bk ak 1where a is the rst number, b is the last number and the ratio between consecu-tive numbers is k. For example,3+6+12+24 = 242321 = 45.This formula can be derived as follows. LetS = a +ak +ak2 ++ b.By multiplying both sides by k, we getkS = ak +ak2 +ak3 ++ bk,and solving the equationkS S = bk ayields the formula.A special case of a sum of a geometric progression is the formula1+2+4+8+... +2n1 = 2n 1.A harmonic sum is a sum of the formnx=11x = 1+ 12 + 13 +... + 1n.An upper bound for a harmonic sum is log2(n) +1. Namely, we can modifyeach term 1/k so that k becomes the nearest power of two that does not exceed k.For example, when n = 6, we can estimate the sum as follows:1+ 12 + 13 + 14 + 15 + 16  1+ 12 + 12 + 14 + 14 + 14.This upper bound consists of log2(n)+1 parts (1, 21/2, 41/4, etc.), and the valueof each part is at most 1.11",
            "Set theoryA set is a collection of elements. For example, the setX = {2,4,7}contains elements 2, 4 and 7. The symbol  denotes an empty set, and |S| denotesthe size of a set S, i.e., the number of elements in the set. For example, in theabove set, |X| =3.If a set S contains an element x, we write x  S, and otherwise we write x  S.For example, in the above set4  X and 5  X.New sets can be constructed using set operations: The intersection A B consists of elements that are in both A and B. Forexample, if A = {1,2,5} and B = {2,4}, then A B = {2}. The union A  B consists of elements that are in A or B or both. Forexample, if A = {3,7} and B = {2,3,8}, then A B = {2,3,7,8}. The complement A consists of elements that are not in A. The interpre-tation of a complement depends on the universal set, which contains allpossible elements. For example, if A = {1,2,5,7} and the universal set is{1,2,..., 10}, then A = {3,4,6,8,9,10}. The difference A \\ B = A  B consists of elements that are in A but notin B. Note that B can contain elements that are not in A. For example, ifA = {2,3,7,8} and B = {3,5,8}, then A \\ B = {2,7}.If each element of A also belongs to S, we say that A is a subset of S, denotedby A  S. A set S always has 2|S| subsets, including the empty set. For example,the subsets of the set {2,4,7} are, {2}, {4}, {7}, {2,4}, {2,7}, {4,7} and {2,4,7}.Some often used sets are N (natural numbers), Z (integers), Q (rationalnumbers) and R (real numbers). The set N can be dened in two ways, dependingon the situation: either N = {0,1,2,... } or N = {1,2,3,...}.We can also construct a set using a rule of the form{f (n) :n  S},where f (n) is some function. This set contains all elements of the form f (n),where n is an element in S. For example, the setX = {2n : n  Z}contains all even integers.12",
            "LogicThe value of a logical expression is either true (1) or false (0). The most impor-tant logical operators are  (negation),  (conjunction),  (disjunction), (implication) and  (equivalence). The following table shows the meaningsof these operators:A B A B A B A B A  B A  B0 0 1 1 0 0 1 10 1 1 0 0 1 1 01 0 0 1 0 1 0 01 1 0 0 1 1 1 1The expression A has the opposite value of A. The expression A B is trueif both A and B are true, and the expression A B is true if A or B or both aretrue. The expression A  B is true if whenever A is true, also B is true. Theexpression A  B is true if A and B are both true or both false.A predicate is an expression that is true or false depending on its parameters.Predicates are usually denoted by capital letters. For example, we can denea predicate P(x) that is true exactly when x is a prime number. Using thisdenition, P(7) is true but P(8) is false.A quantier connects a logical expression to the elements of a set. The mostimportant quantiers are  (for all) and  (there is). For example,x(y(y < x))means that for each element x in the set, there is an element y in the set suchthat y is smaller than x. This is true in the set of integers, but false in the set ofnatural numbers.Using the notation described above, we can express many kinds of logicalpropositions. For example,x((x > 1P(x))  (a(b(a > 1b > 1 x = ab))))means that if a number x is larger than 1 and not a prime number, then there arenumbers a and b that are larger than 1 and whose product is x. This propositionis true in the set of integers.FunctionsThe function x rounds the number x down to an integer, and the function xrounds the number x up to an integer. For example,3/2 =1 and 3/2 =2.The functions min(x1, x2,..., xn) and max(x1, x2,..., xn) give the smallest andlargest of values x1, x2,..., xn. For example,min(1,2,3) = 1 and max(1 ,2,3) = 3.13",
            "The factorial n! can be denednx=1x = 123... nor recursively0! = 1n! = n (n 1)!The Fibonacci numbers arise in many situations. They can be denedrecursively as follows:f (0) = 0f (1) = 1f (n) = f (n 1)+ f (n 2)The rst Fibonacci numbers are0,1,1,2,3,5,8,13,21,34,55,...There is also a closed-form formula for calculating Fibonacci numbers, which issometimes called Binets formula:f (n) = (1+5)n (15)n2n5.LogarithmsThe logarithm of a number x is denoted logk(x), where k is the base of thelogarithm. According to the denition, log k(x) = a exactly when ka = x.A useful property of logarithms is that logk(x) equals the number of times wehave to divide x by k before we reach the number 1. For example, log2(32) = 5because 5 divisions by 2 are needed:32  16  8  4  2  1Logarithms are often used in the analysis of algorithms, because many ef-cient algorithms halve something at each step. Hence, we can estimate theefciency of such algorithms using logarithms.The logarithm of a product islogk(ab) = logk(a)+logk(b),and consequently,logk(xn) = n logk(x).In addition, the logarithm of a quotient islogk(ab)= logk(a)logk(b).Another useful formula islogu(x) = logk(x)logk(u),14",
            "and using this, it is possible to calculate logarithms to any base if there is a wayto calculate logarithms to some xed base.The natural logarithm ln(x) of a number x is a logarithm whose base ise  2.71828. Another property of logarithms is that the number of digits of aninteger x in base b is logb(x)+1. For example, the representation of 123 in base2 is 1111011 and log2(123)+1 =7.1.6 Contests and resourcesIOIThe International Olympiad in Informatics (IOI) is an annual programmingcontest for secondary school students. Each country is allowed to send a team offour students to the contest. There are usually about 300 participants from 80countries.The IOI consists of two ve-hour long contests. In both contests, the partic-ipants are asked to solve three algorithm tasks of various difculty. The tasksare divided into subtasks, each of which has an assigned score. Even if thecontestants are divided into teams, they compete as individuals.The IOI syllabus [ 41] regulates the topics that may appear in IOI tasks.Almost all the topics in the IOI syllabus are covered by this book.Participants for the IOI are selected through national contests. Before the IOI,many regional contests are organized, such as the Baltic Olympiad in Informatics(BOI), the Central European Olympiad in Informatics (CEOI) and the Asia-PacicInformatics Olympiad (APIO).Some countries organize online practice contests for future IOI participants,such as the Croatian Open Competition in Informatics [11] and the USA Comput-ing Olympiad [68]. In addition, a large collection of problems from Polish contestsis available online [60].ICPCThe International Collegiate Programming Contest (ICPC) is an annual program-ming contest for university students. Each team in the contest consists of threestudents, and unlike in the IOI, the students work together; there is only onecomputer available for each team.The ICPC consists of several stages, and nally the best teams are invited tothe World Finals. While there are tens of thousands of participants in the contest,there are only a small number2 of nal slots available, so even advancing to thenals is a great achievement in some regions.In each ICPC contest, the teams have ve hours of time to solve about tenalgorithm problems. A solution to a problem is accepted only if it solves all testcases efciently. During the contest, competitors may view the results of other2The exact number of nal slots varies from year to year; in 2017, there were 133 nal slots.15",
            "teams, but for the last hour the scoreboard is frozen and it is not possible to seethe results of the last submissions.The topics that may appear at the ICPC are not so well specied as thoseat the IOI. In any case, it is clear that more knowledge is needed at the ICPC,especially more mathematical skills.Online contestsThere are also many online contests that are open for everybody. At the moment,the most active contest site is Codeforces, which organizes contests about weekly.In Codeforces, participants are divided into two divisions: beginners compete inDiv2 and more experienced programmers in Div1. Other contest sites includeAtCoder, CS Academy, HackerRank and Topcoder.Some companies organize online contests with onsite nals. Examples of suchcontests are Facebook Hacker Cup, Google Code Jam and Yandex.Algorithm. Ofcourse, companies also use those contests for recruiting: performing well in acontest is a good way to prove ones skills.BooksThere are already some books (besides this book) that focus on competitiveprogramming and algorithmic problem solving: S. S. Skiena and M. A. Revilla:Programming Challenges: The ProgrammingContest Training Manual [59] S. Halim and F. Halim:Competitive Programming 3: The New Lower Boundof Programming Contests [33] K. Diks et al.: Looking for a Challenge? The Ultimate Problem Set from theUniversity of Warsaw Programming Competitions [15]The rst two books are intended for beginners, whereas the last book containsadvanced material.Of course, general algorithm books are also suitable for competitive program-mers. Some popular books are: T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein: Introduction toAlgorithms [13] J. Kleinberg and . Tardos: Algorithm Design [45] S. S. Skiena: The Algorithm Design Manual [58]16",
            "Chapter 2Time complexityThe efciency of algorithms is important in competitive programming. Usually,it is easy to design an algorithm that solves the problem slowly, but the realchallenge is to invent a fast algorithm. If the algorithm is too slow, it will get onlypartial points or no points at all.The time complexity of an algorithm estimates how much time the algo-rithm will use for some input. The idea is to represent the efciency as a functionwhose parameter is the size of the input. By calculating the time complexity, wecan nd out whether the algorithm is fast enough without implementing it.2.1 Calculation rulesThe time complexity of an algorithm is denoted O( ) where the three dotsrepresent some function. Usually, the variable n denotes the input size. Forexample, if the input is an array of numbers, n will be the size of the array, and ifthe input is a string, n will be the length of the string.LoopsA common reason why an algorithm is slow is that it contains many loops that gothrough the input. The more nested loops the algorithm contains, the slower it is.If there are k nested loops, the time complexity is O(nk).For example, the time complexity of the following code is O(n):for (int i = 1; i <= n; i++) {// code}And the time complexity of the following code is O(n2):for (int i = 1; i <= n; i++) {for (int j = 1; j <= n; j++) {// code}}17",
            "Order of magnitudeA time complexity does not tell us the exact number of times the code insidea loop is executed, but it only shows the order of magnitude. In the followingexamples, the code inside the loop is executed 3n, n +5 and n/2 times, but thetime complexity of each code is O(n).for (int i = 1; i <= 3*n; i++) {// code}for (int i = 1; i <= n+5; i++) {// code}for (int i = 1; i <= n; i += 2) {// code}As another example, the time complexity of the following code is O(n2):for (int i = 1; i <= n; i++) {for (int j = i+1; j <= n; j++) {// code}}PhasesIf the algorithm consists of consecutive phases, the total time complexity is thelargest time complexity of a single phase. The reason for this is that the slowestphase is usually the bottleneck of the code.For example, the following code consists of three phases with time complexitiesO(n), O(n2) and O(n). Thus, the total time complexity is O(n2).for (int i = 1; i <= n; i++) {// code}for (int i = 1; i <= n; i++) {for (int j = 1; j <= n; j++) {// code}}for (int i = 1; i <= n; i++) {// code}18",
            "Several variablesSometimes the time complexity depends on several factors. In this case, the timecomplexity formula contains several variables.For example, the time complexity of the following code is O(nm):for (int i = 1; i <= n; i++) {for (int j = 1; j <= m; j++) {// code}}RecursionThe time complexity of a recursive function depends on the number of timesthe function is called and the time complexity of a single call. The total timecomplexity is the product of these values.For example, consider the following function:void f(int n) {if (n == 1) return;f(n-1);}The call f(n) causes n function calls, and the time complexity of each call is O(1).Thus, the total time complexity is O(n).As another example, consider the following function:void g(int n) {if (n == 1) return;g(n-1);g(n-1);}In this case each function call generates two other calls, except for n = 1. Let ussee what happens when g is called with parameter n. The following table showsthe function calls produced by this single call:function call number of callsg(n) 1g(n 1) 2g(n 2) 4 g(1) 2 n1Based on this, the time complexity is1+2+4++ 2n1 = 2n 1 = O(2n).19",
            "2.2 Complexity classesThe following list contains common time complexities of algorithms:O(1) The running time of a constant-time algorithm does not depend on theinput size. A typical constant-time algorithm is a direct formula thatcalculates the answer.O(logn) A logarithmic algorithm often halves the input size at each step. Therunning time of such an algorithm is logarithmic, because log2 n equals thenumber of times n must be divided by 2 to get 1.O(n) A square root algorithm is slower than O(logn) but faster than O(n).A special property of square roots is that n = n/n, so the square root nlies, in some sense, in the middle of the input.O(n) A linear algorithm goes through the input a constant number of times. Thisis often the best possible time complexity, because it is usually necessary toaccess each input element at least once before reporting the answer.O(nlogn) This time complexity often indicates that the algorithm sorts the input,because the time complexity of efcient sorting algorithms is O(nlogn).Another possibility is that the algorithm uses a data structure where eachoperation takes O(logn) time.O(n2) A quadratic algorithm often contains two nested loops. It is possible togo through all pairs of the input elements in O(n2) time.O(n3) A cubic algorithm often contains three nested loops. It is possible to gothrough all triplets of the input elements in O(n3) time.O(2n) This time complexity often indicates that the algorithm iterates throughall subsets of the input elements. For example, the subsets of {1,2,3} are ,{1}, {2}, {3}, {1,2}, {1,3}, {2,3} and {1,2,3}.O(n!) This time complexity often indicates that the algorithm iterates throughall permutations of the input elements. For example, the permutations of{1,2,3} are (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2) and (3,2,1).An algorithm is polynomial if its time complexity is at mostO(nk) where k isa constant. All the above time complexities exceptO(2n) and O(n!) are polynomial.In practice, the constant k is usually small, and therefore a polynomial timecomplexity roughly means that the algorithm is efcient.Most algorithms in this book are polynomial. Still, there are many importantproblems for which no polynomial algorithm is known, i.e., nobody knows how tosolve them efciently. NP-hard problems are an important set of problems, forwhich no polynomial algorithm is known1.1A classic book on the topic is M. R. Gareys and D. S. JohnsonsComputers and Intractability:A Guide to the Theory of NP-Completeness [28].20",
            "2.3 Estimating efciencyBy calculating the time complexity of an algorithm, it is possible to check, beforeimplementing the algorithm, that it is efcient enough for the problem. Thestarting point for estimations is the fact that a modern computer can performsome hundreds of millions of operations in a second.For example, assume that the time limit for a problem is one second and theinput size is n = 105. If the time complexity is O(n2), the algorithm will performabout (105)2 = 1010 operations. This should take at least some tens of seconds, sothe algorithm seems to be too slow for solving the problem.On the other hand, given the input size, we can try to guess the required timecomplexity of the algorithm that solves the problem. The following table containssome useful estimates assuming a time limit of one second.input size required time complexityn  10 O(n!)n  20 O(2n)n  500 O(n3)n  5000 O(n2)n  106 O(nlogn) or O(n)n is large O(1) or O(logn)For example, if the input size is n = 105, it is probably expected that thetime complexity of the algorithm is O(n) or O(nlogn). This information makes iteasier to design the algorithm, because it rules out approaches that would yieldan algorithm with a worse time complexity.Still, it is important to remember that a time complexity is only an estimateof efciency, because it hides the constant factors. For example, an algorithmthat runs in O(n) time may perform n/2 or 5n operations. This has an importanteffect on the actual running time of the algorithm.2.4 Maximum subarray sumThere are often several possible algorithms for solving a problem such that theirtime complexities are different. This section discusses a classic problem that hasa straightforward O(n3) solution. However, by designing a better algorithm, it ispossible to solve the problem in O(n2) time and even in O(n) time.Given an array of n numbers, our task is to calculate the maximum subar-ray sum, i.e., the largest possible sum of a sequence of consecutive values in thearray2. The problem is interesting when there may be negative values in thearray. For example, in the array1 2 4 3 5 2 5 22J. Bentleys bookProgramming Pearls [8] made the problem popular.21",
            "the following subarray produces the maximum sum 10:1 2 4 3 5 2 5 2We assume that an empty subarray is allowed, so the maximum subarraysum is always at least 0.Algorithm 1A straightforward way to solve the problem is to go through all possible subarrays,calculate the sum of values in each subarray and maintain the maximum sum.The following code implements this algorithm:int best = 0;for (int a = 0; a < n; a++) {for (int b = a; b < n; b++) {int sum = 0;for (int k = a; k <= b; k++) {sum += array[k];}best = max(best,sum);}}cout << best << \"\\n\";The variables a and b x the rst and last index of the subarray, and thesum of values is calculated to the variable sum. The variable best contains themaximum sum found during the search.The time complexity of the algorithm is O(n3), because it consists of threenested loops that go through the input.Algorithm 2It is easy to make Algorithm 1 more efcient by removing one loop from it. Thisis possible by calculating the sum at the same time when the right end of thesubarray moves. The result is the following code:int best = 0;for (int a = 0; a < n; a++) {int sum = 0;for (int b = a; b < n; b++) {sum += array[b];best = max(best,sum);}}cout << best << \"\\n\";After this change, the time complexity is O(n2).22",
            "Algorithm 3Surprisingly, it is possible to solve the problem in O(n) time3, which means thatjust one loop is enough. The idea is to calculate, for each array position, themaximum sum of a subarray that ends at that position. After this, the answerfor the problem is the maximum of those sums.Consider the subproblem of nding the maximum-sum subarray that ends atposition k. There are two possibilities:1. The subarray only contains the element at position k.2. The subarray consists of a subarray that ends at position k 1, followed bythe element at position k.In the latter case, since we want to nd a subarray with maximum sum, thesubarray that ends at position k 1 should also have the maximum sum. Thus,we can solve the problem efciently by calculating the maximum subarray sumfor each ending position from left to right.The following code implements the algorithm:int best = 0, sum = 0;for (int k = 0; k < n; k++) {sum = max(array[k],sum+array[k]);best = max(best,sum);}cout << best << \"\\n\";The algorithm only contains one loop that goes through the input, so the timecomplexity is O(n). This is also the best possible time complexity, because anyalgorithm for the problem has to examine all array elements at least once.Eciency comparisonIt is interesting to study how efcient algorithms are in practice. The followingtable shows the running times of the above algorithms for different values of non a modern computer.In each test, the input was generated randomly. The time needed for readingthe input was not measured.array size n Algorithm 1 Algorithm 2 Algorithm 3102 0.0 s 0 .0 s 0 .0 s103 0.1 s 0 .0 s 0 .0 s104 > 10.0 s 0 .1 s 0 .0 s105 > 10.0 s 5 .3 s 0 .0 s106 > 10.0 s > 10 .0 s 0 .0 s107 > 10.0 s > 10 .0 s 0 .0 s3In [8], this linear-time algorithm is attributed to J. B. Kadane, and the algorithm is sometimescalled Kadanes algorithm.23",
            "The comparison shows that all algorithms are efcient when the input size issmall, but larger inputs bring out remarkable differences in the running timesof the algorithms. Algorithm 1 becomes slow when n = 104, and Algorithm 2becomes slow when n = 105. Only Algorithm 3 is able to process even the largestinputs instantly.24",
            "Chapter 3SortingSorting is a fundamental algorithm design problem. Many efcient algorithmsuse sorting as a subroutine, because it is often easier to process data if theelements are in a sorted order.For example, the problem does an array contain two equal elements? is easyto solve using sorting. If the array contains two equal elements, they will be nextto each other after sorting, so it is easy to nd them. Also, the problem what isthe most frequent element in an array? can be solved similarly.There are many algorithms for sorting, and they are also good examples ofhow to apply different algorithm design techniques. The efcient general sortingalgorithms work in O(nlogn) time, and many algorithms that use sorting as asubroutine also have this time complexity.3.1 Sorting theoryThe basic problem in sorting is as follows:Given an array that contains n elements, your task is to sort the elements inincreasing order.For example, the array1 3 8 2 9 2 5 6will be as follows after sorting:1 2 2 3 5 6 8 9O(n2) algorithmsSimple algorithms for sorting an array work in O(n2) time. Such algorithmsare short and usually consist of two nested loops. A famous O(n2) time sorting25",
            "algorithm is bubble sort where the elements bubble in the array according totheir values.Bubble sort consists of n rounds. On each round, the algorithm iteratesthrough the elements of the array. Whenever two consecutive elements are foundthat are not in correct order, the algorithm swaps them. The algorithm can beimplemented as follows:for (int i = 0; i < n; i++) {for (int j = 0; j < n-1; j++) {if (array[j] > array[j+1]) {swap(array[j],array[j+1]);}}}After the rst round of the algorithm, the largest element will be in the correctposition, and in general, after k rounds, the k largest elements will be in thecorrect positions. Thus, after n rounds, the whole array will be sorted.For example, in the array1 3 8 2 9 2 5 6the rst round of bubble sort swaps elements as follows:1 3 2 8 9 2 5 61 3 2 8 2 9 5 61 3 2 8 2 5 9 61 3 2 8 2 5 6 9InversionsBubble sort is an example of a sorting algorithm that always swaps consecutiveelements in the array. It turns out that the time complexity of such an algorithmis always at least O(n2), because in the worst case, O(n2) swaps are required forsorting the array.A useful concept when analyzing sorting algorithms is an inversion: a pairof array elements (array[a],array[b]) such that a < b and array[a] > array[b], i.e.,the elements are in the wrong order. For example, the array26",
            "1 2 2 6 3 5 9 8has three inversions: (6,3), (6,5) and (9,8). The number of inversions indicateshow much work is needed to sort the array. An array is completely sorted whenthere are no inversions. On the other hand, if the array elements are in thereverse order, the number of inversions is the largest possible:1+2++ (n 1) = n(n 1)2 = O(n2)Swapping a pair of consecutive elements that are in the wrong order removesexactly one inversion from the array. Hence, if a sorting algorithm can only swapconsecutive elements, each swap removes at most one inversion, and the timecomplexity of the algorithm is at least O(n2).O(nlogn) algorithmsIt is possible to sort an array efciently in O(nlogn) time using algorithms thatare not limited to swapping consecutive elements. One such algorithm is mergesort1, which is based on recursion.Merge sort sorts a subarray array[a... b] as follows:1. If a = b, do not do anything, because the subarray is already sorted.2. Calculate the position of the middle element: k = (a +b)/2.3. Recursively sort the subarray array[a... k].4. Recursively sort the subarray array[k +1... b].5. Merge the sorted subarrays array[a... k] and array[k +1... b] into a sortedsubarray array[a... b].Merge sort is an efcient algorithm, because it halves the size of the subarrayat each step. The recursion consists of O(logn) levels, and processing each leveltakes O(n) time. Merging the subarrays array[a... k] and array[k +1... b] ispossible in linear time, because they are already sorted.For example, consider sorting the following array:1 3 6 2 8 2 5 9The array will be divided into two subarrays as follows:1 3 6 2 8 2 5 9Then, the subarrays will be sorted recursively as follows:1 2 3 6 2 5 8 91According to [47], merge sort was invented by J. von Neumann in 1945.27",
            "Finally, the algorithm merges the sorted subarrays and creates the nalsorted array:1 2 2 3 5 6 8 9Sorting lower boundIs it possible to sort an array faster than in O(nlogn) time? It turns out that thisis not possible when we restrict ourselves to sorting algorithms that are based oncomparing array elements.The lower bound for the time complexity can be proved by considering sortingas a process where each comparison of two elements gives more informationabout the contents of the array. The process creates the following tree:x < y?x < y? x < y?x < y? x < y? x < y? x < y?Here x < y? means that some elementsx and y are compared. If x < y, theprocess continues to the left, and otherwise to the right. The results of the processare the possible ways to sort the array, a total of n! ways. For this reason, theheight of the tree must be at leastlog2(n!) = log2(1)+log2(2)++ log2(n).We get a lower bound for this sum by choosing the lastn/2 elements and changingthe value of each element to log2(n/2). This yields an estimatelog2(n!)  (n/2)log2(n/2),so the height of the tree and the minimum possible number of steps in a sortingalgorithm in the worst case is at least nlogn.Counting sortThe lower bound nlogn does not apply to algorithms that do not compare arrayelements but use some other information. An example of such an algorithm iscounting sort that sorts an array in O(n) time assuming that every element inthe array is an integer between 0... c and c = O(n).The algorithm creates a bookkeeping array, whose indices are elements of theoriginal array. The algorithm iterates through the original array and calculateshow many times each element appears in the array.28",
            "For example, the array1 3 6 9 9 3 5 9corresponds to the following bookkeeping array:1 0 2 0 1 1 0 0 31 2 3 4 5 6 7 8 9For example, the value at position 3 in the bookkeeping array is 2, becausethe element 3 appears 2 times in the original array.Construction of the bookkeeping array takes O(n) time. After this, the sortedarray can be created in O(n) time because the number of occurrences of eachelement can be retrieved from the bookkeeping array. Thus, the total timecomplexity of counting sort is O(n).Counting sort is a very efcient algorithm but it can only be used when theconstant c is small enough, so that the array elements can be used as indices inthe bookkeeping array.3.2 Sorting in C++It is almost never a good idea to use a home-made sorting algorithm in a contest,because there are good implementations available in programming languages.For example, the C++ standard library contains the function sort that can beeasily used for sorting arrays and other data structures.There are many benets in using a library function. First, it saves timebecause there is no need to implement the function. Second, the library imple-mentation is certainly correct and efcient: it is not probable that a home-madesorting function would be better.In this section we will see how to use the C++ sort function. The followingcode sorts a vector in increasing order:vector<int> v = {4,2,5,3,5,8,3};sort(v.begin(),v.end());After the sorting, the contents of the vector will be [2,3,3,4,5,5,8]. The defaultsorting order is increasing, but a reverse order is possible as follows:sort(v.rbegin(),v.rend());An ordinary array can be sorted as follows:int n = 7; // array sizeint a[] = {4,2,5,3,5,8,3};sort(a,a+n);29",
            "The following code sorts the string s:string s = \"monkey\";sort(s.begin(), s.end());Sorting a string means that the characters of the string are sorted. For example,the string monkey becomes ekmnoy.Comparison operatorsThe function sort requires that a comparison operator is dened for the datatype of the elements to be sorted. When sorting, this operator will be usedwhenever it is necessary to nd out the order of two elements.Most C++ data types have a built-in comparison operator, and elementsof those types can be sorted automatically. For example, numbers are sortedaccording to their values and strings are sorted in alphabetical order.Pairs (pair) are sorted primarily according to their rst elements ( first).However, if the rst elements of two pairs are equal, they are sorted according totheir second elements (second):vector<pair<int,int>> v;v.push_back({1,5});v.push_back({2,3});v.push_back({1,2});sort(v.begin(), v.end());After this, the order of the pairs is (1,2), (1,5) and (2,3).In a similar way, tuples ( tuple) are sorted primarily by the rst element,secondarily by the second element, etc.2:vector<tuple<int,int,int>> v;v.push_back({2,1,4});v.push_back({1,5,3});v.push_back({2,1,3});sort(v.begin(), v.end());After this, the order of the tuples is (1,5,3), (2,1,3) and (2,1,4).User-dened structsUser-dened structs do not have a comparison operator automatically. Theoperator should be dened inside the struct as a function operator<, whoseparameter is another element of the same type. The operator should return trueif the element is smaller than the parameter, and false otherwise.For example, the following struct P contains the x and y coordinates of a point.The comparison operator is dened so that the points are sorted primarily by the2Note that in some older compilers, the function make_tuple has to be used to create a tupleinstead of braces (for example, make_tuple(2,1,4) instead of {2,1,4}).30",
            "x coordinate and secondarily by the y coordinate.struct P {int x, y;bool operator<(const P &p) {if (x != p.x) return x < p.x;else return y < p.y;}};Comparison functionsIt is also possible to give an external comparison function to the sort functionas a callback function. For example, the following comparison function comp sortsstrings primarily by length and secondarily by alphabetical order:bool comp(string a, string b) {if (a.size() != b.size()) return a.size() < b.size();return a < b;}Now a vector of strings can be sorted as follows:sort(v.begin(), v.end(), comp);3.3 Binary searchA general method for searching for an element in an array is to use a for loopthat iterates through the elements of the array. For example, the following codesearches for an element x in an array:for (int i = 0; i < n; i++) {if (array[i] == x) {// x found at index i}}The time complexity of this approach is O(n), because in the worst case, itis necessary to check all elements of the array. If the order of the elements isarbitrary, this is also the best possible approach, because there is no additionalinformation available where in the array we should search for the element x.However, if the array is sorted, the situation is different. In this case it ispossible to perform the search much faster, because the order of the elements inthe array guides the search. The following binary search algorithm efcientlysearches for an element in a sorted array in O(logn) time.31",
            "Method 1The usual way to implement binary search resembles looking for a word in adictionary. The search maintains an active region in the array, which initiallycontains all array elements. Then, a number of steps is performed, each of whichhalves the size of the region.At each step, the search checks the middle element of the active region. Ifthe middle element is the target element, the search terminates. Otherwise, thesearch recursively continues to the left or right half of the region, depending onthe value of the middle element.The above idea can be implemented as follows:int a = 0, b = n-1;while (a <= b) {int k = (a+b)/2;if (array[k] == x) {// x found at index k}if (array[k] > x) b = k-1;else a = k+1;}In this implementation, the active region is a... b, and initially the region is0... n 1. The algorithm halves the size of the region at each step, so the timecomplexity is O(logn).Method 2An alternative method to implement binary search is based on an efcient way toiterate through the elements of the array. The idea is to make jumps and slowthe speed when we get closer to the target element.The search goes through the array from left to right, and the initial jumplength is n/2. At each step, the jump length will be halved: rst n/4, then n/8,n/16, etc., until nally the length is 1. After the jumps, either the target elementhas been found or we know that it does not appear in the array.The following code implements the above idea:int k = 0;for (int b = n/2; b >= 1; b /= 2) {while (k+b < n && array[k+b] <= x) k += b;}if (array[k] == x) {// x found at index k}During the search, the variable b contains the current jump length. Thetime complexity of the algorithm is O(logn), because the code in the while loop isperformed at most twice for each jump length.32",
            "C++ functionsThe C++ standard library contains the following functions that are based onbinary search and work in logarithmic time: lower_bound returns a pointer to the rst array element whose value is atleast x. upper_bound returns a pointer to the rst array element whose value islarger than x. equal_range returns both above pointers.The functions assume that the array is sorted. If there is no such element,the pointer points to the element after the last array element. For example, thefollowing code nds out whether an array contains an element with value x:auto k = lower_bound(array,array+n,x)-array;if (k < n && array[k] == x) {// x found at index k}Then, the following code counts the number of elements whose value is x:auto a = lower_bound(array, array+n, x);auto b = upper_bound(array, array+n, x);cout << b-a << \"\\n\";Using equal_range, the code becomes shorter:auto r = equal_range(array, array+n, x);cout << r.second-r.first << \"\\n\";Finding the smallest solutionAn important use for binary search is to nd the position where the value of afunction changes. Suppose that we wish to nd the smallest value k that is avalid solution for a problem. We are given a function ok(x) that returns true if xis a valid solution and false otherwise. In addition, we know that ok(x) is falsewhen x < k and true when x  k. The situation looks as follows:x 0 1  k 1 k k +1 ok(x) false false  false true true Now, the value of k can be found using binary search:int x = -1;for (int b = z; b >= 1; b /= 2) {while (!ok(x+b)) x += b;}int k = x+1;33",
            "The search nds the largest value of x for which ok(x) is false. Thus, the nextvalue k = x +1 is the smallest possible value for which ok(k) is true. The initialjump length z has to be large enough, for example some value for which we knowbeforehand that ok(z) is true.The algorithm calls the function ok O(log z) times, so the total time complexitydepends on the function ok. For example, if the function works in O(n) time, thetotal time complexity is O(nlog z).Finding the maximum valueBinary search can also be used to nd the maximum value for a function that isrst increasing and then decreasing. Our task is to nd a position k such that f (x) < f (x +1) when x < k, and f (x) > f (x +1) when x  k.The idea is to use binary search for nding the largest value of x for whichf (x) < f (x+1). This implies that k = x+1 because f (x+1) > f (x+2). The followingcode implements the search:int x = -1;for (int b = z; b >= 1; b /= 2) {while (f(x+b) < f(x+b+1)) x += b;}int k = x+1;Note that unlike in the ordinary binary search, here it is not allowed thatconsecutive values of the function are equal. In this case it would not be possibleto know how to continue the search.34",
            "Chapter 4Data structuresA data structure is a way to store data in the memory of a computer. It isimportant to choose an appropriate data structure for a problem, because eachdata structure has its own advantages and disadvantages. The crucial questionis: which operations are efcient in the chosen data structure?This chapter introduces the most important data structures in the C++ stan-dard library. It is a good idea to use the standard library whenever possible,because it will save a lot of time. Later in the book we will learn about moresophisticated data structures that are not available in the standard library.4.1 Dynamic arraysA dynamic array is an array whose size can be changed during the execution ofthe program. The most popular dynamic array in C++ is the vector structure,which can be used almost like an ordinary array.The following code creates an empty vector and adds three elements to it:vector<int> v;v.push_back(3); // [3]v.push_back(2); // [3,2]v.push_back(5); // [3,2,5]After this, the elements can be accessed like in an ordinary array:cout << v[0] << \"\\n\"; // 3cout << v[1] << \"\\n\"; // 2cout << v[2] << \"\\n\"; // 5The function size returns the number of elements in the vector. The followingcode iterates through the vector and prints all elements in it:for (int i = 0; i < v.size(); i++) {cout << v[i] << \"\\n\";}35",
            "A shorter way to iterate through a vector is as follows:for (auto x : v) {cout << x << \"\\n\";}The function back returns the last element in the vector, and the functionpop_back removes the last element:vector<int> v;v.push_back(5);v.push_back(2);cout << v.back() << \"\\n\"; // 2v.pop_back();cout << v.back() << \"\\n\"; // 5The following code creates a vector with ve elements:vector<int> v = {2,4,2,5,1};Another way to create a vector is to give the number of elements and theinitial value for each element:// size 10, initial value 0vector<int> v(10);// size 10, initial value 5vector<int> v(10, 5);The internal implementation of a vector uses an ordinary array. If the size ofthe vector increases and the array becomes too small, a new array is allocatedand all the elements are moved to the new array. However, this does not happenoften and the average time complexity of push_back is O(1).The string structure is also a dynamic array that can be used almost likea vector. In addition, there is special syntax for strings that is not available inother data structures. Strings can be combined using the + symbol. The functionsubstr(k, x) returns the substring that begins at position k and has length x, andthe function find(t) nds the position of the rst occurrence of a substring t.The following code presents some string operations:string a = \"hatti\";string b = a+a;cout << b << \"\\n\"; // hattihattib[5] = v;cout << b << \"\\n\"; // hattivattistring c = b.substr(3,4);cout << c << \"\\n\"; // tiva36",
            "4.2 Set structuresA set is a data structure that maintains a collection of elements. The basicoperations of sets are element insertion, search and removal.The C++ standard library contains two set implementations: The structureset is based on a balanced binary tree and its operations work in O(logn) time.The structure unordered_set uses hashing, and its operations work in O(1) timeon average.The choice of which set implementation to use is often a matter of taste. Thebenet of the set structure is that it maintains the order of the elements andprovides functions that are not available in unordered_set. On the other hand,unordered_set can be more efcient.The following code creates a set that contains integers, and shows some of theoperations. The function insert adds an element to the set, the function countreturns the number of occurrences of an element in the set, and the functionerase removes an element from the set.set<int> s;s.insert(3);s.insert(2);s.insert(5);cout << s.count(3) << \"\\n\"; // 1cout << s.count(4) << \"\\n\"; // 0s.erase(3);s.insert(4);cout << s.count(3) << \"\\n\"; // 0cout << s.count(4) << \"\\n\"; // 1A set can be used mostly like a vector, but it is not possible to access theelements using the [] notation. The following code creates a set, prints thenumber of elements in it, and then iterates through all the elements:set<int> s = {2,5,6,8};cout << s.size() << \"\\n\"; // 4for (auto x : s) {cout << x << \"\\n\";}An important property of sets is that all their elements are distinct. Thus,the function count always returns either 0 (the element is not in the set) or 1 (theelement is in the set), and the function insert never adds an element to the set ifit is already there. The following code illustrates this:set<int> s;s.insert(5);s.insert(5);s.insert(5);cout << s.count(5) << \"\\n\"; // 137",
            "C++ also contains the structuresmultiset and unordered_multiset that other-wise work like set and unordered_set but they can contain multiple instances ofan element. For example, in the following code all three instances of the number5 are added to a multiset:multiset<int> s;s.insert(5);s.insert(5);s.insert(5);cout << s.count(5) << \"\\n\"; // 3The function erase removes all instances of an element from a multiset:s.erase(5);cout << s.count(5) << \"\\n\"; // 0Often, only one instance should be removed, which can be done as follows:s.erase(s.find(5));cout << s.count(5) << \"\\n\"; // 24.3 Map structuresA map is a generalized array that consists of key-value-pairs. While the keys inan ordinary array are always the consecutive integers 0,1,..., n 1, where n isthe size of the array, the keys in a map can be of any data type and they do nothave to be consecutive values.The C++ standard library contains two map implementations that correspondto the set implementations: the structure map is based on a balanced binary treeand accessing elements takes O(logn) time, while the structure unordered_mapuses hashing and accessing elements takes O(1) time on average.The following code creates a map where the keys are strings and the valuesare integers:map<string,int> m;m[\"monkey\"] = 4;m[\"banana\"] = 3;m[\"harpsichord\"] = 9;cout << m[\"banana\"] << \"\\n\"; // 3If the value of a key is requested but the map does not contain it, the keyis automatically added to the map with a default value. For example, in thefollowing code, the key aybabtu with value 0 is added to the map.map<string,int> m;cout << m[\"aybabtu\"] << \"\\n\"; // 038",
            "The function count checks if a key exists in a map:if (m.count(\"aybabtu\")) {// key exists}The following code prints all the keys and values in a map:for (auto x : m) {cout << x.first << \" \" << x.second << \"\\n\";}4.4 Iterators and rangesMany functions in the C++ standard library operate with iterators. An iteratoris a variable that points to an element in a data structure.The often used iterators begin and end dene a range that contains all ele-ments in a data structure. The iterator begin points to the rst element in thedata structure, and the iterator end points to the position after the last element.The situation looks as follows:{ 3, 4, 6, 8, 12, 13, 14, 17 } s.begin() s.end()Note the asymmetry in the iterators: s.begin() points to an element in thedata structure, while s.end() points outside the data structure. Thus, the rangedened by the iterators is half-open.Working with rangesIterators are used in C++ standard library functions that are given a range ofelements in a data structure. Usually, we want to process all elements in a datastructure, so the iterators begin and end are given for the function.For example, the following code sorts a vector using the function sort, thenreverses the order of the elements using the functionreverse, and nally shufesthe order of the elements using the function random_shuffle.sort(v.begin(), v.end());reverse(v.begin(), v.end());random_shuffle(v.begin(), v.end());These functions can also be used with an ordinary array. In this case, thefunctions are given pointers to the array instead of iterators:39",
            "sort(a, a+n);reverse(a, a+n);random_shuffle(a, a+n);Set iteratorsIterators are often used to access elements of a set. The following code creates aniterator it that points to the smallest element in a set:set<int>::iterator it = s.begin();A shorter way to write the code is as follows:auto it = s.begin();The element to which an iterator points can be accessed using the * symbol. Forexample, the following code prints the rst element in the set:auto it = s.begin();cout << *it << \"\\n\";Iterators can be moved using the operators ++ (forward) and -- (backward),meaning that the iterator moves to the next or previous element in the set.The following code prints all the elements in increasing order:for (auto it = s.begin(); it != s.end(); it++) {cout << *it << \"\\n\";}The following code prints the largest element in the set:auto it = s.end(); it--;cout << *it << \"\\n\";The function find(x) returns an iterator that points to an element whosevalue is x. However, if the set does not contain x, the iterator will be end.auto it = s.find(x);if (it == s.end()) {// x is not found}The function lower_bound(x) returns an iterator to the smallest element in theset whose value is at least x, and the function upper_bound(x) returns an iteratorto the smallest element in the set whose value is larger than x. In both functions,if such an element does not exist, the return value is end. These functions arenot supported by the unordered_set structure which does not maintain the orderof the elements.40",
            "For example, the following code nds the element nearest to x:auto it = s.lower_bound(x);if (it == s.begin()) {cout << *it << \"\\n\";} else if (it == s.end()) {it--;cout << *it << \"\\n\";} else {int a = *it; it--;int b = *it;if (x-b < a-x) cout << b << \"\\n\";else cout << a << \"\\n\";}The code assumes that the set is not empty, and goes through all possiblecases using an iterator it. First, the iterator points to the smallest elementwhose value is at least x. If it equals begin, the corresponding element is nearestto x. If it equals end, the largest element in the set is nearest to x. If noneof the previous cases hold, the element nearest to x is either the element thatcorresponds to it or the previous element.4.5 Other structuresBitsetA bitset is an array whose each value is either 0 or 1. For example, the followingcode creates a bitset that contains 10 elements:bitset<10> s;s[1] = 1;s[3] = 1;s[4] = 1;s[7] = 1;cout << s[4] << \"\\n\"; // 1cout << s[5] << \"\\n\"; // 0The benet of using bitsets is that they require less memory than ordinaryarrays, because each element in a bitset only uses one bit of memory. Forexample, if n bits are stored in an int array, 32n bits of memory will be used, buta corresponding bitset only requires n bits of memory. In addition, the values of abitset can be efciently manipulated using bit operators, which makes it possibleto optimize algorithms using bit sets.The following code shows another way to create the above bitset:bitset<10> s(string(\"0010011010\")); // from right to leftcout << s[4] << \"\\n\"; // 1cout << s[5] << \"\\n\"; // 041",
            "The function count returns the number of ones in the bitset:bitset<10> s(string(\"0010011010\"));cout << s.count() << \"\\n\"; // 4The following code shows examples of using bit operations:bitset<10> a(string(\"0010110110\"));bitset<10> b(string(\"1011011000\"));cout << (a&b) << \"\\n\"; // 0010010000cout << (a|b) << \"\\n\"; // 1011111110cout << (a^b) << \"\\n\"; // 1001101110DequeA deque is a dynamic array whose size can be efciently changed at both ends ofthe array. Like a vector, a deque provides the functionspush_back and pop_back,but it also includes the functions push_front and pop_front which are not avail-able in a vector.A deque can be used as follows:deque<int> d;d.push_back(5); // [5]d.push_back(2); // [5,2]d.push_front(3); // [3,5,2]d.pop_back(); // [3,5]d.pop_front(); // [5]The internal implementation of a deque is more complex than that of a vector,and for this reason, a deque is slower than a vector. Still, both adding andremoving elements take O(1) time on average at both ends.StackA stack is a data structure that provides two O(1) time operations: adding anelement to the top, and removing an element from the top. It is only possible toaccess the top element of a stack.The following code shows how a stack can be used:stack<int> s;s.push(3);s.push(2);s.push(5);cout << s.top(); // 5s.pop();cout << s.top(); // 242",
            "QueueA queue also provides two O(1) time operations: adding an element to the endof the queue, and removing the rst element in the queue. It is only possible toaccess the rst and last element of a queue.The following code shows how a queue can be used:queue<int> q;q.push(3);q.push(2);q.push(5);cout << q.front(); // 3q.pop();cout << q.front(); // 2Priority queueA priority queue maintains a set of elements. The supported operations areinsertion and, depending on the type of the queue, retrieval and removal of eitherthe minimum or maximum element. Insertion and removal take O(logn) time,and retrieval takes O(1) time.While an ordered set efciently supports all the operations of a priority queue,the benet of using a priority queue is that it has smaller constant factors. Apriority queue is usually implemented using a heap structure that is muchsimpler than a balanced binary tree used in an ordered set.By default, the elements in a C++ priority queue are sorted in decreasingorder, and it is possible to nd and remove the largest element in the queue. Thefollowing code illustrates this:priority_queue<int> q;q.push(3);q.push(5);q.push(7);q.push(2);cout << q.top() << \"\\n\"; // 7q.pop();cout << q.top() << \"\\n\"; // 5q.pop();q.push(6);cout << q.top() << \"\\n\"; // 6q.pop();If we want to create a priority queue that supports nding and removing thesmallest element, we can do it as follows:priority_queue<int,vector<int>,greater<int>> q;43",
            "Policy-based data structuresThe g++ compiler also supports some data structures that are not part of the C++standard library. Such structures are called policy-based data structures. To usethese structures, the following lines must be added to the code:#include <ext/pb_ds/assoc_container.hpp>using namespace __gnu_pbds;After this, we can dene a data structure indexed_set that is like set but can beindexed like an array. The denition for int values is as follows:typedef tree<int,null_type,less<int>,rb_tree_tag,tree_order_statistics_node_update> indexed_set;Now we can create a set as follows:indexed_set s;s.insert(2);s.insert(3);s.insert(7);s.insert(9);The speciality of this set is that we have access to the indices that the elementswould have in a sorted array. The function find_by_order returns an iterator tothe element at a given position:auto x = s.find_by_order(2);cout << *x << \"\\n\"; // 7And the function order_of_key returns the position of a given element:cout << s.order_of_key(7) << \"\\n\"; // 2If the element does not appear in the set, we get the position that the elementwould have in the set:cout << s.order_of_key(6) << \"\\n\"; // 2cout << s.order_of_key(8) << \"\\n\"; // 3Both the functions work in logarithmic time.4.6 Comparison to sortingIt is often possible to solve a problem using either data structures or sorting.Sometimes there are remarkable differences in the actual efciency of theseapproaches, which may be hidden in their time complexities.Let us consider a problem where we are given two lists A and B that bothcontain n elements. Our task is to calculate the number of elements that belong44",
            "to both of the lists. For example, for the listsA = [5,2,8,9] and B = [3,2,9,5],the answer is 3 because the numbers 2, 5 and 9 belong to both of the lists.A straightforward solution to the problem is to go through all pairs of elementsin O(n2) time, but next we will focus on more efcient algorithms.Algorithm 1We construct a set of the elements that appear in A, and after this, we iteratethrough the elements of B and check for each elements if it also belongs to A.This is efcient because the elements of A are in a set. Using the set structure,the time complexity of the algorithm is O(nlogn).Algorithm 2It is not necessary to maintain an ordered set, so instead of the set structurewe can also use the unordered_set structure. This is an easy way to make thealgorithm more efcient, because we only have to change the underlying datastructure. The time complexity of the new algorithm is O(n).Algorithm 3Instead of data structures, we can use sorting. First, we sort both lists A andB. After this, we iterate through both the lists at the same time and nd thecommon elements. The time complexity of sorting is O(nlogn), and the rest ofthe algorithm works in O(n) time, so the total time complexity is O(nlogn).Eciency comparisonThe following table shows how efcient the above algorithms are when n variesand the elements of the lists are random integers between 1... 109:n Algorithm 1 Algorithm 2 Algorithm 3106 1.5 s 0 .3 s 0 .2 s2106 3.7 s 0 .8 s 0 .3 s3106 5.7 s 1 .3 s 0 .5 s4106 7.7 s 1 .7 s 0 .7 s5106 10.0 s 2 .3 s 0 .9 sAlgorithms 1 and 2 are equal except that they use different set structures. Inthis problem, this choice has an important effect on the running time, becauseAlgorithm 2 is 45 times faster than Algorithm 1.However, the most efcient algorithm is Algorithm 3 which uses sorting.It only uses half the time compared to Algorithm 2. Interestingly, the timecomplexity of both Algorithm 1 and Algorithm 3 is O(nlogn), but despite this,Algorithm 3 is ten times faster. This can be explained by the fact that sorting is a45",
            "simple procedure and it is done only once at the beginning of Algorithm 3, andthe rest of the algorithm works in linear time. On the other hand, Algorithm 1maintains a complex balanced binary tree during the whole algorithm.46",
            "Chapter 5Complete searchComplete search is a general method that can be used to solve almost anyalgorithm problem. The idea is to generate all possible solutions to the problemusing brute force, and then select the best solution or count the number ofsolutions, depending on the problem.Complete search is a good technique if there is enough time to go throughall the solutions, because the search is usually easy to implement and it alwaysgives the correct answer. If complete search is too slow, other techniques, such asgreedy algorithms or dynamic programming, may be needed.5.1 Generating subsetsWe rst consider the problem of generating all subsets of a set of n elements. Forexample, the subsets of {0,1,2} are , {0}, {1}, {2}, {0,1}, {0,2}, {1,2} and {0,1,2}.There are two common methods to generate subsets: we can either perform arecursive search or exploit the bit representation of integers.Method 1An elegant way to go through all subsets of a set is to use recursion. Thefollowing function search generates the subsets of the set {0,1,..., n 1}. Thefunction maintains a vector subset that will contain the elements of each subset.The search begins when the function is called with parameter 0.void search(int k) {if (k == n) {// process subset} else {search(k+1);subset.push_back(k);search(k+1);subset.pop_back();}}47",
            "When the function search is called with parameter k, it decides whether toinclude the element k in the subset or not, and in both cases, then calls itselfwith parameter k +1 However, if k = n, the function notices that all elementshave been processed and a subset has been generated.The following tree illustrates the function calls when n = 3. We can alwayschoose either the left branch (k is not included in the subset) or the right branch(k is included in the subset).search(0)search(1) search(1)search(2) search(2) search(2) search(2)search(3) search(3) search(3) search(3) search(3) search(3) search(3) search(3) {2} {1} {1,2} {0} {0,2} {0,1} {0,1,2}Method 2Another way to generate subsets is based on the bit representation of integers.Each subset of a set of n elements can be represented as a sequence of n bits,which corresponds to an integer between 0... 2n 1. The ones in the bit sequenceindicate which elements are included in the subset.The usual convention is that the last bit corresponds to element 0, the secondlast bit corresponds to element 1, and so on. For example, the bit representationof 25 is 11001, which corresponds to the subset {0,3,4}.The following code goes through the subsets of a set of n elementsfor (int b = 0; b < (1<<n); b++) {// process subset}The following code shows how we can nd the elements of a subset thatcorresponds to a bit sequence. When processing each subset, the code builds avector that contains the elements in the subset.for (int b = 0; b < (1<<n); b++) {vector<int> subset;for (int i = 0; i < n; i++) {if (b&(1<<i)) subset.push_back(i);}}48",
            "5.2 Generating permutationsNext we consider the problem of generating all permutations of a set ofn elements.For example, the permutations of {0,1,2} are (0,1,2), (0,2,1), (1,0,2), (1,2,0),(2,0,1) and (2,1,0). Again, there are two approaches: we can either use recursionor go through the permutations iteratively.Method 1Like subsets, permutations can be generated using recursion. The followingfunction search goes through the permutations of the set {0,1,..., n 1}. Thefunction builds a vector permutation that contains the permutation, and thesearch begins when the function is called without parameters.void search() {if (permutation.size() == n) {// process permutation} else {for (int i = 0; i < n; i++) {if (chosen[i]) continue;chosen[i] = true;permutation.push_back(i);search();chosen[i] = false;permutation.pop_back();}}}Each function call adds a new element to permutation. The array chosenindicates which elements are already included in the permutation. If the size ofpermutation equals the size of the set, a permutation has been generated.Method 2Another method for generating permutations is to begin with the permutation{0,1,..., n 1} and repeatedly use a function that constructs the next permu-tation in increasing order. The C++ standard library contains the functionnext_permutation that can be used for this:vector<int> permutation;for (int i = 0; i < n; i++) {permutation.push_back(i);}do {// process permutation} while (next_permutation(permutation.begin(),permutation.end()));49",
            "5.3 BacktrackingA backtracking algorithm begins with an empty solution and extends thesolution step by step. The search recursively goes through all different ways howa solution can be constructed.As an example, consider the problem of calculating the number of ways nqueens can be placed on an n n chessboard so that no two queens attack eachother. For example, when n = 4, there are two possible solutions:QQQQQQQQThe problem can be solved using backtracking by placing queens to the boardrow by row. More precisely, exactly one queen will be placed on each row so thatno queen attacks any of the queens placed before. A solution has been foundwhen all n queens have been placed on the board.For example, when n = 4, some partial solutions generated by the backtrack-ing algorithm are as follows:Q Q Q QQ Q Q QQ Q Q Qillegal illegal illegal validAt the bottom level, the three rst congurations are illegal, because thequeens attack each other. However, the fourth conguration is valid and it can beextended to a complete solution by placing two more queens to the board. Thereis only one way to place the two remaining queens.The algorithm can be implemented as follows:50",
            "void search(int y) {if (y == n) {count++;return;}for (int x = 0; x < n; x++) {if (column[x] || diag1[x+y] || diag2[x-y+n-1]) continue;column[x] = diag1[x+y] = diag2[x-y+n-1] = 1;search(y+1);column[x] = diag1[x+y] = diag2[x-y+n-1] = 0;}}The search begins by calling search(0). The size of the board is n n, and thecode calculates the number of solutions to count.The code assumes that the rows and columns of the board are numbered from0 to n1. When the function search is called with parameter y, it places a queenon row y and then calls itself with parameter y+1. Then, if y = n, a solution hasbeen found and the variable count is increased by one.The array column keeps track of columns that contain a queen, and the arraysdiag1 and diag2 keep track of diagonals. It is not allowed to add another queento a column or diagonal that already contains a queen. For example, the columnsand diagonals of the 44 board are numbered as follows:0 1 2 30 1 2 30 1 2 30 1 2 30 1 2 31 2 3 42 3 4 53 4 5 63 4 5 62 3 4 51 2 3 40 1 2 3column diag1 diag2Let q(n) denote the number of ways to place n queens on an n n chessboard.The above backtracking algorithm tells us that, for example, q(8) = 92. Whenn increases, the search quickly becomes slow, because the number of solutionsincreases exponentially. For example, calculating q(16) = 14772512 using theabove algorithm already takes about a minute on a modern computer1.5.4 Pruning the searchWe can often optimize backtracking by pruning the search tree. The idea is toadd intelligence to the algorithm so that it will notice as soon as possible if apartial solution cannot be extended to a complete solution. Such optimizationscan have a tremendous effect on the efciency of the search.1There is no known way to efciently calculate larger values of q(n). The current record isq(27) = 234907967154122528, calculated in 2016 [55].51",
            "Let us consider the problem of calculating the number of paths in an n ngrid from the upper-left corner to the lower-right corner such that the path visitseach square exactly once. For example, in a 7 7 grid, there are 111712 suchpaths. One of the paths is as follows:We focus on the 7 7 case, because its level of difculty is appropriate toour needs. We begin with a straightforward backtracking algorithm, and thenoptimize it step by step using observations of how the search can be pruned.After each optimization, we measure the running time of the algorithm and thenumber of recursive calls, so that we clearly see the effect of each optimizationon the efciency of the search.Basic algorithmThe rst version of the algorithm does not contain any optimizations. We simplyuse backtracking to generate all possible paths from the upper-left corner to thelower-right corner and count the number of such paths. running time: 483 seconds number of recursive calls: 76 billionOptimization 1In any solution, we rst move one step down or right. There are always twopaths that are symmetric about the diagonal of the grid after the rst step. Forexample, the following paths are symmetric:Hence, we can decide that we always rst move one step down (or right), andnally multiply the number of solutions by two. running time: 244 seconds number of recursive calls: 38 billion52",
            "Optimization 2If the path reaches the lower-right square before it has visited all other squaresof the grid, it is clear that it will not be possible to complete the solution. Anexample of this is the following path:Using this observation, we can terminate the search immediately if we reach thelower-right square too early. running time: 119 seconds number of recursive calls: 20 billionOptimization 3If the path touches a wall and can turn either left or right, the grid splits intotwo parts that contain unvisited squares. For example, in the following situation,the path can turn either left or right:In this case, we cannot visit all squares anymore, so we can terminate the search.This optimization is very useful: running time: 1.8 seconds number of recursive calls: 221 millionOptimization 4The idea of Optimization 3 can be generalized: if the path cannot continueforward but can turn either left or right, the grid splits into two parts that bothcontain unvisited squares. For example, consider the following path:53",
            "It is clear that we cannot visit all squares anymore, so we can terminate thesearch. After this optimization, the search is very efcient: running time: 0.6 seconds number of recursive calls: 69 millionNow is a good moment to stop optimizing the algorithm and see what we haveachieved. The running time of the original algorithm was 483 seconds, and nowafter the optimizations, the running time is only 0.6 seconds. Thus, the algorithmbecame nearly 1000 times faster after the optimizations.This is a usual phenomenon in backtracking, because the search tree is usuallylarge and even simple observations can effectively prune the search. Especiallyuseful are optimizations that occur during the rst steps of the algorithm, i.e., atthe top of the search tree.5.5 Meet in the middleMeet in the middle is a technique where the search space is divided into twoparts of about equal size. A separate search is performed for both of the parts,and nally the results of the searches are combined.The technique can be used if there is an efcient way to combine the resultsof the searches. In such a situation, the two searches may require less time thanone large search. Typically, we can turn a factor of 2n into a factor of 2n/2 usingthe meet in the middle technique.As an example, consider a problem where we are given a list of n numbersand a number x, and we want to nd out if it is possible to choose some numbersfrom the list so that their sum is x. For example, given the list [2 ,4,5,9] andx = 15, we can choose the numbers [2,4,9] to get 2 +4+9 = 15. However, if x = 10for the same list, it is not possible to form the sum.A simple algorithm to the problem is to go through all subsets of the elementsand check if the sum of any of the subsets is x. The running time of such analgorithm is O(2n), because there are 2n subsets. However, using the meet in themiddle technique, we can achieve a more efcient O(2n/2) time algorithm2. Notethat O(2n) and O(2n/2) are different complexities because 2n/2 equals2n.2This idea was introduced in 1974 by E. Horowitz and S. Sahni [39].54",
            "The idea is to divide the list into two lists A and B such that both lists containabout half of the numbers. The rst search generates all subsets of A and storestheir sums to a list SA. Correspondingly, the second search creates a list SB fromB. After this, it sufces to check if it is possible to choose one element from SAand another element from SB such that their sum is x. This is possible exactlywhen there is a way to form the sum x using the numbers of the original list.For example, suppose that the list is [2 ,4,5,9] and x = 15. First, we dividethe list into A = [2,4] and B = [5,9]. After this, we create lists SA = [0,2,4,6]and SB = [0,5,9,14]. In this case, the sum x = 15 is possible to form, because SAcontains the sum 6, SB contains the sum 9, and 6+9 = 15. This corresponds tothe solution [2,4,9].We can implement the algorithm so that its time complexity is O(2n/2). First,we generate sorted lists SA and SB, which can be done in O(2n/2) time using amerge-like technique. After this, since the lists are sorted, we can check inO(2n/2)time if the sum x can be created from SA and SB.55",
            "56",
            "Chapter 6Greedy algorithmsA greedy algorithm constructs a solution to the problem by always making achoice that looks the best at the moment. A greedy algorithm never takes backits choices, but directly constructs the nal solution. For this reason, greedyalgorithms are usually very efcient.The difculty in designing greedy algorithms is to nd a greedy strategy thatalways produces an optimal solution to the problem. The locally optimal choicesin a greedy algorithm should also be globally optimal. It is often difcult to arguethat a greedy algorithm works.6.1 Coin problemAs a rst example, we consider a problem where we are given a set of coins andour task is to form a sum of money n using the coins. The values of the coins arecoins = {c1, c2,..., ck}, and each coin can be used as many times we want. Whatis the minimum number of coins needed?For example, if the coins are the euro coins (in cents){1,2,5,10,20,50,100,200}and n = 520, we need at least four coins. The optimal solution is to select coins200+200+100+20 whose sum is 520.Greedy algorithmA simple greedy algorithm to the problem always selects the largest possible coin,until the required sum of money has been constructed. This algorithm works inthe example case, because we rst select two 200 cent coins, then one 100 centcoin and nally one 20 cent coin. But does this algorithm always work?It turns out that if the coins are the euro coins, the greedy algorithm alwaysworks, i.e., it always produces a solution with the fewest possible number of coins.The correctness of the algorithm can be shown as follows:First, each coin 1, 5, 10, 50 and 100 appears at most once in an optimalsolution, because if the solution would contain two such coins, we could replace57",
            "them by one coin and obtain a better solution. For example, if the solution wouldcontain coins 5+5, we could replace them by coin 10.In the same way, coins 2 and 20 appear at most twice in an optimal solution,because we could replace coins 2 +2+2 by coins 5 +1 and coins 20 +20+20 bycoins 50 +10. Moreover, an optimal solution cannot contain coins 2 +2 +1 or20+20+10, because we could replace them by coins 5 and 50.Using these observations, we can show for each coin x that it is not possibleto optimally construct a sum x or any larger sum by only using coins that aresmaller than x. For example, if x = 100, the largest optimal sum using the smallercoins is 50+20+20+5+2+2 = 99. Thus, the greedy algorithm that always selectsthe largest coin produces the optimal solution.This example shows that it can be difcult to argue that a greedy algorithmworks, even if the algorithm itself is simple.General caseIn the general case, the coin set can contain any coins and the greedy algorithmdoes not necessarily produce an optimal solution.We can prove that a greedy algorithm does not work by showing a counterex-ample where the algorithm gives a wrong answer. In this problem we can easilynd a counterexample: if the coins are {1,3,4} and the target sum is 6, the greedyalgorithm produces the solution 4+1+1 while the optimal solution is 3+3.It is not known if the general coin problem can be solved using any greedyalgorithm1. However, as we will see in Chapter 7, in some cases, the generalproblem can be efciently solved using a dynamic programming algorithm thatalways gives the correct answer.6.2 SchedulingMany scheduling problems can be solved using greedy algorithms. A classicproblem is as follows: Given n events with their starting and ending times, nd aschedule that includes as many events as possible. It is not possible to select anevent partially. For example, consider the following events:event starting time ending timeA 1 3B 2 5C 3 9D 6 8In this case the maximum number of events is two. For example, we can selectevents B and D as follows:1However, it is possible to check in polynomial time if the greedy algorithm presented in thischapter works for a given set of coins [53].58",
            "ABCDIt is possible to invent several greedy algorithms for the problem, but whichof them works in every case?Algorithm 1The rst idea is to select as short events as possible. In the example case thisalgorithm selects the following events:ABCDHowever, selecting short events is not always a correct strategy. For example,the algorithm fails in the following case:If we select the short event, we can only select one event. However, it would bepossible to select both long events.Algorithm 2Another idea is to always select the next possible event that begins as early aspossible. This algorithm selects the following events:ABCDHowever, we can nd a counterexample also for this algorithm. For example,in the following case, the algorithm only selects one event:If we select the rst event, it is not possible to select any other events. However,it would be possible to select the other two events.59",
            "Algorithm 3The third idea is to always select the next possible event that ends as early aspossible. This algorithm selects the following events:ABCDIt turns out that this algorithm always produces an optimal solution. Thereason for this is that it is always an optimal choice to rst select an event thatends as early as possible. After this, it is an optimal choice to select the nextevent using the same strategy, etc., until we cannot select any more events.One way to argue that the algorithm works is to consider what happens if werst select an event that ends later than the event that ends as early as possible.Now, we will have at most an equal number of choices how we can select the nextevent. Hence, selecting an event that ends later can never yield a better solution,and the greedy algorithm is correct.6.3 Tasks and deadlinesLet us now consider a problem where we are given n tasks with durations anddeadlines and our task is to choose an order to perform the tasks. For each task,we earn d x points where d is the tasks deadline andx is the moment when wenish the task. What is the largest possible total score we can obtain?For example, suppose that the tasks are as follows:task duration deadlineA 4 2B 3 5C 2 7D 4 5In this case, an optimal schedule for the tasks is as follows:C B A D0 5 10In this solution, C yields 5 points, B yields 0 points, A yields 7 points and Dyields 8 points, so the total score is 10.Surprisingly, the optimal solution to the problem does not depend on thedeadlines at all, but a correct greedy strategy is to simply perform the taskssorted by their durations in increasing order. The reason for this is that if weever perform two tasks one after another such that the rst task takes longerthan the second task, we can obtain a better solution if we swap the tasks. Forexample, consider the following schedule:60",
            "X Ya bHere a > b, so we should swap the tasks:Y Xb aNow X gives b points less and Y gives a points more, so the total score increasesby a b > 0. In an optimal solution, for any two consecutive tasks, it must holdthat the shorter task comes before the longer task. Thus, the tasks must beperformed sorted by their durations.6.4 Minimizing sumsWe next consider a problem where we are given n numbers a1,a2,..., an and ourtask is to nd a value x that minimizes the sum|a1  x|c +|a2  x|c ++| an  x|c.We focus on the cases c = 1 and c = 2.Case c = 1In this case, we should minimize the sum|a1  x|+| a2  x|++| an  x|.For example, if the numbers are [1,2,9,2,6], the best solution is to select x = 2which produces the sum|12|+| 22|+| 92|+| 22|+| 62| =12.In the general case, the best choice for x is the median of the numbers, i.e., themiddle number after sorting. For example, the list [1,2,9,2,6] becomes [1,2,2,6,9]after sorting, so the median is 2.The median is an optimal choice, because if x is smaller than the median, thesum becomes smaller by increasing x, and if x is larger then the median, thesum becomes smaller by decreasing x. Hence, the optimal solution is that x isthe median. If n is even and there are two medians, both medians and all valuesbetween them are optimal choices.Case c = 2In this case, we should minimize the sum(a1  x)2 +(a2  x)2 ++ (an  x)2.61",
            "For example, if the numbers are [1,2,9,2,6], the best solution is to select x = 4which produces the sum(14)2 +(24)2 +(94)2 +(24)2 +(64)2 = 46.In the general case, the best choice for x is the average of the numbers. In theexample the average is (1 +2 +9 +2 +6)/5 = 4. This result can be derived bypresenting the sum as follows:nx2 2x(a1 +a2 ++ an)+(a21 +a22 ++ a2n)The last part does not depend on x, so we can ignore it. The remaining partsform a function nx2 2xs where s = a1 +a2 ++ an. This is a parabola openingupwards with roots x = 0 and x = 2s/n, and the minimum value is the average ofthe roots x = s/n, i.e., the average of the numbers a1,a2,..., an.6.5 Data compressionA binary code assigns for each character of a string a codeword that consistsof bits. We can compress the string using the binary code by replacing eachcharacter by the corresponding codeword. For example, the following binary codeassigns codewords for characters AD:character codewordA 00B 01C 10D 11This is a constant-length code which means that the length of each codeword isthe same. For example, we can compress the string AABACDACA as follows:000001001011001000Using this code, the length of the compressed string is 18 bits. However, we cancompress the string better if we use a variable-length code where codewordsmay have different lengths. Then we can give short codewords for charactersthat appear often and long codewords for characters that appear rarely. It turnsout that an optimal code for the above string is as follows:character codewordA 0B 110C 10D 111An optimal code produces a compressed string that is as short as possible. In thiscase, the compressed string using the optimal code is001100101110100 ,62",
            "so only 15 bits are needed instead of 18 bits. Thus, thanks to a better code it waspossible to save 3 bits in the compressed string.We require that no codeword is a prex of another codeword. For example,it is not allowed that a code would contain both codewords 10 and 1011. Thereason for this is that we want to be able to generate the original string fromthe compressed string. If a codeword could be a prex of another codeword, thiswould not always be possible. For example, the following code is not valid:character codewordA 10B 11C 1011D 111Using this code, it would not be possible to know if the compressed string 1011corresponds to the string AB or the string C.Human codingHuffman coding2 is a greedy algorithm that constructs an optimal code forcompressing a given string. The algorithm builds a binary tree based on thefrequencies of the characters in the string, and each characters codeword can beread by following a path from the root to the corresponding node. A move to theleft corresponds to bit 0, and a move to the right corresponds to bit 1.Initially, each character of the string is represented by a node whose weightis the number of times the character occurs in the string. Then at each step twonodes with minimum weights are combined by creating a new node whose weightis the sum of the weights of the original nodes. The process continues until allnodes have been combined.Next we will see how Huffman coding creates the optimal code for the stringAABACDACA. Initially, there are four nodes that correspond to the characters of thestring:5 1 2 1A B C DThe node that represents character A has weight 5 because character A appears 5times in the string. The other weights have been calculated in the same way.The rst step is to combine the nodes that correspond to characters B and D,both with weight 1. The result is:5 2 1 12A C B D0 12D. A. Huffman discovered this method when solving a university course assignment andpublished the algorithm in 1952 [40].63",
            "After this, the nodes with weight 2 are combined:521 124ACB D0 10 1Finally, the two remaining nodes are combined:521 1249ACB D0 10 10 1Now all nodes are in the tree, so the code is ready. The following codewordscan be read from the tree:character codewordA 0B 110C 10D 11164",
            "Chapter 7Dynamic programmingDynamic programming is a technique that combines the correctness of com-plete search and the efciency of greedy algorithms. Dynamic programming canbe applied if the problem can be divided into overlapping subproblems that canbe solved independently.There are two uses for dynamic programming: Finding an optimal solution: We want to nd a solution that is as largeas possible or as small as possible. Counting the number of solutions: We want to calculate the total num-ber of possible solutions.We will rst see how dynamic programming can be used to nd an optimalsolution, and then we will use the same idea for counting the solutions.Understanding dynamic programming is a milestone in every competitiveprogrammers career. While the basic idea is simple, the challenge is how to applydynamic programming to different problems. This chapter introduces a set ofclassic problems that are a good starting point.7.1 Coin problemWe rst focus on a problem that we have already seen in Chapter 6: Given a setof coin values coins = {c1, c2,..., ck} and a target sum of money n, our task is toform the sum n using as few coins as possible.In Chapter 6, we solved the problem using a greedy algorithm that alwayschooses the largest possible coin. The greedy algorithm works, for example, whenthe coins are the euro coins, but in the general case the greedy algorithm doesnot necessarily produce an optimal solution.Now is time to solve the problem efciently using dynamic programming, sothat the algorithm works for any coin set. The dynamic programming algorithmis based on a recursive function that goes through all possibilities how to formthe sum, like a brute force algorithm. However, the dynamic programmingalgorithm is efcient because it uses memoization and calculates the answer toeach subproblem only once.65",
            "Recursive formulationThe idea in dynamic programming is to formulate the problem recursively sothat the solution to the problem can be calculated from solutions to smallersubproblems. In the coin problem, a natural recursive problem is as follows: whatis the smallest number of coins required to form a sum x?Let solve(x) denote the minimum number of coins required for a sum x.The values of the function depend on the values of the coins. For example, ifcoins = {1,3,4}, the rst values of the function are as follows:solve(0) = 0solve(1) = 1solve(2) = 2solve(3) = 1solve(4) = 1solve(5) = 2solve(6) = 2solve(7) = 2solve(8) = 2solve(9) = 3solve(10) = 3For example, solve(10) = 3, because at least 3 coins are needed to form thesum 10. The optimal solution is 3 +3+4 = 10.The essential property of solve is that its values can be recursively calculatedfrom its smaller values. The idea is to focus on the rst coin that we choose forthe sum. For example, in the above scenario, the rst coin can be either 1, 3or 4. If we rst choose coin 1, the remaining task is to form the sum 9 usingthe minimum number of coins, which is a subproblem of the original problem.Of course, the same applies to coins 3 and 4. Thus, we can use the followingrecursive formula to calculate the minimum number of coins:solve(x) = min(solve(x 1)+1,solve(x 3)+1,solve(x 4)+1).The base case of the recursion is solve(0) = 0, because no coins are needed toform an empty sum. For example,solve(10) = solve(7)+1 = solve(4)+2 = solve(0)+3 = 3.Now we are ready to give a general recursive function that calculates theminimum number of coins needed to form a sum x:solve(x) = x < 00 x = 0minccoins solve(x  c)+1 x > 0First, if x < 0, the value is , because it is impossible to form a negative sumof money. Then, if x = 0, the value is 0, because no coins are needed to form an66",
            "empty sum. Finally, if x > 0, the variable c goes through all possibilities how tochoose the rst coin of the sum.Once a recursive function that solves the problem has been found, we candirectly implement a solution in C++ (the constant INF denotes innity):int solve(int x) {if (x < 0) return INF;if (x == 0) return 0;int best = INF;for (auto c : coins) {best = min(best, solve(x-c)+1);}return best;}Still, this function is not efcient, because there may be an exponentialnumber of ways to construct the sum. However, next we will see how to make thefunction efcient using a technique called memoization.Using memoizationThe idea of dynamic programming is to use memoization to efciently calculatevalues of a recursive function. This means that the values of the function arestored in an array after calculating them. For each parameter, the value of thefunction is calculated recursively only once, and after this, the value can bedirectly retrieved from the array.In this problem, we use arraysbool ready[N];int value[N];where ready[x] indicates whether the value of solve(x) has been calculated,and if it is, value[x] contains this value. The constant N has been chosen so thatall required values t in the arrays.Now the function can be efciently implemented as follows:int solve(int x) {if (x < 0) return INF;if (x == 0) return 0;if (ready[x]) return value[x];int best = INF;for (auto c : coins) {best = min(best, solve(x-c)+1);}value[x] = best;ready[x] = true;return best;}67",
            "The function handles the base cases x < 0 and x = 0 as previously. Then thefunction checks from ready[x] if solve(x) has already been stored in value[x], andif it is, the function directly returns it. Otherwise the function calculates thevalue of solve(x) recursively and stores it in value[x].This function works efciently, because the answer for each parameter x iscalculated recursively only once. After a value of solve(x) has been stored invalue[x], it can be efciently retrieved whenever the function will be called againwith the parameter x. The time complexity of the algorithm is O(nk), where n isthe target sum and k is the number of coins.Note that we can also iteratively construct the array value using a loop thatsimply calculates all the values of solve for parameters 0... n:value[0] = 0;for (int x = 1; x <= n; x++) {value[x] = INF;for (auto c : coins) {if (x-c >= 0) {value[x] = min(value[x], value[x-c]+1);}}}In fact, most competitive programmers prefer this implementation, becauseit is shorter and has lower constant factors. From now on, we also use iterativeimplementations in our examples. Still, it is often easier to think about dynamicprogramming solutions in terms of recursive functions.Constructing a solutionSometimes we are asked both to nd the value of an optimal solution and to givean example how such a solution can be constructed. In the coin problem, forexample, we can declare another array that indicates for each sum of money therst coin in an optimal solution:int first[N];Then, we can modify the algorithm as follows:value[0] = 0;for (int x = 1; x <= n; x++) {value[x] = INF;for (auto c : coins) {if (x-c >= 0 && value[x-c]+1 < value[x]) {value[x] = value[x-c]+1;first[x] = c;}}}68",
            "After this, the following code can be used to print the coins that appear in anoptimal solution for the sum n:while (n > 0) {cout << first[n] << \"\\n\";n -= first[n];}Counting the number of solutionsLet us now consider another version of the coin problem where our task is tocalculate the total number of ways to produce a sum x using the coins. Forexample, if coins = {1,3,4} and x = 5, there are a total of 6 ways: 1 +1+1+1+1 1 +1+3 1 +3+1 3 +1+1 1 +4 4 +1Again, we can solve the problem recursively. Let solve(x) denote the numberof ways we can form the sum x. For example, if coins = {1,3,4}, then solve(5) = 6and the recursive formula issolve(x) =solve(x 1)+solve(x 3)+solve(x 4).Then, the general recursive function is as follows:solve(x) =0 x < 01 x = 0ccoins solve(x  c) x > 0If x < 0, the value is 0, because there are no solutions. If x = 0, the value is 1,because there is only one way to form an empty sum. Otherwise we calculate thesum of all values of the form solve(x  c) where c is in coins.The following code constructs an array count such that count[x] equals thevalue of solve(x) for 0  x  n:count[0] = 1;for (int x = 1; x <= n; x++) {for (auto c : coins) {if (x-c >= 0) {count[x] += count[x-c];}}}69",
            "Often the number of solutions is so large that it is not required to calculate theexact number but it is enough to give the answer modulo m where, for example,m = 109 +7. This can be done by changing the code so that all calculations aredone modulo m. In the above code, it sufces to add the linecount[x] %= m;after the linecount[x] += count[x-c];Now we have discussed all basic ideas of dynamic programming. Sincedynamic programming can be used in many different situations, we will now gothrough a set of problems that show further examples about the possibilities ofdynamic programming.7.2 Longest increasing subsequenceOur rst problem is to nd the longest increasing subsequence in an arrayof n elements. This is a maximum-length sequence of array elements that goesfrom left to right, and each element in the sequence is larger than the previouselement. For example, in the array6 2 5 1 7 4 8 30 1 2 3 4 5 6 7the longest increasing subsequence contains 4 elements:6 2 5 1 7 4 8 30 1 2 3 4 5 6 7Let length(k) denote the length of the longest increasing subsequence thatends at position k. Thus, if we calculate all values oflength(k) where 0 k  n1,we will nd out the length of the longest increasing subsequence. For example,the values of the function for the above array are as follows:length(0) = 1length(1) = 1length(2) = 2length(3) = 1length(4) = 3length(5) = 2length(6) = 4length(7) = 2For example, length(6) = 4, because the longest increasing subsequence thatends at position 6 consists of 4 elements.70",
            "To calculate a value of length(k), we should nd a position i < k for whicharray[i] < array[k] and length(i) is as large as possible. Then we know thatlength(k) = length(i) +1, because this is an optimal way to add array[k] to asubsequence. However, if there is no such position i, then length(k) = 1, whichmeans that the subsequence only contains array[k].Since all values of the function can be calculated from its smaller values, wecan use dynamic programming. In the following code, the values of the functionwill be stored in an array length.for (int k = 0; k < n; k++) {length[k] = 1;for (int i = 0; i < k; i++) {if (array[i] < array[k]) {length[k] = max(length[k],length[i]+1);}}}This code works in O(n2) time, because it consists of two nested loops. How-ever, it is also possible to implement the dynamic programming calculation moreefciently in O(nlogn) time. Can you nd a way to do this?7.3 Paths in a gridOur next problem is to nd a path from the upper-left corner to the lower-rightcorner of an n  n grid, such that we only move down and right. Each squarecontains a positive integer, and the path should be constructed so that the sum ofthe values along the path is as large as possible.The following picture shows an optimal path in a grid:3 7 9 2 79 8 3 5 51 7 9 8 53 8 6 4 106 3 9 7 8The sum of the values on the path is 67, and this is the largest possible sum on apath from the upper-left corner to the lower-right corner.Assume that the rows and columns of the grid are numbered from 1 to n, andvalue[y][x] equals the value of square (y, x). Let sum(y, x) denote the maximumsum on a path from the upper-left corner to square ( y, x). Now sum(n,n) tellsus the maximum sum from the upper-left corner to the lower-right corner. Forexample, in the above grid, sum(5,5) = 67.We can recursively calculate the sums as follows:sum(y, x) = max(sum(y, x 1),sum(y1, x))+value[y][x]71",
            "The recursive formula is based on the observation that a path that ends atsquare (y, x) can come either from square (y, x 1) or square (y1, x):Thus, we select the direction that maximizes the sum. We assume thatsum(y, x) = 0 if y = 0 or x = 0 (because no such paths exist), so the recursiveformula also works when y = 1 or x = 1.Since the function sum has two parameters, the dynamic programming arrayalso has two dimensions. For example, we can use an arrayint sum[N][N];and calculate the sums as follows:for (int y = 1; y <= n; y++) {for (int x = 1; x <= n; x++) {sum[y][x] = max(sum[y][x-1],sum[y-1][x])+value[y][x];}}The time complexity of the algorithm is O(n2).7.4 Knapsack problemsThe term knapsack refers to problems where a set of objects is given, andsubsets with some properties have to be found. Knapsack problems can often besolved using dynamic programming.In this section, we focus on the following problem: Given a list of weights[w1,w2,..., wn], determine all sums that can be constructed using the weights.For example, if the weights are [1,3,3,5], the following sums are possible:0 1 2 3 4 5 6 7 8 9 10 11 12X X X X X X X X X X XIn this case, all sums between 0 ... 12 are possible, except 2 and 10. Forexample, the sum 7 is possible because we can select the weights [1,3,3].To solve the problem, we focus on subproblems where we only use the rst kweights to construct sums. Let possible(x,k) = true if we can construct a sum xusing the rst k weights, and otherwise possible(x,k) = false. The values of thefunction can be recursively calculated as follows:possible(x,k) = possible(x wk,k 1)possible(x,k 1)72",
            "The formula is based on the fact that we can either use or not use the weight wkin the sum. If we use wk, the remaining task is to form the sum x wk using therst k1 weights, and if we do not use wk, the remaining task is to form the sumx using the rst k 1 weights. As the base cases,possible(x,0) ={true x = 0false x = 0because if no weights are used, we can only form the sum 0.The following table shows all values of the function for the weights [1,3,3,5](the symbol X indicates the true values):k\\x 0 1 2 3 4 5 6 7 8 9 10 11 120 X1 X X2 X X X X3 X X X X X X4 X X X X X X X X X X XAfter calculating those values, possible(x,n) tells us whether we can con-struct a sum x using all weights.Let W denote the total sum of the weights. The followingO(nW) time dynamicprogramming solution corresponds to the recursive function:possible[0][0] = true;for (int k = 1; k <= n; k++) {for (int x = 0; x <= W; x++) {if (x-w[k] >= 0) possible[x][k] |= possible[x-w[k]][k-1];possible[x][k] |= possible[x][k-1];}}However, here is a better implementation that only uses a one-dimensionalarray possible[x] that indicates whether we can construct a subset with sum x.The trick is to update the array from right to left for each new weight:possible[0] = true;for (int k = 1; k <= n; k++) {for (int x = W; x >= 0; x--) {if (possible[x]) possible[x+w[k]] = true;}}Note that the general idea presented here can be used in many knapsackproblems. For example, if we are given objects with weights and values, we candetermine for each weight sum the maximum value sum of a subset.73",
            "7.5 Edit distanceThe edit distance or Levenshtein distance1 is the minimum number of edit-ing operations needed to transform a string into another string. The allowedediting operations are as follows: insert a character (e.g. ABC  ABCA) remove a character (e.g. ABC  AC) modify a character (e.g. ABC  ADC)For example, the edit distance between LOVE and MOVIE is 2, because we canrst perform the operation LOVE  MOVE (modify) and then the operation MOVE MOVIE (insert). This is the smallest possible number of operations, because it isclear that only one operation is not enough.Suppose that we are given a string x of length n and a string y of length m,and we want to calculate the edit distance between x and y. To solve the problem,we dene a function distance(a,b) that gives the edit distance between prexesx[0... a] and y[0... b]. Thus, using this function, the edit distance between x andy equals distance(n 1,m 1).We can calculate values of distance as follows:distance(a,b) = min(distance(a,b 1)+1,distance(a 1,b)+1,distance(a 1,b 1)+cost(a,b)).Here cost(a,b) = 0 if x[a] = y[b], and otherwise cost(a,b) = 1. The formulaconsiders the following ways to edit the string x: distance(a,b 1): insert a character at the end of x distance(a 1,b): remove the last character from x distance(a 1,b 1): match or modify the last character of xIn the two rst cases, one editing operation is needed (insert or remove). In thelast case, if x[a] = y[b], we can match the last characters without editing, andotherwise one editing operation is needed (modify).The following table shows the values of distance in the example case:LOVEM O V I E0123411234221233321244322554321The distance is named after V. I. Levenshtein who studied it in connection with binary codes[49].74",
            "The lower-right corner of the table tells us that the edit distance betweenLOVE and MOVIE is 2. The table also shows how to construct the shortest sequenceof editing operations. In this case the path is as follows:LOVEM O V I E012341123422123332124432255432The last characters of LOVE and MOVIE are equal, so the edit distance betweenthem equals the edit distance between LOV and MOVI. We can use one editingoperation to remove the character I from MOVI. Thus, the edit distance is onelarger than the edit distance between LOV and MOV, etc.7.6 Counting tilingsSometimes the states of a dynamic programming solution are more complexthan xed combinations of numbers. As an example, consider the problem ofcalculating the number of distinct ways to ll an n m grid using 12 and 21size tiles. For example, one valid solution for the 47 grid isand the total number of solutions is 781.The problem can be solved using dynamic programming by going throughthe grid row by row. Each row in a solution can be represented as a string thatcontains m characters from the set {,,,}. For example, the above solutionconsists of four rows that correspond to the following strings:       Let count(k, x) denote the number of ways to construct a solution for rows1... k of the grid such that string x corresponds to row k. It is possible to usedynamic programming here, because the state of a row is constrained only by thestate of the previous row.75",
            "A solution is valid if row 1 does not contain the character , row n does notcontain the character, and all consecutive rows arecompatible. For example, therows   and  are compatible, while the rows   and  are not compatible.Since a row consists of m characters and there are four choices for eachcharacter, the number of distinct rows is at most 4m. Thus, the time complexityof the solution is O(n42m) because we can go through the O(4m) possible statesfor each row, and for each state, there areO(4m) possible states for the previousrow. In practice, it is a good idea to rotate the grid so that the shorter side haslength m, because the factor 42m dominates the time complexity.It is possible to make the solution more efcient by using a more compactrepresentation for the rows. It turns out that it is sufcient to know whichcolumns of the previous row contain the upper square of a vertical tile. Thus, wecan represent a row using only characters  and , where is a combinationof characters , and . Using this representation, there are only 2m distinctrows and the time complexity is O(n22m).As a nal note, there is also a surprising direct formula for calculating thenumber of tilings2:n/2a=1m/2b=14(cos2 an +1 +cos2 bm +1)This formula is very efcient, because it calculates the number of tilings inO(nm)time, but since the answer is a product of real numbers, a problem when usingthe formula is how to store the intermediate results accurately.2Surprisingly, this formula was discovered in 1961 by two research teams [43, 67] that workedindependently.76",
            "Chapter 8Amortized analysisThe time complexity of an algorithm is often easy to analyze just by examiningthe structure of the algorithm: what loops does the algorithm contain and howmany times the loops are performed. However, sometimes a straightforwardanalysis does not give a true picture of the efciency of the algorithm.Amortized analysis can be used to analyze algorithms that contain opera-tions whose time complexity varies. The idea is to estimate the total time used toall such operations during the execution of the algorithm, instead of focusing onindividual operations.8.1 Two pointers methodIn the two pointers method, two pointers are used to iterate through the arrayvalues. Both pointers can move to one direction only, which ensures that thealgorithm works efciently. Next we discuss two problems that can be solvedusing the two pointers method.Subarray sumAs the rst example, consider a problem where we are given an array ofn positiveintegers and a target sum x, and we want to nd a subarray whose sum is x orreport that there is no such subarray.For example, the array1 3 2 5 1 1 2 3contains a subarray whose sum is 8:1 3 2 5 1 1 2 3This problem can be solved in O(n) time by using the two pointers method.The idea is to maintain pointers that point to the rst and last value of a subarray.On each turn, the left pointer moves one step to the right, and the right pointermoves to the right as long as the resulting subarray sum is at most x. If the sumbecomes exactly x, a solution has been found.77",
            "As an example, consider the following array and a target sum x = 8:1 3 2 5 1 1 2 3The initial subarray contains the values 1, 3 and 2 whose sum is 6:1 3 2 5 1 1 2 3Then, the left pointer moves one step to the right. The right pointer does notmove, because otherwise the subarray sum would exceed x.1 3 2 5 1 1 2 3Again, the left pointer moves one step to the right, and this time the rightpointer moves three steps to the right. The subarray sum is 2 +5 +1 = 8, so asubarray whose sum is x has been found.1 3 2 5 1 1 2 3The running time of the algorithm depends on the number of steps the rightpointer moves. While there is no useful upper bound on how many steps thepointer can move on a single turn. we know that the pointer moves a total ofO(n) steps during the algorithm, because it only moves to the right.Since both the left and right pointer move O(n) steps during the algorithm,the algorithm works in O(n) time.2SUM problemAnother problem that can be solved using the two pointers method is the followingproblem, also known as the 2SUM problem: given an array of n numbers and atarget sum x, nd two array values such that their sum is x, or report that nosuch values exist.To solve the problem, we rst sort the array values in increasing order. Afterthat, we iterate through the array using two pointers. The left pointer starts atthe rst value and moves one step to the right on each turn. The right pointerbegins at the last value and always moves to the left until the sum of the left andright value is at most x. If the sum is exactly x, a solution has been found.For example, consider the following array and a target sum x = 12:1 4 5 6 7 9 9 10The initial positions of the pointers are as follows. The sum of the values is1+10 = 11 that is smaller than x.78",
            "1 4 5 6 7 9 9 10Then the left pointer moves one step to the right. The right pointer movesthree steps to the left, and the sum becomes 4+7 = 11.1 4 5 6 7 9 9 10After this, the left pointer moves one step to the right again. The right pointerdoes not move, and a solution 5+7 = 12 has been found.1 4 5 6 7 9 9 10The running time of the algorithm is O(nlogn), because it rst sorts the arrayin O(nlogn) time, and then both pointers move O(n) steps.Note that it is possible to solve the problem in another way in O(nlogn) timeusing binary search. In such a solution, we iterate through the array and foreach array value, we try to nd another value that yields the sum x. This can bedone by performing n binary searches, each of which takes O(logn) time.A more difcult problem is the 3SUM problem that asks to nd three arrayvalues whose sum is x. Using the idea of the above algorithm, this problem canbe solved in O(n2) time1. Can you see how?8.2 Nearest smaller elementsAmortized analysis is often used to estimate the number of operations performedon a data structure. The operations may be distributed unevenly so that mostoperations occur during a certain phase of the algorithm, but the total number ofthe operations is limited.As an example, consider the problem of nding for each array element thenearest smaller element , i.e., the rst smaller element that precedes theelement in the array. It is possible that no such element exists, in which case thealgorithm should report this. Next we will see how the problem can be efcientlysolved using a stack structure.We go through the array from left to right and maintain a stack of arrayelements. At each array position, we remove elements from the stack until thetop element is smaller than the current element, or the stack is empty. Then, wereport that the top element is the nearest smaller element of the current element,or if the stack is empty, there is no such element. Finally, we add the currentelement to the stack.As an example, consider the following array:1For a long time, it was thought that solving the 3SUM problem more efciently than in O(n2)time would not be possible. However, in 2014, it turned out [30] that this is not the case.79",
            "1 3 4 2 5 3 4 2First, the elements 1, 3 and 4 are added to the stack, because each element islarger than the previous element. Thus, the nearest smaller element of 4 is 3,and the nearest smaller element of 3 is 1.1 3 4 2 5 3 4 21 3 4The next element 2 is smaller than the two top elements in the stack. Thus,the elements 3 and 4 are removed from the stack, and then the element 2 isadded to the stack. Its nearest smaller element is 1:1 3 4 2 5 3 4 21 2Then, the element 5 is larger than the element 2, so it will be added to thestack, and its nearest smaller element is 2:1 3 4 2 5 3 4 21 2 5After this, the element 5 is removed from the stack and the elements 3 and 4are added to the stack:1 3 4 2 5 3 4 21 2 3 4Finally, all elements except 1 are removed from the stack and the last element2 is added to the stack:1 3 4 2 5 3 4 21 2The efciency of the algorithm depends on the total number of stack opera-tions. If the current element is larger than the top element in the stack, it isdirectly added to the stack, which is efcient. However, sometimes the stack cancontain several larger elements and it takes time to remove them. Still, eachelement is added exactly once to the stack and removed at most once from thestack. Thus, each element causes O(1) stack operations, and the algorithm worksin O(n) time.80",
            "8.3 Sliding window minimumA sliding window is a constant-size subarray that moves from left to rightthrough the array. At each window position, we want to calculate some infor-mation about the elements inside the window. In this section, we focus on theproblem of maintaining the sliding window minimum, which means that weshould report the smallest value inside each window.The sliding window minimum can be calculated using a similar idea thatwe used to calculate the nearest smaller elements. We maintain a queue whereeach element is larger than the previous element, and the rst element alwayscorresponds to the minimum element inside the window. After each windowmove, we remove elements from the end of the queue until the last queue elementis smaller than the new window element, or the queue becomes empty. We alsoremove the rst queue element if it is not inside the window anymore. Finally,we add the new window element to the end of the queue.As an example, consider the following array:2 1 4 5 3 4 1 2Suppose that the size of the sliding window is 4. At the rst window position,the smallest value is 1:2 1 4 5 3 4 1 21 4 5Then the window moves one step right. The new element 3 is smaller thanthe elements 4 and 5 in the queue, so the elements 4 and 5 are removed from thequeue and the element 3 is added to the queue. The smallest value is still 1.2 1 4 5 3 4 1 21 3After this, the window moves again, and the smallest element 1 does notbelong to the window anymore. Thus, it is removed from the queue and thesmallest value is now 3. Also the new element 4 is added to the queue.2 1 4 5 3 4 1 23 4The next new element 1 is smaller than all elements in the queue. Thus, allelements are removed from the queue and it will only contain the element 1:2 1 4 5 3 4 1 2181",
            "Finally the window reaches its last position. The element 2 is added to thequeue, but the smallest value inside the window is still 1.2 1 4 5 3 4 1 21 2Since each array element is added to the queue exactly once and removedfrom the queue at most once, the algorithm works in O(n) time.82",
            "Chapter 9Range queriesIn this chapter, we discuss data structures that allow us to efciently processrange queries. In a range query, our task is to calculate a value based on asubarray of an array. Typical range queries are: sumq(a,b): calculate the sum of values in range [a,b] minq(a,b): nd the minimum value in range [ a,b] maxq(a,b): nd the maximum value in range [ a,b]For example, consider the range [3,6] in the following array:1 3 8 4 6 1 3 40 1 2 3 4 5 6 7In this case, sumq(3,6) = 14, minq(3,6) = 1 and maxq(3,6) = 6.A simple way to process range queries is to use a loop that goes through allarray values in the range. For example, the following function can be used toprocess sum queries on an array:int sum(int a, int b) {int s = 0;for (int i = a; i <= b; i++) {s += array[i];}return s;}This function works in O(n) time, where n is the size of the array. Thus, wecan process q queries in O(nq) time using the function. However, if both n and qare large, this approach is slow. Fortunately, it turns out that there are ways toprocess range queries much more efciently.83",
            "9.1 Static array queriesWe rst focus on a situation where the array is static, i.e., the array values arenever updated between the queries. In this case, it sufces to construct a staticdata structure that tells us the answer for any possible query.Sum queriesWe can easily process sum queries on a static array by constructing a prexsum array. Each value in the prex sum array equals the sum of values in theoriginal array up to that position, i.e., the value at position k is sumq(0,k). Theprex sum array can be constructed in O(n) time.For example, consider the following array:1 3 4 8 6 1 4 20 1 2 3 4 5 6 7The corresponding prex sum array is as follows:1 4 8 16 22 23 27 290 1 2 3 4 5 6 7Since the prex sum array contains all values of sumq(0,k), we can calculate anyvalue of sumq(a,b) in O(1) time as follows:sumq(a,b) = sumq(0,b)sumq(0,a 1)By dening sumq(0,1) = 0, the above formula also holds when a = 0.For example, consider the range [3,6]:1 3 4 8 6 1 4 20 1 2 3 4 5 6 7In this case sumq(3,6) = 8+6+1+4 = 19. This sum can be calculated from twovalues of the prex sum array:1 4 8 16 22 23 27 290 1 2 3 4 5 6 7Thus, sumq(3,6) = sumq(0,6)sumq(0,2) = 278 = 19.It is also possible to generalize this idea to higher dimensions. For example,we can construct a two-dimensional prex sum array that can be used to calculatethe sum of any rectangular subarray in O(1) time. Each sum in such an arraycorresponds to a subarray that begins at the upper-left corner of the array.84",
            "The following picture illustrates the idea:ABCDThe sum of the gray subarray can be calculated using the formulaS(A)S(B)S(C)+S(D),where S(X) denotes the sum of values in a rectangular subarray from the upper-left corner to the position of X.Minimum queriesMinimum queries are more difcult to process than sum queries. Still, there isa quite simple O(nlogn) time preprocessing method after which we can answerany minimum query in O(1) time1. Note that since minimum and maximumqueries can be processed similarly, we can focus on minimum queries.The idea is to precalculate all values of minq(a,b) where b a +1 (the lengthof the range) is a power of two. For example, for the array1 3 4 8 6 1 4 20 1 2 3 4 5 6 7the following values are calculated:a b minq(a,b)0 0 11 1 32 2 43 3 84 4 65 5 16 6 47 7 2a b minq(a,b)0 1 11 2 32 3 43 4 64 5 15 6 16 7 2a b minq(a,b)0 3 11 4 32 5 13 6 14 7 10 7 1The number of precalculated values is O(nlogn), because there are O(logn)range lengths that are powers of two. The values can be calculated efcientlyusing the recursive formulaminq(a,b) = min(minq(a,a +w 1),minq(a +w,b)),1This technique was introduced in [7] and sometimes called the sparse table method. Thereare also more sophisticated techniques [22] where the preprocessing time is only O(n), but suchalgorithms are not needed in competitive programming.85",
            "where ba+1 is a power of two and w = (ba+1)/2. Calculating all those valuestakes O(nlogn) time.After this, any value ofminq(a,b) can be calculated inO(1) time as a minimumof two precalculated values. Let k be the largest power of two that does not exceedb a +1. We can calculate the value of minq(a,b) using the formulaminq(a,b) = min(minq(a,a +k 1),minq(b k +1,b)).In the above formula, the range [a,b] is represented as the union of the ranges[a,a +k 1] and [b k +1,b], both of length k.As an example, consider the range [1,6]:1 3 4 8 6 1 4 20 1 2 3 4 5 6 7The length of the range is 6, and the largest power of two that does not exceed 6is 4. Thus the range [1,6] is the union of the ranges [1,4] and [3,6]:1 3 4 8 6 1 4 20 1 2 3 4 5 6 71 3 4 8 6 1 4 20 1 2 3 4 5 6 7Since minq(1,4) = 3 and minq(3,6) = 1, we conclude that minq(1,6) = 1.9.2 Binary indexed treeA binary indexed tree or a Fenwick tree2 can be seen as a dynamic variantof a prex sum array. It supports two O(logn) time operations on an array:processing a range sum query and updating a value.The advantage of a binary indexed tree is that it allows us to efciently updatearray values between sum queries. This would not be possible using a prex sumarray, because after each update, it would be necessary to build the whole prexsum array again in O(n) time.StructureEven if the name of the structure is a binary indexedtree, it is usually representedas an array. In this section we assume that all arrays are one-indexed, because itmakes the implementation easier.Let p(k) denote the largest power of two that divides k. We store a binaryindexed tree as an array tree such thattree[k] = sumq(k  p(k)+1,k),2The binary indexed tree structure was presented by P. M. Fenwick in 1994 [21].86",
            "i.e., each position k contains the sum of values in a range of the original arraywhose length is p(k) and that ends at position k. For example, since p(6) = 2,tree[6] contains the value of sumq(5,6).For example, consider the following array:1 3 4 8 6 1 4 21 2 3 4 5 6 7 8The corresponding binary indexed tree is as follows:1 4 4 16 6 7 4 291 2 3 4 5 6 7 8The following picture shows more clearly how each value in the binary indexedtree corresponds to a range in the original array:1 4 4 16 6 7 4 291 2 3 4 5 6 7 8Using a binary indexed tree, any value of sumq(1,k) can be calculated inO(logn) time, because a range [1,k] can always be divided into O(logn) rangeswhose sums are stored in the tree.For example, the range [1,7] consists of the following ranges:1 4 4 16 6 7 4 291 2 3 4 5 6 7 8Thus, we can calculate the corresponding sum as follows:sumq(1,7) = sumq(1,4)+sumq(5,6)+sumq(7,7) = 16+7+4 = 27To calculate the value of sumq(a,b) where a > 1, we can use the same trickthat we used with prex sum arrays:sumq(a,b) = sumq(1,b)sumq(1,a 1).87",
            "Since we can calculate both sumq(1,b) and sumq(1,a1) in O(logn) time, the totaltime complexity is O(logn).Then, after updating a value in the original array, several values in the binaryindexed tree should be updated. For example, if the value at position 3 changes,the sums of the following ranges change:1 4 4 16 6 7 4 291 2 3 4 5 6 7 8Since each array element belongs to O(logn) ranges in the binary indexedtree, it sufces to update O(logn) values in the tree.ImplementationThe operations of a binary indexed tree can be efciently implemented using bitoperations. The key fact needed is that we can calculate any value of p(k) usingthe formulap(k) = k&k.The following function calculates the value of sumq(1,k):int sum(int k) {int s = 0;while (k >= 1) {s += tree[k];k -= k&-k;}return s;}The following function increases the array value at position k by x (x can bepositive or negative):void add(int k, int x) {while (k <= n) {tree[k] += x;k += k&-k;}}The time complexity of both the functions is O(logn), because the functionsaccess O(logn) values in the binary indexed tree, and each move to the nextposition takes O(1) time.88",
            "9.3 Segment treeA segment tree3 is a data structure that supports two operations: processinga range query and updating an array value. Segment trees can support sumqueries, minimum and maximum queries and many other queries so that bothoperations work in O(logn) time.Compared to a binary indexed tree, the advantage of a segment tree is that itis a more general data structure. While binary indexed trees only support sumqueries4, segment trees also support other queries. On the other hand, a segmenttree requires more memory and is a bit more difcult to implement.StructureA segment tree is a binary tree such that the nodes on the bottom level of thetree correspond to the array elements, and the other nodes contain informationneeded for processing range queries.In this section, we assume that the size of the array is a power of two andzero-based indexing is used, because it is convenient to build a segment tree forsuch an array. If the size of the array is not a power of two, we can always appendextra elements to it.We will rst discuss segment trees that support sum queries. As an example,consider the following array:5 8 6 3 2 7 2 60 1 2 3 4 5 6 7The corresponding segment tree is as follows:5 8 6 3 2 7 2 613 9 9 822 1739Each internal tree node corresponds to an array range whose size is a powerof two. In the above tree, the value of each internal node is the sum of thecorresponding array values, and it can be calculated as the sum of the values ofits left and right child node.3The bottom-up-implementation in this chapter corresponds to that in [62]. Similar structureswere used in late 1970s to solve geometric problems [9].4In fact, using two binary indexed trees it is possible to support minimum queries [16], butthis is more complicated than to use a segment tree.89",
            "It turns out that any range [a,b] can be divided into O(logn) ranges whosevalues are stored in tree nodes. For example, consider the range [2,7]:5 8 6 3 2 7 2 60 1 2 3 4 5 6 7Here sumq(2,7) = 6+3+2+7+2+6 = 26. In this case, the following two tree nodescorrespond to the range:5 8 6 3 2 7 2 613 9 9 822 1739Thus, another way to calculate the sum is 9+17 = 26.When the sum is calculated using nodes located as high as possible in thetree, at most two nodes on each level of the tree are needed. Hence, the totalnumber of nodes is O(logn).After an array update, we should update all nodes whose value depends onthe updated value. This can be done by traversing the path from the updatedarray element to the top node and updating the nodes along the path.The following picture shows which tree nodes change if the array value 7changes:5 8 6 3 2 7 2 613 9 9 822 1739The path from bottom to top always consists of O(logn) nodes, so each updatechanges O(logn) nodes in the tree.ImplementationWe store a segment tree as an array of 2 n elements where n is the size of theoriginal array and a power of two. The tree nodes are stored from top to bottom:90",
            "tree[1] is the top node, tree[2] and tree[3] are its children, and so on. Finally,the values from tree[n] to tree[2n 1] correspond to the values of the originalarray on the bottom level of the tree.For example, the segment tree5 8 6 3 2 7 2 613 9 9 822 1739is stored as follows:39 22 17 13 9 9 8 5 8 6 3 2 7 2 61 2 3 4 5 6 7 8 9 10 11 12 13 14 15Using this representation, the parent of tree[k] is tree[k/2], and its childrenare tree[2k] and tree[2k +1]. Note that this implies that the position of a nodeis even if it is a left child and odd if it is a right child.The following function calculates the value of sumq(a,b):int sum(int a, int b) {a += n; b += n;int s = 0;while (a <= b) {if (a%2 == 1) s += tree[a++];if (b%2 == 0) s += tree[b--];a /= 2; b /= 2;}return s;}The function maintains a range that is initially [a +n,b +n]. Then, at each step,the range is moved one level higher in the tree, and before that, the values of thenodes that do not belong to the higher range are added to the sum.The following function increases the array value at position k by x:void add(int k, int x) {k += n;tree[k] += x;for (k /= 2; k >= 1; k /= 2) {tree[k] = tree[2*k]+tree[2*k+1];}}91",
            "First the function updates the value at the bottom level of the tree. After this,the function updates the values of all internal tree nodes, until it reaches the topnode of the tree.Both the above functions work in O(logn) time, because a segment tree of nelements consists of O(logn) levels, and the functions move one level higher inthe tree at each step.Other queriesSegment trees can support all range queries where it is possible to divide a rangeinto two parts, calculate the answer separately for both parts and then efcientlycombine the answers. Examples of such queries are minimum and maximum,greatest common divisor, and bit operations and, or and xor.For example, the following segment tree supports minimum queries:5 8 6 3 1 7 2 65 3 1 23 11In this case, every tree node contains the smallest value in the correspondingarray range. The top node of the tree contains the smallest value in the wholearray. The operations can be implemented like previously, but instead of sums,minima are calculated.The structure of a segment tree also allows us to use binary search for locatingarray elements. For example, if the tree supports minimum queries, we can ndthe position of an element with the smallest value in O(logn) time.For example, in the above tree, an element with the smallest value 1 can befound by traversing a path downwards from the top node:5 8 6 3 1 7 2 65 3 1 23 1192",
            "9.4 Additional techniquesIndex compressionA limitation in data structures that are built upon an array is that the elementsare indexed using consecutive integers. Difculties arise when large indices areneeded. For example, if we wish to use the index 109, the array should contain109 elements which would require too much memory.However, we can often bypass this limitation by using index compression,where the original indices are replaced with indices 1,2,3, etc. This can be doneif we know all the indices needed during the algorithm beforehand.The idea is to replace each original indexx with c(x) where c is a function thatcompresses the indices. We require that the order of the indices does not change,so if a < b, then c(a) < c(b). This allows us to conveniently perform queries evenif the indices are compressed.For example, if the original indices are 555, 109 and 8, the new indices are:c(8) = 1c(555) = 2c(109) = 3Range updatesSo far, we have implemented data structures that support range queries andupdates of single values. Let us now consider an opposite situation, where weshould update ranges and retrieve single values. We focus on an operation thatincreases all elements in a range [a,b] by x.Surprisingly, we can use the data structures presented in this chapter also inthis situation. To do this, we build adifference array whose values indicate thedifferences between consecutive values in the original array. Thus, the originalarray is the prex sum array of the difference array. For example, consider thefollowing array:3 3 1 1 1 5 2 20 1 2 3 4 5 6 7The difference array for the above array is as follows:3 0 2 0 0 4 3 00 1 2 3 4 5 6 7For example, the value 2 at position 6 in the original array corresponds to thesum 32+43 = 2 in the difference array.The advantage of the difference array is that we can update a range in theoriginal array by changing just two elements in the difference array. For example,if we want to increase the original array values between positions 1 and 4 by 5, itsufces to increase the difference array value at position 1 by 5 and decrease thevalue at position 5 by 5. The result is as follows:93",
            "3 5 2 0 0 1 3 00 1 2 3 4 5 6 7More generally, to increase the values in range [ a,b] by x, we increase thevalue at position a by x and decrease the value at position b +1 by x. Thus, it isonly needed to update single values and process sum queries, so we can use abinary indexed tree or a segment tree.A more difcult problem is to support both range queries and range updates.In Chapter 28 we will see that even this is possible.94",
            "Chapter 10Bit manipulationAll data in computer programs is internally stored as bits, i.e., as numbers 0and 1. This chapter discusses the bit representation of integers, and showsexamples of how to use bit operations. It turns out that there are many uses forbit manipulation in algorithm programming.10.1 Bit representationIn programming, an n bit integer is internally stored as a binary number thatconsists of n bits. For example, the C++ type int is a 32-bit type, which meansthat every int number consists of 32 bits.Here is the bit representation of the int number 43:00000000000000000000000000101011The bits in the representation are indexed from right to left. To convert a bitrepresentation bk  b2b1b0 into a number, we can use the formulabk2k +... +b222 +b121 +b020.For example,125 +123 +121 +120 = 43.The bit representation of a number is either signed or unsigned. Usuallya signed representation is used, which means that both negative and positivenumbers can be represented. A signed variable of n bits can contain any integerbetween 2n1 and 2n1 1. For example, the int type in C++ is a signed type,so an int variable can contain any integer between 231 and 231 1.The rst bit in a signed representation is the sign of the number (0 fornonnegative numbers and 1 for negative numbers), and the remaining n 1 bitscontain the magnitude of the number. Twos complementis used, which meansthat the opposite number of a number is calculated by rst inverting all the bitsin the number, and then increasing the number by one.For example, the bit representation of the int number 43 is11111111111111111111111111010101.95",
            "In an unsigned representation, only nonnegative numbers can be used, butthe upper bound for the values is larger. An unsigned variable of n bits cancontain any integer between 0 and 2n 1. For example, in C++, an unsigned intvariable can contain any integer between 0 and 232 1.There is a connection between the representations: a signed number xequals an unsigned number 2n x. For example, the following code shows thatthe signed number x = 43 equals the unsigned number y = 232 43:int x = -43;unsigned int y = x;cout << x << \"\\n\"; // -43cout << y << \"\\n\"; // 4294967253If a number is larger than the upper bound of the bit representation, thenumber will overow. In a signed representation, the next number after 2n1 1is 2n1, and in an unsigned representation, the next number after 2n 1 is 0.For example, consider the following code:int x = 2147483647cout << x << \"\\n\"; // 2147483647x++;cout << x << \"\\n\"; // -2147483648Initially, the value of x is 231 1. This is the largest value that can be storedin an int variable, so the next number after 231 1 is 231.10.2 Bit operationsAnd operationThe and operation x & y produces a number that has one bits in positions whereboth x and y have one bits. For example, 22 & 26 = 18, because10110 (22)& 11010 (26)= 10010 (18)Using the and operation, we can check if a number x is even because x & 1 =0 if x is even, and x & 1 = 1 if x is odd. More generally, x is divisible by 2k exactlywhen x & (2k 1) = 0.Or operationThe or operation x | y produces a number that has one bits in positions where atleast one of x and y have one bits. For example, 22 | 26 = 30, because10110 (22)| 11010 (26)= 11110 (30)96",
            "Xor operationThe xor operation x ^ y produces a number that has one bits in positions whereexactly one of x and y have one bits. For example, 22 ^ 26 = 12, because10110 (22)^ 11010 (26)= 01100 (12)Not operationThe not operation ~ x produces a number where all the bits of x have beeninverted. The formula ~x = x 1 holds, for example, ~29 = 30.The result of the not operation at the bit level depends on the length of thebit representation, because the operation inverts all bits. For example, if thenumbers are 32-bit int numbers, the result is as follows:x = 29 00000000000000000000000000011101~x = 30 11111111111111111111111111100010Bit shiftsThe left bit shift x << k appends k zero bits to the number, and the right bitshift x >> k removes the k last bits from the number. For example, 14<< 2 = 56,because 14 and 56 correspond to 1110 and 111000. Similarly, 49>> 3 = 6, because49 and 6 correspond to 110001 and 110.Note that x << k corresponds to multiplying x by 2k, and x >> k correspondsto dividing x by 2k rounded down to an integer.ApplicationsA number of the form 1<< k has a one bit in position k and all other bits are zero,so we can use such numbers to access single bits of numbers. In particular, thekth bit of a number is one exactly when x & (1 << k) is not zero. The followingcode prints the bit representation of an int number x:for (int i = 31; i >= 0; i--) {if (x&(1<<i)) cout << \"1\";else cout << \"0\";}It is also possible to modify single bits of numbers using similar ideas. Forexample, the formula x | (1 << k) sets the kth bit of x to one, the formula x &~(1 << k) sets the kth bit of x to zero, and the formula x ^ (1 << k) inverts thekth bit of x.The formula x & (x 1) sets the last one bit of x to zero, and the formula x &x sets all the one bits to zero, except for the last one bit. The formula x | (x 1)inverts all the bits after the last one bit. Also note that a positive number x is apower of two exactly when x & (x 1) = 0.97",
            "Additional functionsThe g++ compiler provides the following functions for counting bits: __builtin_clz(x): the number of zeros at the beginning of the number __builtin_ctz(x): the number of zeros at the end of the number __builtin_popcount(x): the number of ones in the number __builtin_parity(x): the parity (even or odd) of the number of onesThe functions can be used as follows:int x = 5328; // 00000000000000000001010011010000cout << __builtin_clz(x) << \"\\n\"; // 19cout << __builtin_ctz(x) << \"\\n\"; // 4cout << __builtin_popcount(x) << \"\\n\"; // 5cout << __builtin_parity(x) << \"\\n\"; // 1While the above functions only support int numbers, there are also long longversions of the functions available with the sufx ll.10.3 Representing setsEvery subset of a set{0,1,2,..., n1} can be represented as an n bit integer whoseone bits indicate which elements belong to the subset. This is an efcient way torepresent sets, because every element requires only one bit of memory, and setoperations can be implemented as bit operations.For example, since int is a 32-bit type, an int number can represent anysubset of the set {0,1,2,..., 31}. The bit representation of the set {1,3,4,8} is00000000000000000000000100011010,which corresponds to the number 28 +24 +23 +21 = 282.Set implementationThe following code declares an int variable x that can contain a subset of{0,1,2,..., 31}. After this, the code adds the elements 1, 3, 4 and 8 to the setand prints the size of the set.int x = 0;x |= (1<<1);x |= (1<<3);x |= (1<<4);x |= (1<<8);cout << __builtin_popcount(x) << \"\\n\"; // 498",
            "Then, the following code prints all elements that belong to the set:for (int i = 0; i < 32; i++) {if (x&(1<<i)) cout << i << \" \";}// output: 1 3 4 8Set operationsSet operations can be implemented as follows as bit operations:set syntax bit syntaxintersection a b a & bunion a b a | bcomplement a ~adifference a \\ b a & (~b)For example, the following code rst constructs the sets x = {1,3,4,8} andy = {3,6,8,9}, and then constructs the set z = x  y = {1,3,4,6,8,9}:int x = (1<<1)|(1<<3)|(1<<4)|(1<<8);int y = (1<<3)|(1<<6)|(1<<8)|(1<<9);int z = x|y;cout << __builtin_popcount(z) << \"\\n\"; // 6Iterating through subsetsThe following code goes through the subsets of {0,1,..., n 1}:for (int b = 0; b < (1<<n); b++) {// process subset b}The following code goes through the subsets with exactly k elements:for (int b = 0; b < (1<<n); b++) {if (__builtin_popcount(b) == k) {// process subset b}}The following code goes through the subsets of a set x:int b = 0;do {// process subset b} while (b=(b-x)&x);99",
            "10.4 Bit optimizationsMany algorithms can be optimized using bit operations. Such optimizationsdo not change the time complexity of the algorithm, but they may have a largeimpact on the actual running time of the code. In this section we discuss examplesof such situations.Hamming distancesThe Hamming distance hamming(a,b) between two strings a and b of equallength is the number of positions where the strings differ. For example,hamming(01101,11001) = 2.Consider the following problem: Given a list of n bit strings, each of length k,calculate the minimum Hamming distance between two strings in the list. Forexample, the answer for [00111,01101,11110] is 2, because hamming(00111,01101) = 2, hamming(00111,11110) = 3, and hamming(01101,11110) = 3.A straightforward way to solve the problem is to go through all pairs of stringsand calculate their Hamming distances, which yields an O(n2k) time algorithm.The following function can be used to calculate distances:int hamming(string a, string b) {int d = 0;for (int i = 0; i < k; i++) {if (a[i] != b[i]) d++;}return d;}However, if k is small, we can optimize the code by storing the bit stringsas integers and calculating the Hamming distances using bit operations. Inparticular, if k  32, we can just store the strings as int values and use thefollowing function to calculate distances:int hamming(int a, int b) {return __builtin_popcount(a^b);}In the above function, the xor operation constructs a bit string that has one bitsin positions where a and b differ. Then, the number of bits is calculated usingthe __builtin_popcount function.To compare the implementations, we generated a list of 10000 random bitstrings of length 30. Using the rst approach, the search took 13.5 seconds, andafter the bit optimization, it only took 0.5 seconds. Thus, the bit optimized codewas almost 30 times faster than the original code.100",
            "Counting subgridsAs another example, consider the following problem: Given an n n grid whoseeach square is either black (1) or white (0), calculate the number of subgridswhose all corners are black. For example, the gridcontains two such subgrids:There is an O(n3) time algorithm for solving the problem: go through allO(n2) pairs of rows and for each pair (a,b) calculate the number of columns thatcontain a black square in both rows in O(n) time. The following code assumesthat color[y][x] denotes the color in row y and column x:int count = 0;for (int i = 0; i < n; i++) {if (color[a][i] == 1 && color[b][i] == 1) count++;}Then, those columns account for count(count 1)/2 subgrids with black corners,because we can choose any two of them to form a subgrid.To optimize this algorithm, we divide the grid into blocks of columns such thateach block consists of N consecutive columns. Then, each row is stored as a listof N-bit numbers that describe the colors of the squares. Now we can process Ncolumns at the same time using bit operations. In the following code, color[y][k]represents a block of N colors as bits.int count = 0;for (int i = 0; i <= n/N; i++) {count += __builtin_popcount(color[a][i]&color[b][i]);}The resulting algorithm works in O(n3/N) time.We generated a random grid of size 25002500 and compared the originaland bit optimized implementation. While the original code took 29.6 seconds, thebit optimized version only took 3.1 seconds with N = 32 (int numbers) and 1.7seconds with N = 64 (long long numbers).101",
            "10.5 Dynamic programmingBit operations provide an efcient and convenient way to implement dynamicprogramming algorithms whose states contain subsets of elements, because suchstates can be stored as integers. Next we discuss examples of combining bitoperations and dynamic programming.Optimal selectionAs a rst example, consider the following problem: We are given the prices of kproducts over n days, and we want to buy each product exactly once. However,we are allowed to buy at most one product in a day. What is the minimum totalprice? For example, consider the following scenario (k = 3 and n = 8):product 0product 1product 20 1 2 3 4 5 6 76 9 5 2 8 9 1 68 2 6 2 7 5 7 25 3 9 7 3 5 1 4In this scenario, the minimum total price is 5:product 0product 1product 20 1 2 3 4 5 6 76 9 5 2 8 9 1 68 2 6 2 7 5 7 25 3 9 7 3 5 1 4Let price[x][d] denote the price of product x on day d. For example, in theabove scenario price[2][3] = 7. Then, let total(S,d) denote the minimum totalprice for buying a subset S of products by day d. Using this function, the solutionto the problem is total({0... k 1},n 1).First, total(,d) = 0, because it does not cost anything to buy an empty set,and total({x},0) = price[x][0], because there is one way to buy one product onthe rst day. Then, the following recurrence can be used:total(S,d) = min(total(S,d 1),minxS(total(S \\ x,d 1)+price[x][d]))This means that we either do not buy any product on day d or buy a product xthat belongs to S. In the latter case, we remove x from S and add the price of xto the total price.The next step is to calculate the values of the function using dynamic pro-gramming. To store the function values, we declare an arrayint total[1<<K][N];102",
            "where K and N are suitably large constants. The rst dimension of the arraycorresponds to a bit representation of a subset.First, the cases where d = 0 can be processed as follows:for (int x = 0; x < k; x++) {total[1<<x][0] = price[x][0];}Then, the recurrence translates into the following code:for (int d = 1; d < n; d++) {for (int s = 0; s < (1<<k); s++) {total[s][d] = total[s][d-1];for (int x = 0; x < k; x++) {if (s&(1<<x)) {total[s][d] = min(total[s][d],total[s^(1<<x)][d-1]+price[x][d]);}}}}The time complexity of the algorithm is O(n2kk).From permutations to subsetsUsing dynamic programming, it is often possible to change an iteration overpermutations into an iteration over subsets1. The benet of this is that n!, thenumber of permutations, is much larger than 2 n, the number of subsets. Forexample, if n = 20, then n!  2.41018 and 2n  106. Thus, for certain values of n,we can efciently go through the subsets but not through the permutations.As an example, consider the following problem: There is an elevator withmaximum weight x, and n people with known weights who want to get from theground oor to the top oor. What is the minimum number of rides needed if thepeople enter the elevator in an optimal order?For example, suppose that x = 10, n = 5 and the weights are as follows:person weight0 21 32 33 54 6In this case, the minimum number of rides is 2. One optimal order is {0,2,3,1,4},which partitions the people into two rides: rst {0,2,3} (total weight 10), and then{1,4} (total weight 9).1This technique was introduced in 1962 by M. Held and R. M. Karp [34].103",
            "The problem can be easily solved in O(n!n) time by testing all possible permu-tations of n people. However, we can use dynamic programming to get a moreefcient O(2nn) time algorithm. The idea is to calculate for each subset of peopletwo values: the minimum number of rides needed and the minimum weight ofpeople who ride in the last group.Let weight[p] denote the weight of personp. We dene two functions: rides(S)is the minimum number of rides for a subset S, and last(S) is the minimumweight of the last ride. For example, in the above scenariorides({1,3,4}) = 2 and last({1,3,4}) = 5,because the optimal rides are {1,4} and {3}, and the second ride has weight 5. Ofcourse, our nal goal is to calculate the value of rides({0... n 1}).We can calculate the values of the functions recursively and then applydynamic programming. The idea is to go through all people who belong to S andoptimally choose the last person p who enters the elevator. Each such choiceyields a subproblem for a smaller subset of people. If last(S \\ p)+weight[p]  x,we can add p to the last ride. Otherwise, we have to reserve a new ride thatinitially only contains p.To implement dynamic programming, we declare an arraypair<int,int> best[1<<N];that contains for each subset S a pair (rides(S),last(S)). We set the value for anempty group as follows:best[0] = {1,0};Then, we can ll the array as follows:for (int s = 1; s < (1<<n); s++) {// initial value: n+1 rides are neededbest[s] = {n+1,0};for (int p = 0; p < n; p++) {if (s&(1<<p)) {auto option = best[s^(1<<p)];if (option.second+weight[p] <= x) {// add p to an existing rideoption.second += weight[p];} else {// reserve a new ride for poption.first++;option.second = weight[p];}best[s] = min(best[s], option);}}}104",
            "Note that the above loop guarantees that for any two subsets S1 and S2 suchthat S1  S2, we process S1 before S2. Thus, the dynamic programming valuesare calculated in the correct order.Counting subsetsOur last problem in this chapter is as follows: Let X = {0... n1}, and each subsetS  X is assigned an integer value[S]. Our task is to calculate for each Ssum(S) =ASvalue[A],i.e., the sum of values of subsets of S.For example, suppose that n = 3 and the values are as follows: value[] = 3 value[{0}] = 1 value[{1}] = 4 value[{0,1}] = 5 value[{2}] = 5 value[{0,2}] = 1 value[{1,2}] = 3 value[{0,1,2}] = 3In this case, for example,sum({0,2}) = value[]+value[{0}]+value[{2}]+value[{0,2}]= 3+1+5+1 = 10.Because there are a total of 2n subsets, one possible solution is to go throughall pairs of subsets in O(22n) time. However, using dynamic programming, wecan solve the problem in O(2nn) time. The idea is to focus on sums where theelements that may be removed from S are restricted.Let partial(S,k) denote the sum of values of subsets of S with the restrictionthat only elements 0... k may be removed from S. For example,partial({0,2},1) = value[{2}]+value[{0,2}],because we may only remove elements 0... 1. We can calculate values ofsum usingvalues of partial, becausesum(S) = partial(S,n 1).The base cases for the function arepartial(S,1) = value[S],because in this case no elements can be removed from S. Then, in the generalcase we can use the following recurrence:partial(S,k) ={partial(S,k 1) k  Spartial(S,k 1)+partial(S \\ {k},k 1) k  S105",
            "Here we focus on the element k. If k  S, we have two options: we may eitherkeep k in S or remove it from S.There is a particularly clever way to implement the calculation of sums. Wecan declare an arrayint sum[1<<N];that will contain the sum of each subset. The array is initialized as follows:for (int s = 0; s < (1<<n); s++) {sum[s] = value[s];}Then, we can ll the array as follows:for (int k = 0; k < n; k++) {for (int s = 0; s < (1<<n); s++) {if (s&(1<<k)) sum[s] += sum[s^(1<<k)];}}This code calculates the values of partial(S,k) for k = 0... n 1 to the array sum.Since partial(S,k) is always based on partial(S,k 1), we can reuse the arraysum, which yields a very efcient implementation.106",
            "Part IIGraph algorithms107",
            "",
            "Chapter 11Basics of graphsMany programming problems can be solved by modeling the problem as a graphproblem and using an appropriate graph algorithm. A typical example of a graphis a network of roads and cities in a country. Sometimes, though, the graph ishidden in the problem and it may be difcult to detect it.This part of the book discusses graph algorithms, especially focusing on topicsthat are important in competitive programming. In this chapter, we go throughconcepts related to graphs, and study different ways to represent graphs inalgorithms.11.1 Graph terminologyA graph consists of nodes and edges. In this book, the variable n denotes thenumber of nodes in a graph, and the variable m denotes the number of edges.The nodes are numbered using integers 1,2,..., n.For example, the following graph consists of 5 nodes and 7 edges:1 23 45A path leads from node a to node b through edges of the graph. The lengthof a path is the number of edges in it. For example, the above graph contains apath 1  3  4  5 of length 3 from node 1 to node 5:1 23 45A path is a cycle if the rst and last node is the same. For example, the abovegraph contains a cycle 1  3  4  1. A path is simple if each node appears atmost once in the path.109",
            "ConnectivityA graph is connected if there is a path between any two nodes. For example,the following graph is connected:1 23 4The following graph is not connected, because it is not possible to get fromnode 4 to any other node:1 23 4The connected parts of a graph are called its components. For example, thefollowing graph contains three components: {1, 2, 3}, {4, 5, 6, 7} and {8}.1 23 6 74 58A tree is a connected graph that consists of n nodes and n 1 edges. There isa unique path between any two nodes of a tree. For example, the following graphis a tree:1 23 45Edge directionsA graph is directed if the edges can be traversed in one direction only. Forexample, the following graph is directed:1 23 45The above graph contains a path 3  1  2  5 from node 3 to node 5, butthere is no path from node 5 to node 3.110",
            "Edge weightsIn a weighted graph, each edge is assigned a weight. The weights are ofteninterpreted as edge lengths. For example, the following graph is weighted:1 23 45517673The length of a path in a weighted graph is the sum of the edge weights onthe path. For example, in the above graph, the length of the path 1  2  5 is 12,and the length of the path 1  3  4  5 is 11. The latter path is the shortestpath from node 1 to node 5.Neighbors and degreesTwo nodes are neighbors or adjacent if there is an edge between them. Thedegree of a node is the number of its neighbors. For example, in the followinggraph, the neighbors of node 2 are 1, 4 and 5, so its degree is 3.1 23 45The sum of degrees in a graph is always 2m, where m is the number of edges,because each edge increases the degree of exactly two nodes by one. For thisreason, the sum of degrees is always even.A graph is regular if the degree of every node is a constant d. A graph iscomplete if the degree of every node is n 1, i.e., the graph contains all possibleedges between the nodes.In a directed graph, the indegree of a node is the number of edges that endat the node, and the outdegree of a node is the number of edges that start atthe node. For example, in the following graph, the indegree of node 2 is 2, andthe outdegree of node 2 is 1.1 23 45111",
            "ColoringsIn a coloring of a graph, each node is assigned a color so that no adjacent nodeshave the same color.A graph is bipartite if it is possible to color it using two colors. It turns outthat a graph is bipartite exactly when it does not contain a cycle with an oddnumber of edges. For example, the graph2 35 641is bipartite, because it can be colored as follows:2 35 641However, the graph2 35 641is not bipartite, because it is not possible to color the following cycle of threenodes using two colors:2 35 641SimplicityA graph is simple if no edge starts and ends at the same node, and there are nomultiple edges between two nodes. Often we assume that graphs are simple. Forexample, the following graph is not simple:2 35 641112",
            "11.2 Graph representationThere are several ways to represent graphs in algorithms. The choice of a datastructure depends on the size of the graph and the way the algorithm processesit. Next we will go through three common representations.Adjacency list representationIn the adjacency list representation, each node x in the graph is assigned anadjacency list that consists of nodes to which there is an edge fromx. Adjacencylists are the most popular way to represent graphs, and most algorithms can beefciently implemented using them.A convenient way to store the adjacency lists is to declare an array of vectorsas follows:vector<int> adj[N];The constant N is chosen so that all adjacency lists can be stored. For example,the graph1 2 34can be stored as follows:adj[1].push_back(2);adj[2].push_back(3);adj[2].push_back(4);adj[3].push_back(4);adj[4].push_back(1);If the graph is undirected, it can be stored in a similar way, but each edge isadded in both directions.For a weighted graph, the structure can be extended as follows:vector<pair<int,int>> adj[N];In this case, the adjacency list of node a contains the pair (b,w) always whenthere is an edge from node a to node b with weight w. For example, the graph1 2 345 76 52113",
            "can be stored as follows:adj[1].push_back({2,5});adj[2].push_back({3,7});adj[2].push_back({4,6});adj[3].push_back({4,5});adj[4].push_back({1,2});The benet of using adjacency lists is that we can efciently nd the nodesto which we can move from a given node through an edge. For example, thefollowing loop goes through all nodes to which we can move from node s:for (auto u : adj[s]) {// process node u}Adjacency matrix representationAn adjacency matrix is a two-dimensional array that indicates which edgesthe graph contains. We can efciently check from an adjacency matrix if there isan edge between two nodes. The matrix can be stored as an arrayint adj[N][N];where each value adj[a][b] indicates whether the graph contains an edge fromnode a to node b. If the edge is included in the graph, then adj[a][b] = 1, andotherwise adj[a][b] = 0. For example, the graph1 2 34can be represented as follows:1 0 0 00 0 0 10 0 1 10 1 0 043211 2 3 4If the graph is weighted, the adjacency matrix representation can be extendedso that the matrix contains the weight of the edge if the edge exists. Using thisrepresentation, the graph114",
            "1 2 345 76 52corresponds to the following matrix:2 0 0 00 0 0 50 0 7 60 5 0 043211 2 3 4The drawback of the adjacency matrix representation is that the matrixcontains n2 elements, and usually most of them are zero. For this reason, therepresentation cannot be used if the graph is large.Edge list representationAn edge list contains all edges of a graph in some order. This is a convenientway to represent a graph if the algorithm processes all edges of the graph and itis not needed to nd edges that start at a given node.The edge list can be stored in a vectorvector<pair<int,int>> edges;where each pair (a,b) denotes that there is an edge from node a to node b. Thus,the graph1 2 34can be represented as follows:edges.push_back({1,2});edges.push_back({2,3});edges.push_back({2,4});edges.push_back({3,4});edges.push_back({4,1});If the graph is weighted, the structure can be extended as follows:115",
            "vector<tuple<int,int,int>> edges;Each element in this list is of the form ( a,b,w), which means that there is anedge from node a to node b with weight w. For example, the graph1 2 345 76 52can be represented as follows1:edges.push_back({1,2,5});edges.push_back({2,3,7});edges.push_back({2,4,6});edges.push_back({3,4,5});edges.push_back({4,1,2});1In some older compilers, the function make_tuple must be used instead of the braces (forexample, make_tuple(1,2,5) instead of {1,2,5}).116",
            "Chapter 12Graph traversalThis chapter discusses two fundamental graph algorithms: depth-rst search andbreadth-rst search. Both algorithms are given a starting node in the graph, andthey visit all nodes that can be reached from the starting node. The difference inthe algorithms is the order in which they visit the nodes.12.1 Depth-rst searchDepth-rst search (DFS) is a straightforward graph traversal technique. Thealgorithm begins at a starting node, and proceeds to all other nodes that arereachable from the starting node using the edges of the graph.Depth-rst search always follows a single path in the graph as long as itnds new nodes. After this, it returns to previous nodes and begins to exploreother parts of the graph. The algorithm keeps track of visited nodes, so that itprocesses each node only once.ExampleLet us consider how depth-rst search processes the following graph:1 234 5We may begin the search at any node of the graph; now we will begin the searchat node 1.The search rst proceeds to node 2:1 234 5117",
            "After this, nodes 3 and 5 will be visited:1 234 5The neighbors of node 5 are 2 and 3, but the search has already visited both ofthem, so it is time to return to the previous nodes. Also the neighbors of nodes 3and 2 have been visited, so we next move from node 1 to node 4:1 234 5After this, the search terminates because it has visited all nodes.The time complexity of depth-rst search is O(n +m) where n is the numberof nodes and m is the number of edges, because the algorithm processes eachnode and edge once.ImplementationDepth-rst search can be conveniently implemented using recursion. The fol-lowing function dfs begins a depth-rst search at a given node. The functionassumes that the graph is stored as adjacency lists in an arrayvector<int> adj[N];and also maintains an arraybool visited[N];that keeps track of the visited nodes. Initially, each array value is false, andwhen the search arrives at node s, the value of visited[s] becomes true. Thefunction can be implemented as follows:void dfs(int s) {if (visited[s]) return;visited[s] = true;// process node sfor (auto u: adj[s]) {dfs(u);}}118",
            "12.2 Breadth-rst searchBreadth-rst search (BFS) visits the nodes in increasing order of their distancefrom the starting node. Thus, we can calculate the distance from the startingnode to all other nodes using breadth-rst search. However, breadth-rst searchis more difcult to implement than depth-rst search.Breadth-rst search goes through the nodes one level after another. First thesearch explores the nodes whose distance from the starting node is 1, then thenodes whose distance is 2, and so on. This process continues until all nodes havebeen visited.ExampleLet us consider how breadth-rst search processes the following graph:1 2 34 5 6Suppose that the search begins at node 1. First, we process all nodes that can bereached from node 1 using a single edge:1 2 34 5 6After this, we proceed to nodes 3 and 5:1 2 34 5 6Finally, we visit node 6:1 2 34 5 6119",
            "Now we have calculated the distances from the starting node to all nodes of thegraph. The distances are as follows:node distance1 02 13 24 15 26 3Like in depth-rst search, the time complexity of breadth-rst search isO(n +m), where n is the number of nodes and m is the number of edges.ImplementationBreadth-rst search is more difcult to implement than depth-rst search, be-cause the algorithm visits nodes in different parts of the graph. A typical imple-mentation is based on a queue that contains nodes. At each step, the next nodein the queue will be processed.The following code assumes that the graph is stored as adjacency lists andmaintains the following data structures:queue<int> q;bool visited[N];int distance[N];The queue q contains nodes to be processed in increasing order of theirdistance. New nodes are always added to the end of the queue, and the node atthe beginning of the queue is the next node to be processed. The array visitedindicates which nodes the search has already visited, and the array distance willcontain the distances from the starting node to all nodes of the graph.The search can be implemented as follows, starting at node x:visited[x] = true;distance[x] = 0;q.push(x);while (!q.empty()) {int s = q.front(); q.pop();// process node sfor (auto u : adj[s]) {if (visited[u]) continue;visited[u] = true;distance[u] = distance[s]+1;q.push(u);}}120",
            "12.3 ApplicationsUsing the graph traversal algorithms, we can check many properties of graphs.Usually, both depth-rst search and breadth-rst search may be used, but inpractice, depth-rst search is a better choice, because it is easier to implement.In the following applications we will assume that the graph is undirected.Connectivity checkA graph is connected if there is a path between any two nodes of the graph. Thus,we can check if a graph is connected by starting at an arbitrary node and ndingout if we can reach all other nodes.For example, in the graph21354a depth-rst search from node 1 visits the following nodes:21354Since the search did not visit all the nodes, we can conclude that the graphis not connected. In a similar way, we can also nd all connected components ofa graph by iterating through the nodes and always starting a new depth-rstsearch if the current node does not belong to any component yet.Finding cyclesA graph contains a cycle if during a graph traversal, we nd a node whoseneighbor (other than the previous node in the current path) has already beenvisited. For example, the graph21354contains two cycles and we can nd one of them as follows:121",
            "21354After moving from node 2 to node 5 we notice that the neighbor 3 of node 5 hasalready been visited. Thus, the graph contains a cycle that goes through node 3,for example, 3  2  5  3.Another way to nd out whether a graph contains a cycle is to simply calculatethe number of nodes and edges in every component. If a component contains cnodes and no cycle, it must contain exactly c 1 edges (so it has to be a tree). Ifthere are c or more edges, the component surely contains a cycle.Bipartiteness checkA graph is bipartite if its nodes can be colored using two colors so that there areno adjacent nodes with the same color. It is surprisingly easy to check if a graphis bipartite using graph traversal algorithms.The idea is to color the starting node blue, all its neighbors red, all theirneighbors blue, and so on. If at some point of the search we notice that twoadjacent nodes have the same color, this means that the graph is not bipartite.Otherwise the graph is bipartite and one coloring has been found.For example, the graph21354is not bipartite, because a search from node 1 proceeds as follows:21354We notice that the color or both nodes 2 and 5 is red, while they are adjacentnodes in the graph. Thus, the graph is not bipartite.This algorithm always works, because when there are only two colors avail-able, the color of the starting node in a component determines the colors of allother nodes in the component. It does not make any difference whether thestarting node is red or blue.Note that in the general case, it is difcult to nd out if the nodes in a graphcan be colored using k colors so that no adjacent nodes have the same color. Evenwhen k = 3, no efcient algorithm is known but the problem is NP-hard.122",
            "Chapter 13Shortest pathsFinding a shortest path between two nodes of a graph is an important problemthat has many practical applications. For example, a natural problem related toa road network is to calculate the shortest possible length of a route between twocities, given the lengths of the roads.In an unweighted graph, the length of a path equals the number of its edges,and we can simply use breadth-rst search to nd a shortest path. However, inthis chapter we focus on weighted graphs where more sophisticated algorithmsare needed for nding shortest paths.13.1 BellmanFord algorithmThe BellmanFord algorithm1 nds shortest paths from a starting node to allnodes of the graph. The algorithm can process all kinds of graphs, provided thatthe graph does not contain a cycle with negative length. If the graph contains anegative cycle, the algorithm can detect this.The algorithm keeps track of distances from the starting node to all nodesof the graph. Initially, the distance to the starting node is 0 and the distance toall other nodes in innite. The algorithm reduces the distances by nding edgesthat shorten the paths until it is not possible to reduce any distance.ExampleLet us consider how the BellmanFord algorithm works in the following graph:1 23 460  53132271The algorithm is named after R. E. Bellman and L. R. Ford who published it independentlyin 1958 and 1956, respectively [5, 24].123",
            "Each node of the graph is assigned a distance. Initially, the distance to thestarting node is 0, and the distance to all other nodes is innite.The algorithm searches for edges that reduce distances. First, all edges fromnode 1 reduce distances:1 23 450 53 75313227After this, edges 2  5 and 3  4 reduce distances:1 23 450 53 475313227Finally, there is one more change:1 23 450 53 465313227After this, no edge can reduce any distance. This means that the distancesare nal, and we have successfully calculated the shortest distances from thestarting node to all nodes of the graph.For example, the shortest distance 3 from node 1 to node 5 corresponds to thefollowing path:1 23 450 53 465313227124",
            "ImplementationThe following implementation of the BellmanFord algorithm determines theshortest distances from a node x to all nodes of the graph. The code assumesthat the graph is stored as an edge list edges that consists of tuples of the form(a,b,w), meaning that there is an edge from node a to node b with weight w.The algorithm consists of n 1 rounds, and on each round the algorithm goesthrough all edges of the graph and tries to reduce the distances. The algorithmconstructs an array distance that will contain the distances from x to all nodesof the graph. The constant INF denotes an innite distance.for (int i = 1; i <= n; i++) distance[i] = INF;distance[x] = 0;for (int i = 1; i <= n-1; i++) {for (auto e : edges) {int a, b, w;tie(a, b, w) = e;distance[b] = min(distance[b], distance[a]+w);}}The time complexity of the algorithm isO(nm), because the algorithm consistsof n 1 rounds and iterates through all m edges during a round. If there are nonegative cycles in the graph, all distances are nal after n 1 rounds, becauseeach shortest path can contain at most n 1 edges.In practice, the nal distances can usually be found faster than inn1 rounds.Thus, a possible way to make the algorithm more efcient is to stop the algorithmif no distance can be reduced during a round.Negative cyclesThe BellmanFord algorithm can also be used to check if the graph contains acycle with negative length. For example, the graph12343 15 72contains a negative cycle 2  3  4  2 with length 4.If the graph contains a negative cycle, we can shorten innitely many timesany path that contains the cycle by repeating the cycle again and again. Thus,the concept of a shortest path is not meaningful in this situation.A negative cycle can be detected using the BellmanFord algorithm by runningthe algorithm for n rounds. If the last round reduces any distance, the graphcontains a negative cycle. Note that this algorithm can be used to search for anegative cycle in the whole graph regardless of the starting node.125",
            "SPFA algorithmThe SPFA algorithm (Shortest Path Faster Algorithm) [20] is a variant of theBellmanFord algorithm, that is often more efcient than the original algorithm.The SPFA algorithm does not go through all the edges on each round, but instead,it chooses the edges to be examined in a more intelligent way.The algorithm maintains a queue of nodes that might be used for reducingthe distances. First, the algorithm adds the starting node x to the queue. Then,the algorithm always processes the rst node in the queue, and when an edgea  b reduces a distance, node b is added to the queue.The efciency of the SPFA algorithm depends on the structure of the graph:the algorithm is often efcient, but its worst case time complexity is still O(nm)and it is possible to create inputs that make the algorithm as slow as the originalBellmanFord algorithm.13.2 Dijkstras algorithmDijkstras algorithm2 nds shortest paths from the starting node to all nodes ofthe graph, like the BellmanFord algorithm. The benet of Dijsktras algorithmis that it is more efcient and can be used for processing large graphs. However,the algorithm requires that there are no negative weight edges in the graph.Like the BellmanFord algorithm, Dijkstras algorithm maintains distancesto the nodes and reduces them during the search. Dijkstras algorithm is efcient,because it only processes each edge in the graph once, using the fact that thereare no negative edges.ExampleLet us consider how Dijkstras algorithm works in the following graph when thestarting node is node 1:3 42 15  0625921Like in the BellmanFord algorithm, initially the distance to the starting node is0 and the distance to all other nodes is innite.At each step, Dijkstras algorithm selects a node that has not been processedyet and whose distance is as small as possible. The rst such node is node 1 withdistance 0.2E. W. Dijkstra published the algorithm in 1959 [ 14]; however, his original paper does notmention how to implement the algorithm efciently.126",
            "When a node is selected, the algorithm goes through all edges that start atthe node and reduces the distances using them:3 42 15 95 01625921In this case, the edges from node 1 reduced the distances of nodes 2, 4 and 5,whose distances are now 5, 9 and 1.The next node to be processed is node 5 with distance 1. This reduces thedistance to node 4 from 9 to 3:3 42 15 35 01625921After this, the next node is node 4, which reduces the distance to node 3 to 9:3 42 159 35 01625921A remarkable property in Dijkstras algorithm is that whenever a node isselected, its distance is nal. For example, at this point of the algorithm, thedistances 0, 1 and 3 are the nal distances to nodes 1, 5 and 4.After this, the algorithm processes the two remaining nodes, and the naldistances are as follows:3 42 157 35 01625921127",
            "Negative edgesThe efciency of Dijkstras algorithm is based on the fact that the graph doesnot contain negative edges. If there is a negative edge, the algorithm may giveincorrect results. As an example, consider the following graph:12342 36 5The shortest path from node 1 to node 4 is 1  3  4 and its length is 1. However,Dijkstras algorithm nds the path 1 2  4 by following the minimum weightedges. The algorithm does not take into account that on the other path, theweight 5 compensates the previous large weight 6.ImplementationThe following implementation of Dijkstras algorithm calculates the minimumdistances from a node x to other nodes of the graph. The graph is stored asadjacency lists so that adj[a] contains a pair (b,w) always when there is an edgefrom node a to node b with weight w.An efcient implementation of Dijkstras algorithm requires that it is possibleto efciently nd the minimum distance node that has not been processed. Anappropriate data structure for this is a priority queue that contains the nodesordered by their distances. Using a priority queue, the next node to be processedcan be retrieved in logarithmic time.In the following code, the priority queue q contains pairs of the form (d, x),meaning that the current distance to node x is d. The array distance containsthe distance to each node, and the array processed indicates whether a node hasbeen processed. Initially the distance is 0 to x and  to all other nodes.for (int i = 1; i <= n; i++) distance[i] = INF;distance[x] = 0;q.push({0,x});while (!q.empty()) {int a = q.top().second; q.pop();if (processed[a]) continue;processed[a] = true;for (auto u : adj[a]) {int b = u.first, w = u.second;if (distance[a]+w < distance[b]) {distance[b] = distance[a]+w;q.push({-distance[b],b});}}}128",
            "Note that the priority queue contains negative distances to nodes. The reasonfor this is that the default version of the C++ priority queue nds maximumelements, while we want to nd minimum elements. By using negative distances,we can directly use the default priority queue 3. Also note that there may beseveral instances of the same node in the priority queue; however, only theinstance with the minimum distance will be processed.The time complexity of the above implementation is O(n +mlogm), becausethe algorithm goes through all nodes of the graph and adds for each edge at mostone distance to the priority queue.13.3 FloydWarshall algorithmThe FloydWarshall algorithm4 provides an alternative way to approach theproblem of nding shortest paths. Unlike the other algorithms of this chapter, itnds all shortest paths between the nodes in a single run.The algorithm maintains a two-dimensional array that contains distancesbetween the nodes. First, distances are calculated only using direct edges betweenthe nodes, and after this, the algorithm reduces distances by using intermediatenodes in paths.ExampleLet us consider how the FloydWarshall algorithm works in the following graph:3 42 15725921Initially, the distance from each node to itself is 0, and the distance betweennodes a and b is x if there is an edge between nodes a and b with weight x. Allother distances are innite.In this graph, the initial array is as follows:1 2 3 4 51 0 5  9 12 5 0 2  3  2 0 7 4 9  7 0 25 1   2 03Of course, we could also declare the priority queue as in Chapter 4.5 and use positive distances,but the implementation would be a bit longer.4The algorithm is named after R. W. Floyd and S. Warshall who published it independently in1962 [23, 70].129",
            "The algorithm consists of consecutive rounds. On each round, the algorithmselects a new node that can act as an intermediate node in paths from now on,and distances are reduced using this node.On the rst round, node 1 is the new intermediate node. There is a new pathbetween nodes 2 and 4 with length 14, because node 1 connects them. There isalso a new path between nodes 2 and 5 with length 6.1 2 3 4 51 0 5  9 12 5 0 2 14 63  2 0 7 4 9 14 7 0 25 1 6  2 0On the second round, node 2 is the new intermediate node. This creates newpaths between nodes 1 and 3 and between nodes 3 and 5:1 2 3 4 51 0 5 7 9 12 5 0 2 14 63 7 2 0 7 84 9 14 7 0 25 1 6 8 2 0On the third round, node 3 is the new intermediate round. There is a newpath between nodes 2 and 4:1 2 3 4 51 0 5 7 9 12 5 0 2 9 63 7 2 0 7 84 9 9 7 0 25 1 6 8 2 0The algorithm continues like this, until all nodes have been appointed inter-mediate nodes. After the algorithm has nished, the array contains the minimumdistances between any two nodes:1 2 3 4 51 0 5 7 3 12 5 0 2 8 63 7 2 0 7 84 3 8 7 0 25 1 6 8 2 0For example, the array tells us that the shortest distance between nodes 2and 4 is 8. This corresponds to the following path:130",
            "3 42 15725921ImplementationThe advantage of the FloydWarshall algorithm that it is easy to implement. Thefollowing code constructs a distance matrix where distance[a][b] is the shortestdistance between nodes a and b. First, the algorithm initializes distance usingthe adjacency matrix adj of the graph:for (int i = 1; i <= n; i++) {for (int j = 1; j <= n; j++) {if (i == j) distance[i][j] = 0;else if (adj[i][j]) distance[i][j] = adj[i][j];else distance[i][j] = INF;}}After this, the shortest distances can be found as follows:for (int k = 1; k <= n; k++) {for (int i = 1; i <= n; i++) {for (int j = 1; j <= n; j++) {distance[i][j] = min(distance[i][j],distance[i][k]+distance[k][j]);}}}The time complexity of the algorithm is O(n3), because it contains threenested loops that go through the nodes of the graph.Since the implementation of the FloydWarshall algorithm is simple, thealgorithm can be a good choice even if it is only needed to nd a single shortestpath in the graph. However, the algorithm can only be used when the graph is sosmall that a cubic time complexity is fast enough.131",
            "132",
            "Chapter 14Tree algorithmsA tree is a connected, acyclic graph that consists of n nodes and n 1 edges.Removing any edge from a tree divides it into two components, and adding anyedge to a tree creates a cycle. Moreover, there is always a unique path betweenany two nodes of a tree.For example, the following tree consists of 8 nodes and 7 edges:1 42 3 7568The leaves of a tree are the nodes with degree 1, i.e., with only one neighbor.For example, the leaves of the above tree are nodes 3, 5, 7 and 8.In a rooted tree, one of the nodes is appointed the root of the tree, and allother nodes are placed underneath the root. For example, in the following tree,node 1 is the root node.142 375 68In a rooted tree, the children of a node are its lower neighbors, and theparent of a node is its upper neighbor. Each node has exactly one parent, exceptfor the root that does not have a parent. For example, in the above tree, thechildren of node 2 are nodes 5 and 6, and its parent is node 1.133",
            "The structure of a rooted tree is recursive: each node of the tree acts as theroot of a subtree that contains the node itself and all nodes that are in thesubtrees of its children. For example, in the above tree, the subtree of node 2consists of nodes 2, 5, 6 and 8:25 6814.1 Tree traversalGeneral graph traversal algorithms can be used to traverse the nodes of a tree.However, the traversal of a tree is easier to implement than that of a generalgraph, because there are no cycles in the tree and it is not possible to reach anode from multiple directions.The typical way to traverse a tree is to start a depth-rst search at an arbitrarynode. The following recursive function can be used:void dfs(int s, int e) {// process node sfor (auto u : adj[s]) {if (u != e) dfs(u, s);}}The function is given two parameters: the current node s and the previousnode e. The purpose of the parameter e is to make sure that the search onlymoves to nodes that have not been visited yet.The following function call starts the search at node x:dfs(x, 0);In the rst call e = 0, because there is no previous node, and it is allowed toproceed to any direction in the tree.Dynamic programmingDynamic programming can be used to calculate some information during a treetraversal. Using dynamic programming, we can, for example, calculate in O(n)time for each node of a rooted tree the number of nodes in its subtree or thelength of the longest path from the node to a leaf.134",
            "As an example, let us calculate for each node s a value count[s]: the numberof nodes in its subtree. The subtree contains the node itself and all nodes inthe subtrees of its children, so we can calculate the number of nodes recursivelyusing the following code:void dfs(int s, int e) {count[s] = 1;for (auto u : adj[s]) {if (u == e) continue;dfs(u, s);count[s] += count[u];}}14.2 DiameterThe diameter of a tree is the maximum length of a path between two nodes. Forexample, consider the following tree:1 42 3 756The diameter of this tree is 4, which corresponds to the following path:1 42 3 756Note that there may be several maximum-length paths. In the above path, wecould replace node 6 with node 5 to obtain another path with length 4.Next we will discuss two O(n) time algorithms for calculating the diameterof a tree. The rst algorithm is based on dynamic programming, and the secondalgorithm uses two depth-rst searches.Algorithm 1A general way to approach many tree problems is to rst root the tree arbitrarily.After this, we can try to solve the problem separately for each subtree. Our rstalgorithm for calculating the diameter is based on this idea.An important observation is that every path in a rooted tree has a highestpoint: the highest node that belongs to the path. Thus, we can calculate for each135",
            "node the length of the longest path whose highest point is the node. One of thosepaths corresponds to the diameter of the tree.For example, in the following tree, node 1 is the highest point on the paththat corresponds to the diameter:142 375 6We calculate for each node x two values: toLeaf(x): the maximum length of a path from x to any leaf maxLength(x): the maximum length of a path whose highest point is xFor example, in the above tree, toLeaf(1) = 2, because there is a path 1  2  6,and maxLength(1) = 4, because there is a path 6  2  1  4  7. In this case,maxLength(1) equals the diameter.Dynamic programming can be used to calculate the above values for all nodesin O(n) time. First, to calculate toLeaf(x), we go through the children of x,choose a child c with maximum toLeaf(c) and add one to this value. Then, tocalculate maxLength(x), we choose two distinct children a and b such that the sumtoLeaf(a)+toLeaf(b) is maximum and add two to this sum.Algorithm 2Another efcient way to calculate the diameter of a tree is based on two depth-rst searches. First, we choose an arbitrary node a in the tree and nd thefarthest node b from a. Then, we nd the farthest node c from b. The diameterof the tree is the distance between b and c.In the following graph, a, b and c could be:1 42 3 756ab cThis is an elegant method, but why does it work?It helps to draw the tree differently so that the path that corresponds to thediameter is horizontal, and all other nodes hang from it:136",
            "1 423756ab cxNode x indicates the place where the path from node a joins the path thatcorresponds to the diameter. The farthest node from a is node b, node c or someother node that is at least as far from node x. Thus, this node is always a validchoice for an endpoint of a path that corresponds to the diameter.14.3 All longest pathsOur next problem is to calculate for every node in the tree the maximum lengthof a path that begins at the node. This can be seen as a generalization of the treediameter problem, because the largest of those lengths equals the diameter ofthe tree. Also this problem can be solved in O(n) time.As an example, consider the following tree:142365Let maxLength(x) denote the maximum length of a path that begins at nodex. For example, in the above tree, maxLength(4) = 3, because there is a path4  1  2  6. Here is a complete table of the values:node x 1 2 3 4 5 6maxLength(x) 2 2 3 3 3 3Also in this problem, a good starting point for solving the problem is to rootthe tree arbitrarily:142 35 6The rst part of the problem is to calculate for every node x the maximumlength of a path that goes through a child of x. For example, the longest pathfrom node 1 goes through its child 2:137",
            "142 35 6This part is easy to solve in O(n) time, because we can use dynamic programmingas we have done previously.Then, the second part of the problem is to calculate for every node x themaximum length of a path through its parent p. For example, the longest pathfrom node 3 goes through its parent 1:142 35 6At rst glance, it seems that we should choose the longest path from p.However, this does not always work, because the longest path from p may gothrough x. Here is an example of this situation:142 35 6Still, we can solve the second part in O(n) time by storing two maximumlengths for each node x: maxLength1(x): the maximum length of a path from x maxLength2(x) the maximum length of a path from x in another directionthan the rst pathFor example, in the above graph, maxLength1(1) = 2 using the path 1  2  5, andmaxLength2(1) = 1 using the path 1  3.Finally, if the path that corresponds to maxLength1(p) goes through x, we con-clude that the maximum length ismaxLength2(p)+1, and otherwise the maximumlength is maxLength1(p)+1.138",
            "14.4 Binary treesA binary tree is a rooted tree where each node has a left and right subtree. It ispossible that a subtree of a node is empty. Thus, every node in a binary tree haszero, one or two children.For example, the following tree is a binary tree:12 34 567The nodes of a binary tree have three natural orderings that correspond todifferent ways to recursively traverse the tree: pre-order: rst process the root, then traverse the left subtree, thentraverse the right subtree in-order: rst traverse the left subtree, then process the root, then traversethe right subtree post-order: rst traverse the left subtree, then traverse the right subtree,then process the rootFor the above tree, the nodes in pre-order are [1 ,2,4,5,6,3,7], in in-order[4,2,6,5,1,3,7] and in post-order [4,6,5,2,7,3,1].If we know the pre-order and in-order of a tree, we can reconstruct the exactstructure of the tree. For example, the above tree is the only possible tree withpre-order [1,2,4,5,6,3,7] and in-order [4 ,2,6,5,1,3,7]. In a similar way, thepost-order and in-order also determine the structure of a tree.However, the situation is different if we only know the pre-order and post-order of a tree. In this case, there may be more than one tree that match theorderings. For example, in both of the trees1212the pre-order is [1,2] and the post-order is [2,1], but the structures of the treesare different.139",
            "140",
            "Chapter 15Spanning treesA spanning tree of a graph consists of all nodes of the graph and some of theedges of the graph so that there is a path between any two nodes. Like treesin general, spanning trees are connected and acyclic. Usually there are severalways to construct a spanning tree.For example, consider the following graph:12 345 63595276 3One spanning tree for the graph is as follows:12 345 635923The weight of a spanning tree is the sum of its edge weights. For example,the weight of the above spanning tree is 3+5+9+3+2 = 22.A minimum spanning tree is a spanning tree whose weight is as small aspossible. The weight of a minimum spanning tree for the example graph is 20,and such a tree can be constructed as follows:12 345 635273141",
            "In a similar way, a maximum spanning tree is a spanning tree whoseweight is as large as possible. The weight of a maximum spanning tree for theexample graph is 32:12 345 6595 76Note that a graph may have several minimum and maximum spanning trees,so the trees are not unique.It turns out that several greedy methods can be used to construct minimumand maximum spanning trees. In this chapter, we discuss two algorithms thatprocess the edges of the graph ordered by their weights. We focus on ndingminimum spanning trees, but the same algorithms can nd maximum spanningtrees by processing the edges in reverse order.15.1 Kruskals algorithmIn Kruskals algorithm1, the initial spanning tree only contains the nodes ofthe graph and does not contain any edges. Then the algorithm goes through theedges ordered by their weights, and always adds an edge to the tree if it does notcreate a cycle.The algorithm maintains the components of the tree. Initially, each node ofthe graph belongs to a separate component. Always when an edge is added to thetree, two components are joined. Finally, all nodes belong to the same component,and a minimum spanning tree has been found.ExampleLet us consider how Kruskals algorithm processes the following graph:12 345 63595276 3The rst step of the algorithm is to sort the edges in increasing order of theirweights. The result is the following list:1The algorithm was published in 1956 by J. B. Kruskal [48].142",
            "edge weight56 212 336 315 523 525 646 734 9After this, the algorithm goes through the list and adds each edge to the treeif it joins two separate components.Initially, each node is in its own component:12 345 6The rst edge to be added to the tree is the edge 56 that creates a component{5,6} by joining the components {5} and {6}:12 345 62After this, the edges 12, 36 and 15 are added in a similar way:12 345 63523After those steps, most components have been joined and there are twocomponents in the tree: {1,2,3,5,6} and {4}.The next edge in the list is the edge 23, but it will not be included in the tree,because nodes 2 and 3 are already in the same component. For the same reason,the edge 25 will not be included in the tree.143",
            "Finally, the edge 46 will be included in the tree:12 345 635273After this, the algorithm will not add any new edges, because the graph isconnected and there is a path between any two nodes. The resulting graph is aminimum spanning tree with weight 2+3+3+5+7 = 20.Why does this work?It is a good question why Kruskals algorithm works. Why does the greedystrategy guarantee that we will nd a minimum spanning tree?Let us see what happens if the minimum weight edge of the graph is notincluded in the spanning tree. For example, suppose that a spanning tree for theprevious graph would not contain the minimum weight edge 56. We do not knowthe exact structure of such a spanning tree, but in any case it has to contain someedges. Assume that the tree would be as follows:12 345 6However, it is not possible that the above tree would be a minimum spanningtree for the graph. The reason for this is that we can remove an edge from thetree and replace it with the minimum weight edge 56. This produces a spanningtree whose weight is smaller:12 345 62For this reason, it is always optimal to include the minimum weight edge inthe tree to produce a minimum spanning tree. Using a similar argument, wecan show that it is also optimal to add the next edge in weight order to the tree,and so on. Hence, Kruskals algorithm works correctly and always produces aminimum spanning tree.144",
            "ImplementationWhen implementing Kruskals algorithm, it is convenient to use the edge listrepresentation of the graph. The rst phase of the algorithm sorts the edges inthe list in O(mlogm) time. After this, the second phase of the algorithm buildsthe minimum spanning tree as follows:for (...) {if (!same(a,b)) unite(a,b);}The loop goes through the edges in the list and always processes an edgeab where a and b are two nodes. Two functions are needed: the function samedetermines if a and b are in the same component, and the function unite joinsthe components that contain a and b.The problem is how to efciently implement the functions same and unite.One possibility is to implement the function same as a graph traversal and checkif we can get from node a to node b. However, the time complexity of such afunction would be O(n +m) and the resulting algorithm would be slow, becausethe function same will be called for each edge in the graph.We will solve the problem using a union-nd structure that implements bothfunctions in O(logn) time. Thus, the time complexity of Kruskals algorithm willbe O(mlogn) after sorting the edge list.15.2 Union-nd structureA union-nd structure maintains a collection of sets. The sets are disjoint,so no element belongs to more than one set. Two O(logn) time operations aresupported: the unite operation joins two sets, and the find operation nds therepresentative of the set that contains a given element2.StructureIn a union-nd structure, one element in each set is the representative of the set,and there is a chain from any other element of the set to the representative. Forexample, assume that the sets are {1,4,7}, {5} and {2,3,6,8}:1234 56782The structure presented here was introduced in 1971 by J. D. Hopcroft and J. D. Ullman [38].Later, in 1975, R. E. Tarjan studied a more sophisticated variant of the structure [ 64] that isdiscussed in many algorithm textbooks nowadays.145",
            "In this case the representatives of the sets are 4, 5 and 2. We can nd therepresentative of any element by following the chain that begins at the element.For example, the element 2 is the representative for the element 6, because wefollow the chain 6  3  2. Two elements belong to the same set exactly whentheir representatives are the same.Two sets can be joined by connecting the representative of one set to therepresentative of the other set. For example, the sets {1,4,7} and {2,3,6,8} can bejoined as follows:1234678The resulting set contains the elements {1,2,3,4,6,7,8}. From this on, theelement 2 is the representative for the entire set and the old representative 4points to the element 2.The efciency of the union-nd structure depends on how the sets are joined.It turns out that we can follow a simple strategy: always connect the representa-tive of the smaller set to the representative of the larger set (or if the sets areof equal size, we can make an arbitrary choice). Using this strategy, the lengthof any chain will be O(logn), so we can nd the representative of any elementefciently by following the corresponding chain.ImplementationThe union-nd structure can be implemented using arrays. In the followingimplementation, the array link contains for each element the next element in thechain or the element itself if it is a representative, and the array size indicatesfor each representative the size of the corresponding set.Initially, each element belongs to a separate set:for (int i = 1; i <= n; i++) link[i] = i;for (int i = 1; i <= n; i++) size[i] = 1;The function find returns the representative for an element x. The represen-tative can be found by following the chain that begins at x.int find(int x) {while (x != link[x]) x = link[x];return x;}The function same checks whether elements a and b belong to the same set.This can easily be done by using the function find:146",
            "bool same(int a, int b) {return find(a) == find(b);}The function unite joins the sets that contain elements a and b (the elementshave to be in different sets). The function rst nds the representatives of thesets and then connects the smaller set to the larger set.void unite(int a, int b) {a = find(a);b = find(b);if (size[a] < size[b]) swap(a,b);size[a] += size[b];link[b] = a;}The time complexity of the function find is O(logn) assuming that the lengthof each chain is O(logn). In this case, the functions same and unite also work inO(logn) time. The function unite makes sure that the length of each chain isO(logn) by connecting the smaller set to the larger set.15.3 Prims algorithmPrims algorithm3 is an alternative method for nding a minimum spanningtree. The algorithm rst adds an arbitrary node to the tree. After this, thealgorithm always chooses a minimum-weight edge that adds a new node to thetree. Finally, all nodes have been added to the tree and a minimum spanningtree has been found.Prims algorithm resembles Dijkstras algorithm. The difference is that Dijk-stras algorithm always selects an edge whose distance from the starting node isminimum, but Prims algorithm simply selects the minimum weight edge thatadds a new node to the tree.ExampleLet us consider how Prims algorithm works in the following graph:12 345 63595276 33The algorithm is named after R. C. Prim who published it in 1957 [54]. However, the samealgorithm was discovered already in 1930 by V. Jarnk.147",
            "Initially, there are no edges between the nodes:12 345 6An arbitrary node can be the starting node, so let us choose node 1. First, we addnode 2 that is connected by an edge of weight 3:12 345 63After this, there are two edges with weight 5, so we can add either node 3 ornode 5 to the tree. Let us add node 3 rst:12 345 635The process continues until all nodes have been included in the tree:12 345 635273ImplementationLike Dijkstras algorithm, Prims algorithm can be efciently implemented using apriority queue. The priority queue should contain all nodes that can be connectedto the current component using a single edge, in increasing order of the weightsof the corresponding edges.The time complexity of Prims algorithm isO(n+mlogm) that equals the timecomplexity of Dijkstras algorithm. In practice, Prims and Kruskals algorithmsare both efcient, and the choice of the algorithm is a matter of taste. Still, mostcompetitive programmers use Kruskals algorithm.148",
            "Chapter 16Directed graphsIn this chapter, we focus on two classes of directed graphs: Acyclic graphs: There are no cycles in the graph, so there is no path fromany node to itself1. Successor graphs: The outdegree of each node is 1, so each node has aunique successor.It turns out that in both cases, we can design efcient algorithms that are basedon the special properties of the graphs.16.1 Topological sortingA topological sort is an ordering of the nodes of a directed graph such that ifthere is a path from node a to node b, then node a appears before node b in theordering. For example, for the graph1 2 34 5 6one topological sort is [4,1,5,2,3,6]:1 2 34 5 6An acyclic graph always has a topological sort. However, if the graph containsa cycle, it is not possible to form a topological sort, because no node of the cyclecan appear before the other nodes of the cycle in the ordering. It turns out thatdepth-rst search can be used to both check if a directed graph contains a cycleand, if it does not contain a cycle, to construct a topological sort.1Directed acyclic graphs are sometimes called DAGs.149",
            "AlgorithmThe idea is to go through the nodes of the graph and always begin a depth-rstsearch at the current node if it has not been processed yet. During the searches,the nodes have three possible states: state 0: the node has not been processed (white) state 1: the node is under processing (light gray) state 2: the node has been processed (dark gray)Initially, the state of each node is 0. When a search reaches a node for therst time, its state becomes 1. Finally, after all successors of the node have beenprocessed, its state becomes 2.If the graph contains a cycle, we will nd this out during the search, becausesooner or later we will arrive at a node whose state is 1. In this case, it is notpossible to construct a topological sort.If the graph does not contain a cycle, we can construct a topological sort byadding each node to a list when the state of the node becomes 2. This list inreverse order is a topological sort.Example 1In the example graph, the search rst proceeds from node 1 to node 6:1 2 34 5 6Now node 6 has been processed, so it is added to the list. After this, also nodes3, 2 and 1 are added to the list:1 2 34 5 6At this point, the list is [6,3,2,1]. The next search begins at node 4:1 2 34 5 6150",
            "Thus, the nal list is [6,3,2,1,5,4]. We have processed all nodes, so a topologi-cal sort has been found. The topological sort is the reverse list [4,5,1,2,3,6]:1 2 34 5 6Note that a topological sort is not unique, and there can be several topologicalsorts for a graph.Example 2Let us now consider a graph for which we cannot construct a topological sort,because the graph contains a cycle:1 2 34 5 6The search proceeds as follows:1 2 34 5 6The search reaches node 2 whose state is 1, which means that the graph containsa cycle. In this example, there is a cycle 2  3  5  2.16.2 Dynamic programmingIf a directed graph is acyclic, dynamic programming can be applied to it. Forexample, we can efciently solve the following problems concerning paths from astarting node to an ending node: how many different paths are there? what is the shortest/longest path? what is the minimum/maximum number of edges in a path? which nodes certainly appear in any path?151",
            "Counting the number of pathsAs an example, let us calculate the number of paths from node 1 to node 6 in thefollowing graph:1 2 34 5 6There are a total of three such paths: 1  2  3  6 1  4  5  2  3  6 1  4  5  3  6Let paths(x) denote the number of paths from node 1 to node x. As a basecase, paths(1) = 1. Then, to calculate other values of paths(x), we may use therecursionpaths(x) = paths(a1)+paths(a2)++ paths(ak)where a1,a2,..., ak are the nodes from which there is an edge tox. Since the graphis acyclic, the values of paths(x) can be calculated in the order of a topologicalsort. A topological sort for the above graph is as follows:1 2 34 5 6Hence, the numbers of paths are as follows:1 2 34 5 61 1 31 2 3For example, to calculate the value of paths(3), we can use the formulapaths(2)+paths(5), because there are edges from nodes 2 and 5 to node 3. Sincepaths(2) = 2 and paths(5) = 1, we conclude that paths(3) = 3.152",
            "Extending Dijkstras algorithmA by-product of Dijkstras algorithm is a directed, acyclic graph that indicatesfor each node of the original graph the possible ways to reach the node using ashortest path from the starting node. Dynamic programming can be applied tothat graph. For example, in the graph1 23 4535 48212the shortest paths from node 1 may use the following edges:1 23 4535 4212Now we can, for example, calculate the number of shortest paths from node 1to node 5 using dynamic programming:1 23 4535 42121 12 33Representing problems as graphsActually, any dynamic programming problem can be represented as a directed,acyclic graph. In such a graph, each node corresponds to a dynamic programmingstate and the edges indicate how the states depend on each other.As an example, consider the problem of forming a sum of money n usingcoins {c1, c2,..., ck}. In this problem, we can construct a graph where each nodecorresponds to a sum of money, and the edges show how the coins can be chosen.For example, for coins {1,3,4} and n = 6, the graph is as follows:153",
            "0 1 2 3 4 5 6Using this representation, the shortest path from node 0 to noden correspondsto a solution with the minimum number of coins, and the total number of pathsfrom node 0 to node n equals the total number of solutions.16.3 Successor pathsFor the rest of the chapter, we will focus on successor graphs. In those graphs,the outdegree of each node is 1, i.e., exactly one edge starts at each node. Asuccessor graph consists of one or more components, each of which contains onecycle and some paths that lead to it.Successor graphs are sometimes called functional graphs. The reason forthis is that any successor graph corresponds to a function that denes the edgesof the graph. The parameter for the function is a node of the graph, and thefunction gives the successor of that node.For example, the functionx 1 2 3 4 5 6 7 8 9succ(x) 3 5 7 6 2 2 1 6 3denes the following graph:1 23456789Since each node of a successor graph has a unique successor, we can alsodene a function succ(x,k) that gives the node that we will reach if we begin atnode x and walk k steps forward. For example, in the above graph succ(4,6) = 2,because we will reach node 2 by walking 6 steps from node 4:4 6 2 5 2 5 2A straightforward way to calculate a value of succ(x,k) is to start at node xand walk k steps forward, which takes O(k) time. However, using preprocessing,any value of succ(x,k) can be calculated in only O(logk) time.The idea is to precalculate all values of succ(x,k) where k is a power of twoand at most u, where u is the maximum number of steps we will ever walk. Thiscan be efciently done, because we can use the following recursion:154",
            "succ(x,k) ={succ(x) k = 1succ(succ(x,k/2),k/2) k > 1Precalculating the values takes O(nlogu) time, because O(logu) values arecalculated for each node. In the above graph, the rst values are as follows:x 1 2 3 4 5 6 7 8 9succ(x,1) 3 5 7 6 2 2 1 6 3succ(x,2) 7 2 1 2 5 5 3 2 7succ(x,4) 3 2 7 2 5 5 1 2 3succ(x,8) 7 2 1 2 5 5 3 2 7After this, any value of succ(x,k) can be calculated by presenting the numberof steps k as a sum of powers of two. For example, if we want to calculate thevalue of succ(x,11), we rst form the representation 11 = 8+2+1. Using that,succ(x,11) = succ(succ(succ(x,8),2),1).For example, in the previous graphsucc(4,11) = succ(succ(succ(4,8),2),1) = 5.Such a representation always consists of O(logk) parts, so calculating a valueof succ(x,k) takes O(logk) time.16.4 Cycle detectionConsider a successor graph that only contains a path that ends in a cycle. Wemay ask the following questions: if we begin our walk at the starting node, whatis the rst node in the cycle and how many nodes does the cycle contain?For example, in the graph546321we begin our walk at node 1, the rst node that belongs to the cycle is node 4,and the cycle consists of three nodes (4, 5 and 6).A simple way to detect the cycle is to walk in the graph and keep track of allnodes that have been visited. Once a node is visited for the second time, we canconclude that the node is the rst node in the cycle. This method works in O(n)time and also uses O(n) memory.However, there are better algorithms for cycle detection. The time complexityof such algorithms is still O(n), but they only use O(1) memory. This is animportant improvement if n is large. Next we will discuss Floyds algorithm thatachieves these properties.155",
            "Floyds algorithmFloyds algorithm2 walks forward in the graph using two pointers a and b.Both pointers begin at a node x that is the starting node of the graph. Then,on each turn, the pointer a walks one step forward and the pointer b walks twosteps forward. The process continues until the pointers meet each other:a = succ(x);b = succ(succ(x));while (a != b) {a = succ(a);b = succ(succ(b));}At this point, the pointer a has walked k steps and the pointer b has walked2k steps, so the length of the cycle divides k. Thus, the rst node that belongsto the cycle can be found by moving the pointer a to node x and advancing thepointers step by step until they meet again.a = x;while (a != b) {a = succ(a);b = succ(b);}first = a;After this, the length of the cycle can be calculated as follows:b = succ(a);length = 1;while (a != b) {b = succ(b);length++;}2The idea of the algorithm is mentioned in [46] and attributed to R. W. Floyd; however, it isnot known if Floyd actually discovered the algorithm.156",
            "Chapter 17Strong connectivityIn a directed graph, the edges can be traversed in one direction only, so even ifthe graph is connected, this does not guarantee that there would be a path froma node to another node. For this reason, it is meaningful to dene a new conceptthat requires more than connectivity.A graph is strongly connected if there is a path from any node to all othernodes in the graph. For example, in the following picture, the left graph isstrongly connected while the right graph is not.1 23 41 23 4The right graph is not strongly connected because, for example, there is nopath from node 2 to node 1.The strongly connected components of a graph divide the graph intostrongly connected parts that are as large as possible. The strongly connectedcomponents form an acyclic component graph that represents the deep struc-ture of the original graph.For example, for the graph7321654the strongly connected components are as follows:7321654157",
            "The corresponding component graph is as follows:BADCThe components are A = {1,2}, B = {3,6,7}, C = {4} and D = {5}.A component graph is an acyclic, directed graph, so it is easier to processthan the original graph. Since the graph does not contain cycles, we can alwaysconstruct a topological sort and use dynamic programming techniques like thosepresented in Chapter 16.17.1 Kosarajus algorithmKosarajus algorithm1 is an efcient method for nding the strongly connectedcomponents of a directed graph. The algorithm performs two depth-rst searches:the rst search constructs a list of nodes according to the structure of the graph,and the second search forms the strongly connected components.Search 1The rst phase of Kosarajus algorithm constructs a list of nodes in the orderin which a depth-rst search processes them. The algorithm goes through thenodes, and begins a depth-rst search at each unprocessed node. Each node willbe added to the list after it has been processed.In the example graph, the nodes are processed in the following order:73216541/8 2/7 9/144/5 3/6 11/1210/13The notation x/y means that processing the node started at timex and nishedat time y. Thus, the corresponding list is as follows:1According to [1], S. R. Kosaraju invented this algorithm in 1978 but did not publish it. In1981, the same algorithm was rediscovered and published by M. Sharir [57].158",
            "node processing time4 55 62 71 86 127 133 14Search 2The second phase of the algorithm forms the strongly connected components ofthe graph. First, the algorithm reverses every edge in the graph. This guaranteesthat during the second search, we will always nd strongly connected componentsthat do not have extra nodes.After reversing the edges, the example graph is as follows:7321654After this, the algorithm goes through the list of nodes created by the rstsearch, in reverse order. If a node does not belong to a component, the algorithmcreates a new component and starts a depth-rst search that adds all new nodesfound during the search to the new component.In the example graph, the rst component begins at node 3:7321654Note that since all edges are reversed, the component does not leak to otherparts in the graph.159",
            "The next nodes in the list are nodes 7 and 6, but they already belong to acomponent, so the next new component begins at node 1:7321654Finally, the algorithm processes nodes 5 and 4 that create the remainingstrongly connected components:7321654The time complexity of the algorithm is O(n + m), because the algorithmperforms two depth-rst searches.17.2 2SAT problemStrong connectivity is also linked with the 2SAT problem2. In this problem, weare given a logical formula(a1 b1)(a2 b2) (am bm),where each ai and bi is either a logical variable ( x1, x2,..., xn) or a negation ofa logical variable ( x1,x2,..., xn). The symbols   and  denote logicaloperators and and or. Our task is to assign each variable a value so that theformula is true, or state that this is not possible.For example, the formulaL1 = (x2 x1)(x1 x2)(x1  x3)(x2 x3)(x1  x4)is true when the variables are assigned as follows:x1 = falsex2 = falsex3 = truex4 = true2The algorithm presented here was introduced in [ 4]. There is also another well-knownlinear-time algorithm [19] that is based on backtracking.160",
            "However, the formulaL2 = (x1  x2)(x1 x2)(x1  x3)(x1 x3)is always false, regardless of how we assign the values. The reason for this isthat we cannot choose a value for x1 without creating a contradiction. If x1 isfalse, both x2 and x2 should be true which is impossible, and if x1 is true, bothx3 and x3 should be true which is also impossible.The 2SAT problem can be represented as a graph whose nodes correspond tovariables xi and negations xi, and edges determine the connections betweenthe variables. Each pair ( ai  bi) generates two edges: ai  bi and bi  ai.This means that if ai does not hold, bi must hold, and vice versa.The graph for the formula L1 is:x3 x2x4 x1x1 x4x2 x3And the graph for the formula L2 is:x3 x2 x2 x3x1x1The structure of the graph tells us whether it is possible to assign the valuesof the variables so that the formula is true. It turns out that this can be doneexactly when there are no nodes xi and xi such that both nodes belong to thesame strongly connected component. If there are such nodes, the graph containsa path from xi to xi and also a path from xi to xi, so both xi and xi should betrue which is not possible.In the graph of the formula L1 there are no nodes xi and xi such that bothnodes belong to the same strongly connected component, so a solution exists. Inthe graph of the formula L2 all nodes belong to the same strongly connectedcomponent, so a solution does not exist.If a solution exists, the values for the variables can be found by going throughthe nodes of the component graph in a reverse topological sort order. At each step,we process a component that does not contain edges that lead to an unprocessedcomponent. If the variables in the component have not been assigned values,their values will be determined according to the values in the component, and if161",
            "they already have values, they remain unchanged. The process continues untileach variable has been assigned a value.The component graph for the formula L1 is as follows:A B C DThe components are A = {x4}, B = {x1, x2,x3}, C = {x1,x2, x3} and D = {x4}.When constructing the solution, we rst process the component D where x4becomes true. After this, we process the component C where x1 and x2 becomefalse and x3 becomes true. All variables have been assigned values, so theremaining components A and B do not change the variables.Note that this method works, because the graph has a special structure: ifthere are paths from node xi to node xj and from node xj to node xj, then nodexi never becomes true. The reason for this is that there is also a path from nodexj to node xi, and both xi and xj become false.A more difcult problem is the3SAT problem, where each part of the formulais of the form (ai bi  ci). This problem is NP-hard, so no efcient algorithm forsolving the problem is known.162",
            "Chapter 18Tree queriesThis chapter discusses techniques for processing queries on subtrees and pathsof a rooted tree. For example, such queries are: what is the kth ancestor of a node? what is the sum of values in the subtree of a node? what is the sum of values on a path between two nodes? what is the lowest common ancestor of two nodes?18.1 Finding ancestorsThe kth ancestor of a node x in a rooted tree is the node that we will reachif we move k levels up from x. Let ancestor(x,k) denote the kth ancestor of anode x (or 0 if there is no such an ancestor). For example, in the following tree,ancestor(2,1) = 1 and ancestor(8,2) = 4.124 563 78An easy way to calculate any value of ancestor(x,k) is to perform a sequenceof k moves in the tree. However, the time complexity of this method is O(k),which may be slow, because a tree of n nodes may have a chain of n nodes.163",
            "Fortunately, using a technique similar to that used in Chapter 16.3, any valueof ancestor(x,k) can be efciently calculated in O(logk) time after preprocessing.The idea is to precalculate all values ancestor(x,k) where k  n is a power of two.For example, the values for the above tree are as follows:x 1 2 3 4 5 6 7 8ancestor(x,1) 0 1 4 1 1 2 4 7ancestor(x,2) 0 0 1 0 0 1 1 4ancestor(x,4) 0 0 0 0 0 0 0 0The preprocessing takesO(nlogn) time, becauseO(logn) values are calculatedfor each node. After this, any value of ancestor(x,k) can be calculated in O(logk)time by representing k as a sum where each term is a power of two.18.2 Subtrees and pathsA tree traversal array contains the nodes of a rooted tree in the order in whicha depth-rst search from the root node visits them. For example, in the tree12 3 4 56 7 8 9a depth-rst search proceeds as follows:12 3 4 56 7 8 9Hence, the corresponding tree traversal array is as follows:1 2 6 3 4 7 8 9 5164",
            "Subtree queriesEach subtree of a tree corresponds to a subarray of the tree traversal array suchthat the rst element of the subarray is the root node. For example, the followingsubarray contains the nodes of the subtree of node 4:1 2 6 3 4 7 8 9 5Using this fact, we can efciently process queries that are related to subtrees ofa tree. As an example, consider a problem where each node is assigned a value,and our task is to support the following queries: update the value of a node calculate the sum of values in the subtree of a nodeConsider the following tree where the blue numbers are the values of thenodes. For example, the sum of the subtree of node 4 is 3+4+3+1 = 11.12 3 4 56 7 8 923 5 3 14 4 3 1The idea is to construct a tree traversal array that contains three values foreach node: the identier of the node, the size of the subtree, and the value of thenode. For example, the array for the above tree is as follows:node idsubtree sizenode value1 2 6 3 4 7 8 9 59 2 1 1 4 1 1 1 12 3 4 5 3 4 3 1 1Using this array, we can calculate the sum of values in any subtree by rstnding out the size of the subtree and then the values of the corresponding nodes.For example, the values in the subtree of node 4 can be found as follows:node idsubtree sizenode value1 2 6 3 4 7 8 9 59 2 1 1 4 1 1 1 12 3 4 5 3 4 3 1 1To answer the queries efciently, it sufces to store the values of the nodesin a binary indexed or segment tree. After this, we can both update a value andcalculate the sum of values in O(logn) time.165",
            "Path queriesUsing a tree traversal array, we can also efciently calculate sums of values onpaths from the root node to any node of the tree. Consider a problem where ourtask is to support the following queries: change the value of a node calculate the sum of values on a path from the root to a nodeFor example, in the following tree, the sum of values from the root node tonode 7 is 4+5+5 = 14:12 3 4 56 7 8 945 3 5 23 5 3 1We can solve this problem like before, but now each value in the last row ofthe array is the sum of values on a path from the root to the node. For example,the following array corresponds to the above tree:node idsubtree sizepath sum1 2 6 3 4 7 8 9 59 2 1 1 4 1 1 1 14 9 12 7 9 14 12 10 6When the value of a node increases by x, the sums of all nodes in its subtreeincrease by x. For example, if the value of node 4 increases by 1, the arraychanges as follows:node idsubtree sizepath sum1 2 6 3 4 7 8 9 59 2 1 1 4 1 1 1 14 9 12 7 10 15 13 11 6Thus, to support both the operations, we should be able to increase all valuesin a range and retrieve a single value. This can be done in O(logn) time using abinary indexed or segment tree (see Chapter 9.4).166",
            "18.3 Lowest common ancestorThe lowest common ancestor of two nodes of a rooted tree is the lowest nodewhose subtree contains both the nodes. A typical problem is to efciently processqueries that ask to nd the lowest common ancestor of two nodes.For example, in the following tree, the lowest common ancestor of nodes 5 and8 is node 2:142 375 68Next we will discuss two efcient techniques for nding the lowest commonancestor of two nodes.Method 1One way to solve the problem is to use the fact that we can efciently nd thekth ancestor of any node in the tree. Using this, we can divide the problem ofnding the lowest common ancestor into two parts.We use two pointers that initially point to the two nodes whose lowest commonancestor we should nd. First, we move one of the pointers upwards so that bothpointers point to nodes at the same level.In the example scenario, we move the second pointer one level up so that itpoints to node 6 which is at the same level with node 5:142 375 68167",
            "After this, we determine the minimum number of steps needed to move bothpointers upwards so that they will point to the same node. The node to which thepointers point after this is the lowest common ancestor.In the example scenario, it sufces to move both pointers one step upwards tonode 2, which is the lowest common ancestor:142 375 68Since both parts of the algorithm can be performed in O(logn) time usingprecomputed information, we can nd the lowest common ancestor of any twonodes in O(logn) time.Method 2Another way to solve the problem is based on a tree traversal array1. Once again,the idea is to traverse the nodes using a depth-rst search:142 375 68However, we use a different tree traversal array than before: we add eachnode to the array always when the depth-rst search walks through the node,and not only at the rst visit. Hence, a node that has k children appears k +1times in the array and there are a total of 2n 1 nodes in the array.1This lowest common ancestor algorithm was presented in [7]. This technique is sometimescalled the Euler tour technique [66].168",
            "We store two values in the array: the identier of the node and the depth ofthe node in the tree. The following array corresponds to the above tree:node iddepth1 2 5 2 6 8 6 2 1 3 1 4 7 4 11 2 3 2 3 4 3 2 1 2 1 2 3 2 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14Now we can nd the lowest common ancestor of nodes a and b by nding thenode with the minimum depth between nodes a and b in the array. For example,the lowest common ancestor of nodes 5 and 8 can be found as follows:node iddepth1 2 5 2 6 8 6 2 1 3 1 4 7 4 11 2 3 2 3 4 3 2 1 2 1 2 3 2 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14Node 5 is at position 2, node 8 is at position 5, and the node with minimumdepth between positions 2... 5 is node 2 at position 3 whose depth is 2. Thus, thelowest common ancestor of nodes 5 and 8 is node 2.Thus, to nd the lowest common ancestor of two nodes it sufces to process arange minimum query. Since the array is static, we can process such queries inO(1) time after an O(nlogn) time preprocessing.Distances of nodesThe distance between nodes a and b equals the length of the path from a to b. Itturns out that the problem of calculating the distance between nodes reduces tonding their lowest common ancestor.First, we root the tree arbitrarily. After this, the distance of nodes a and bcan be calculated using the formuladepth(a)+depth(b)2depth(c),where c is the lowest common ancestor of a and b and depth(s) denotes the depthof node s. For example, consider the distance of nodes 5 and 8:142 375 68169",
            "The lowest common ancestor of nodes 5 and 8 is node 2. The depths of thenodes are depth(5) = 3, depth(8) = 4 and depth(2) = 2, so the distance betweennodes 5 and 8 is 3+422 = 3.18.4 Ofine algorithmsSo far, we have discussed online algorithms for tree queries. Those algorithmsare able to process queries one after another so that each query is answeredbefore receiving the next query.However, in many problems, the online property is not necessary. In thissection, we focus on ofine algorithms. Those algorithms are given a set ofqueries which can be answered in any order. It is often easier to design an ofinealgorithm compared to an online algorithm.Merging data structuresOne method to construct an ofine algorithm is to perform a depth-rst treetraversal and maintain data structures in nodes. At each node s, we create adata structure d[s] that is based on the data structures of the children of s. Then,using this data structure, all queries related to s are processed.As an example, consider the following problem: We are given a tree whereeach node has some value. Our task is to process queries of the form calculatethe number of nodes with value x in the subtree of node s. For example, in thefollowing tree, the subtree of node 4 contains two nodes whose value is 3.12 3 4 56 7 8 923 5 3 14 4 3 1In this problem, we can use map structures to answer the queries. Forexample, the maps for node 4 and its children are as follows:4131111 3 41 2 1170",
            "If we create such a data structure for each node, we can easily process allgiven queries, because we can handle all queries related to a node immediatelyafter creating its data structure. For example, the above map structure for node4 tells us that its subtree contains two nodes whose value is 3.However, it would be too slow to create all data structures from scratch.Instead, at each node s, we create an initial data structure d[s] that only containsthe value of s. After this, we go through the children of s and merge d[s] and alldata structures d[u] where u is a child of s.For example, in the above tree, the map for node 4 is created by merging thefollowing maps:41311131Here the rst map is the initial data structure for node 4, and the other threemaps correspond to nodes 7, 8 and 9.The merging at node s can be done as follows: We go through the childrenof s and at each child u merge d[s] and d[u]. We always copy the contents fromd[u] to d[s]. However, before this, we swap the contents of d[s] and d[u] if d[s] issmaller than d[u]. By doing this, each value is copied only O(logn) times duringthe tree traversal, which ensures that the algorithm is efcient.To swap the contents of two data structures a and b efciently, we can justuse the following code:swap(a,b);It is guaranteed that the above code works in constant time when a and b areC++ standard library data structures.Lowest common ancestorsThere is also an ofine algorithm for processing a set of lowest common ancestorqueries2. The algorithm is based on the union-nd data structure (see Chapter15.2), and the benet of the algorithm is that it is easier to implement than thealgorithms discussed earlier in this chapter.The algorithm is given as input a set of pairs of nodes, and it determines foreach such pair the lowest common ancestor of the nodes. The algorithm performsa depth-rst tree traversal and maintains disjoint sets of nodes. Initially, eachnode belongs to a separate set. For each set, we also store the highest node in thetree that belongs to the set.When the algorithm visits a node x, it goes through all nodes y such that thelowest common ancestor of x and y has to be found. If y has already been visited,the algorithm reports that the lowest common ancestor of x and y is the highestnode in the set of y. Then, after processing node x, the algorithm joins the sets ofx and its parent.2This algorithm was published by R. E. Tarjan in 1979 [65].171",
            "For example, suppose that we want to nd the lowest common ancestors ofnode pairs (5,8) and (2,7) in the following tree:142 375 68In the following trees, gray nodes denote visited nodes and dashed groups ofnodes belong to the same set. When the algorithm visits node 8, it notices thatnode 5 has been visited and the highest node in its set is 2. Thus, the lowestcommon ancestor of nodes 5 and 8 is 2:142 375 68Later, when visiting node 7, the algorithm determines that the lowest commonancestor of nodes 2 and 7 is 1:142 375 68172",
            "Chapter 19Paths and circuitsThis chapter focuses on two types of paths in graphs: An Eulerian path is a path that goes through each edge exactly once. A Hamiltonian path is a path that visits each node exactly once.While Eulerian and Hamiltonian paths look like similar concepts at rstglance, the computational problems related to them are very different. It turnsout that there is a simple rule that determines whether a graph contains anEulerian path, and there is also an efcient algorithm to nd such a path ifit exists. On the contrary, checking the existence of a Hamiltonian path is aNP-hard problem, and no efcient algorithm is known for solving the problem.19.1 Eulerian pathsAn Eulerian path1 is a path that goes exactly once through each edge of thegraph. For example, the graph1 234 5has an Eulerian path from node 2 to node 5:1 234 51.2.3.4.5.6.1L. Euler studied such paths in 1736 when he solved the famous Knigsberg bridge problem.This was the birth of graph theory.173",
            "An Eulerian circuit is an Eulerian path that starts and ends at the same node.For example, the graph1 234 5has an Eulerian circuit that starts and ends at node 1:1 234 51.2.3.4.5.6.ExistenceThe existence of Eulerian paths and circuits depends on the degrees of the nodes.First, an undirected graph has an Eulerian path exactly when all the edgesbelong to the same connected component and the degree of each node is even or the degree of exactly two nodes is odd, and the degree of all other nodes iseven.In the rst case, each Eulerian path is also an Eulerian circuit. In the secondcase, the odd-degree nodes are the starting and ending nodes of an Eulerian pathwhich is not an Eulerian circuit.For example, in the graph1 234 5nodes 1, 3 and 4 have a degree of 2, and nodes 2 and 5 have a degree of 3. Exactlytwo nodes have an odd degree, so there is an Eulerian path between nodes 2 and5, but the graph does not contain an Eulerian circuit.In a directed graph, we focus on indegrees and outdegrees of the nodes. Adirected graph contains an Eulerian path exactly when all the edges belong tothe same connected component and in each node, the indegree equals the outdegree, or174",
            "in one node, the indegree is one larger than the outdegree, in another node,the outdegree is one larger than the indegree, and in all other nodes, theindegree equals the outdegree.In the rst case, each Eulerian path is also an Eulerian circuit, and in thesecond case, the graph contains an Eulerian path that begins at the node whoseoutdegree is larger and ends at the node whose indegree is larger.For example, in the graph1 234 5nodes 1, 3 and 4 have both indegree 1 and outdegree 1, node 2 has indegree 1and outdegree 2, and node 5 has indegree 2 and outdegree 1. Hence, the graphcontains an Eulerian path from node 2 to node 5:1 234 51.2.3.4.5.6.Hierholzers algorithmHierholzers algorithm2 is an efcient method for constructing an Euleriancircuit. The algorithm consists of several rounds, each of which adds new edgesto the circuit. Of course, we assume that the graph contains an Eulerian circuit;otherwise Hierholzers algorithm cannot nd it.First, the algorithm constructs a circuit that contains some (not necessarilyall) of the edges of the graph. After this, the algorithm extends the circuit step bystep by adding subcircuits to it. The process continues until all edges have beenadded to the circuit.The algorithm extends the circuit by always nding a node x that belongsto the circuit but has an outgoing edge that is not included in the circuit. Thealgorithm constructs a new path from node x that only contains edges that arenot yet in the circuit. Sooner or later, the path will return to nodex, which createsa subcircuit.If the graph only contains an Eulerian path, we can still use Hierholzersalgorithm to nd it by adding an extra edge to the graph and removing the edgeafter the circuit has been constructed. For example, in an undirected graph, weadd the extra edge between the two odd-degree nodes.Next we will see how Hierholzers algorithm constructs an Eulerian circuitfor an undirected graph.2The algorithm was published in 1873 after Hierholzers death [35].175",
            "ExampleLet us consider the following graph:12 3 45 6 7Suppose that the algorithm rst creates a circuit that begins at node 1. Apossible circuit is 1  2  3  1:12 3 45 6 71.2.3.After this, the algorithm adds the subcircuit 2  5  6  2 to the circuit:12 3 45 6 71.2.3.4.5.6.Finally, the algorithm adds the subcircuit 6 3  4  7  6 to the circuit:12 3 45 6 71.2.3.4.5.6.7.8.9.10.176",
            "Now all edges are included in the circuit, so we have successfully constructed anEulerian circuit.19.2 Hamiltonian pathsA Hamiltonian path is a path that visits each node of the graph exactly once.For example, the graph1 234 5contains a Hamiltonian path from node 1 to node 3:1 234 51.2.3.4.If a Hamiltonian path begins and ends at the same node, it is called a Hamil-tonian circuit. The graph above also has an Hamiltonian circuit that beginsand ends at node 1:1 234 51.2.3.4.5.ExistenceNo efcient method is known for testing if a graph contains a Hamiltonian path,and the problem is NP-hard. Still, in some special cases, we can be certain that agraph contains a Hamiltonian path.A simple observation is that if the graph is complete, i.e., there is an edgebetween all pairs of nodes, it also contains a Hamiltonian path. Also strongerresults have been achieved: Diracs theorem: If the degree of each node is at least n/2, the graphcontains a Hamiltonian path. Ores theorem: If the sum of degrees of each non-adjacent pair of nodes isat least n, the graph contains a Hamiltonian path.177",
            "A common property in these theorems and other results is that they guaranteethe existence of a Hamiltonian path if the graph hasa large number of edges. Thismakes sense, because the more edges the graph contains, the more possibilitiesthere is to construct a Hamiltonian path.ConstructionSince there is no efcient way to check if a Hamiltonian path exists, it is clearthat there is also no method to efciently construct the path, because otherwisewe could just try to construct the path and see whether it exists.A simple way to search for a Hamiltonian path is to use a backtrackingalgorithm that goes through all possible ways to construct the path. The timecomplexity of such an algorithm is at least O(n!), because there are n! differentways to choose the order of n nodes.A more efcient solution is based on dynamic programming (see Chapter10.5). The idea is to calculate values of a function possible(S, x), where S is asubset of nodes and x is one of the nodes. The function indicates whether there isa Hamiltonian path that visits the nodes of S and ends at node x. It is possible toimplement this solution in O(2nn2) time.19.3 De Bruijn sequencesA De Bruijn sequence is a string that contains every string of length n exactlyonce as a substring, for a xed alphabet of k characters. The length of such astring is kn +n 1 characters. For example, when n = 3 and k = 2, an example ofa De Bruijn sequence is0001011100.The substrings of this string are all combinations of three bits: 000, 001, 010,011, 100, 101, 110 and 111.It turns out that each De Bruijn sequence corresponds to an Eulerian path ina graph. The idea is to construct a graph where each node contains a string ofn 1 characters and each edge adds one character to the string. The followinggraph corresponds to the above scenario:00 1101101 100010 1An Eulerian path in this graph corresponds to a string that contains allstrings of length n. The string contains the characters of the starting node andall characters of the edges. The starting node has n 1 characters and there arekn characters in the edges, so the length of the string is kn +n 1.178",
            "19.4 Knights toursA knights tour is a sequence of moves of a knight on an n  n chessboardfollowing the rules of chess such that the knight visits each square exactly once.A knights tour is called aclosed tour if the knight nally returns to the startingsquare and otherwise it is called an open tour.For example, here is an open knights tour on a 55 board:1 4 11 16 2512 17 2 5 103 20 7 24 1518 13 22 9 621 8 19 14 23A knights tour corresponds to a Hamiltonian path in a graph whose nodesrepresent the squares of the board, and two nodes are connected with an edge ifa knight can move between the squares according to the rules of chess.A natural way to construct a knights tour is to use backtracking. The searchcan be made more efcient by using heuristics that attempt to guide the knightso that a complete tour will be found quickly.Warnsdorfs ruleWarnsdorfs ruleis a simple and effective heuristic for nding a knights tour3.Using the rule, it is possible to efciently construct a tour even on a large board.The idea is to always move the knight so that it ends up in a square where thenumber of possible moves is as small as possible.For example, in the following situation, there are ve possible squares towhich the knight can move (squares a... e):12ab ec dIn this situation, Warnsdorfs rule moves the knight to squarea, because afterthis choice, there is only a single possible move. The other choices would movethe knight to squares where there would be three moves available.3This heuristic was proposed in Warnsdorfs book [69] in 1823. There are also polynomialalgorithms for nding knights tours [52], but they are more complicated.179",
            "180",
            "Chapter 20Flows and cutsIn this chapter, we focus on the following two problems: Finding a maximum ow: What is the maximum amount of ow we cansend from a node to another node? Finding a minimum cut: What is a minimum-weight set of edges thatseparates two nodes of the graph?The input for both these problems is a directed, weighted graph that containstwo special nodes: the source is a node with no incoming edges, and the sink is anode with no outgoing edges.As an example, we will use the following graph where node 1 is the sourceand node 6 is the sink:12 364 55654123 8Maximum owIn the maximum ow problem, our task is to send as much ow as possiblefrom the source to the sink. The weight of each edge is a capacity that restrictsthe ow that can go through the edge. In each intermediate node, the incomingand outgoing ow has to be equal.For example, the maximum size of a ow in the example graph is 7. Thefollowing picture shows how we can route the ow:12 364 53/56/65/54/41/12/23/3 1/8181",
            "The notation v/k means that a ow of v units is routed through an edge whosecapacity is k units. The size of the ow is 7, because the source sends 3 +4 unitsof ow and the sink receives 5 +2 units of ow. It is easy see that this ow ismaximum, because the total capacity of the edges leading to the sink is 7.Minimum cutIn the minimum cut problem, our task is to remove a set of edges from the graphsuch that there will be no path from the source to the sink after the removal andthe total weight of the removed edges is minimum.The minimum size of a cut in the example graph is 7. It sufces to removethe edges 2  3 and 4  5:12 364 55654123 8After removing the edges, there will be no path from the source to the sink.The size of the cut is 7, because the weights of the removed edges are 6 and 1.The cut is minimum, because there is no valid way to remove edges from thegraph such that their total weight would be less than 7.It is not a coincidence that the maximum size of a ow and the minimum sizeof a cut are the same in the above example. It turns out that a maximum owand a minimum cut are always equally large, so the concepts are two sides of thesame coin.Next we will discuss the FordFulkerson algorithm that can be used to ndthe maximum ow and minimum cut of a graph. The algorithm also helps us tounderstand why they are equally large.20.1 FordFulkerson algorithmThe FordFulkerson algorithm[25] nds the maximum ow in a graph. Thealgorithm begins with an empty ow, and at each step nds a path from thesource to the sink that generates more ow. Finally, when the algorithm cannotincrease the ow anymore, the maximum ow has been found.The algorithm uses a special representation of the graph where each originaledge has a reverse edge in another direction. The weight of each edge indicateshow much more ow we could route through it. At the beginning of the algorithm,the weight of each original edge equals the capacity of the edge and the weight ofeach reverse edge is zero.182",
            "The new representation for the example graph is as follows:12 364 55060 5040 10203 0 80Algorithm descriptionThe FordFulkerson algorithm consists of several rounds. On each round, thealgorithm nds a path from the source to the sink such that each edge on thepath has a positive weight. If there is more than one possible path available, wecan choose any of them.For example, suppose we choose the following path:12 364 55060 5040 10203 0 80After choosing the path, the ow increases by x units, where x is the smallestedge weight on the path. In addition, the weight of each edge on the pathdecreases by x and the weight of each reverse edge increases by x.In the above path, the weights of the edges are 5, 6, 8 and 2. The smallestweight is 2, so the ow increases by 2 and the new graph is as follows:12 364 53242 5040 10023 0 62The idea is that increasing the ow decreases the amount of ow that cango through the edges in the future. On the other hand, it is possible to cancelow later using the reverse edges of the graph if it turns out that it would bebenecial to route the ow in another way.The algorithm increases the ow as long as there is a path from the source tothe sink through positive-weight edges. In the present example, our next pathcan be as follows:183",
            "12 364 53242 5040 10023 0 62The minimum edge weight on this path is 3, so the path increases the ow by3, and the total ow after processing the path is 5.The new graph will be as follows:12 364 53215 2313 10020 3 62We still need two more rounds before reaching the maximum ow. For exam-ple, we can choose the paths 1  2  3  6 and 1  4  5  3  6. Both pathsincrease the ow by 1, and the nal graph is as follows:12 364 52306 0504 01020 3 71It is not possible to increase the ow anymore, because there is no pathfrom the source to the sink with positive edge weights. Hence, the algorithmterminates and the maximum ow is 7.Finding pathsThe FordFulkerson algorithm does not specify how we should choose the pathsthat increase the ow. In any case, the algorithm will terminate sooner or laterand correctly nd the maximum ow. However, the efciency of the algorithmdepends on the way the paths are chosen.A simple way to nd paths is to use depth-rst search. Usually, this workswell, but in the worst case, each path only increases the ow by 1 and thealgorithm is slow. Fortunately, we can avoid this situation by using one of thefollowing techniques:184",
            "The EdmondsKarp algorithm [18] chooses each path so that the numberof edges on the path is as small as possible. This can be done by using breadth-rst search instead of depth-rst search for nding paths. It can be proven thatthis guarantees that the ow increases quickly, and the time complexity of thealgorithm is O(m2n).The scaling algorithm [2] uses depth-rst search to nd paths where eachedge weight is at least a threshold value. Initially, the threshold value is somelarge number, for example the sum of all edge weights of the graph. Always whena path cannot be found, the threshold value is divided by 2. The time complexityof the algorithm is O(m2 log c), where c is the initial threshold value.In practice, the scaling algorithm is easier to implement, because depth-rstsearch can be used for nding paths. Both algorithms are efcient enough forproblems that typically appear in programming contests.Minimum cutsIt turns out that once the FordFulkerson algorithm has found a maximum ow,it has also determined a minimum cut. Let A be the set of nodes that can bereached from the source using positive-weight edges. In the example graph, Acontains nodes 1, 2 and 4:12 364 52306 0504 01020 3 71Now the minimum cut consists of the edges of the original graph that start atsome node in A, end at some node outside A, and whose capacity is fully usedin the maximum ow. In the above graph, such edges are 2  3 and 4  5, thatcorrespond to the minimum cut 6+1 = 7.Why is the ow produced by the algorithm maximum and why is the cutminimum? The reason is that a graph cannot contain a ow whose size is largerthan the weight of any cut of the graph. Hence, always when a ow and a cut areequally large, they are a maximum ow and a minimum cut.Let us consider any cut of the graph such that the source belongs to A, thesink belongs to B and there are some edges between the sets:A B185",
            "The size of the cut is the sum of the edges that go from A to B. This is anupper bound for the ow in the graph, because the ow has to proceed from A toB. Thus, the size of a maximum ow is smaller than or equal to the size of anycut in the graph.On the other hand, the FordFulkerson algorithm produces a ow whose sizeis exactly as large as the size of a cut in the graph. Thus, the ow has to be amaximum ow and the cut has to be a minimum cut.20.2 Disjoint pathsMany graph problems can be solved by reducing them to the maximum owproblem. Our rst example of such a problem is as follows: we are given adirected graph with a source and a sink, and our task is to nd the maximumnumber of disjoint paths from the source to the sink.Edge-disjoint pathsWe will rst focus on the problem of nding the maximum number of edge-disjoint paths from the source to the sink. This means that we should constructa set of paths such that each edge appears in at most one path.For example, consider the following graph:12 34 56In this graph, the maximum number of edge-disjoint paths is 2. We can choosethe paths 1  2  4  3  6 and 1  4  5  6 as follows:12 34 56It turns out that the maximum number of edge-disjoint paths equals themaximum ow of the graph, assuming that the capacity of each edge is one. Afterthe maximum ow has been constructed, the edge-disjoint paths can be foundgreedily by following paths from the source to the sink.Node-disjoint pathsLet us now consider another problem: nding the maximum number of node-disjoint paths from the source to the sink. In this problem, every node, except186",
            "for the source and sink, may appear in at most one path. The number of node-disjoint paths may be smaller than the number of edge-disjoint paths.For example, in the previous graph, the maximum number of node-disjointpaths is 1:12 34 56We can reduce also this problem to the maximum ow problem. Since eachnode can appear in at most one path, we have to limit the ow that goes throughthe nodes. A standard method for this is to divide each node into two nodes suchthat the rst node has the incoming edges of the original node, the second nodehas the outgoing edges of the original node, and there is a new edge from the rstnode to the second node.In our example, the graph becomes as follows:12 34 52 34 56The maximum ow for the graph is as follows:12 34 52 34 56Thus, the maximum number of node-disjoint paths from the source to thesink is 1.20.3 Maximum matchingsThe maximum matching problem asks to nd a maximum-size set of nodepairs in an undirected graph such that each pair is connected with an edge andeach node belongs to at most one pair.There are polynomial algorithms for nding maximum matchings in generalgraphs [17], but such algorithms are complex and rarely seen in programmingcontests. However, in bipartite graphs, the maximum matching problem is mucheasier to solve, because we can reduce it to the maximum ow problem.187",
            "Finding maximum matchingsThe nodes of a bipartite graph can be always divided into two groups such thatall edges of the graph go from the left group to the right group. For example, inthe following bipartite graph, the groups are {1,2,3,4} and {5,6,7,8}.12345678The size of a maximum matching of this graph is 3:12345678We can reduce the bipartite maximum matching problem to the maximumow problem by adding two new nodes to the graph: a source and a sink. We alsoadd edges from the source to each left node and from each right node to the sink.After this, the size of a maximum ow in the graph equals the size of a maximummatching in the original graph.For example, the reduction for the above graph is as follows:12345678The maximum ow of this graph is as follows:12345678188",
            "Halls theoremHalls theoremcan be used to nd out whether a bipartite graph has a matchingthat contains all left or right nodes. If the number of left and right nodes is thesame, Halls theorem tells us if it is possible to construct aperfect matchingthat contains all nodes of the graph.Assume that we want to nd a matching that contains all left nodes. Let Xbe any set of left nodes and let f (X) be the set of their neighbors. According toHalls theorem, a matching that contains all left nodes exists exactly when foreach X, the condition |X|  |f (X)| holds.Let us study Halls theorem in the example graph. First, letX = {1,3} whichyields f (X) = {5,6,8}:12345678The condition of Halls theorem holds, because|X| =2 and |f (X)| =3. Next,let X = {2,4} which yields f (X) = {7}:12345678In this case, |X| =2 and |f (X)| =1, so the condition of Halls theorem doesnot hold. This means that it is not possible to form a perfect matching for thegraph. This result is not surprising, because we already know that the maximummatching of the graph is 3 and not 4.If the condition of Halls theorem does not hold, the setX provides an expla-nation why we cannot form such a matching. Since X contains more nodes thanf (X), there are no pairs for all nodes in X. For example, in the above graph, bothnodes 2 and 4 should be connected with node 7 which is not possible.Knigs theoremA minimum node cover of a graph is a minimum set of nodes such that eachedge of the graph has at least one endpoint in the set. In a general graph, ndinga minimum node cover is a NP-hard problem. However, if the graph is bipartite,Konigs theoremtells us that the size of a minimum node cover and the size189",
            "of a maximum matching are always equal. Thus, we can calculate the size of aminimum node cover using a maximum ow algorithm.Let us consider the following graph with a maximum matching of size 3:12345678Now Konigs theorem tells us that the size of a minimum node cover is also 3.Such a cover can be constructed as follows:12345678The nodes that do not belong to a minimum node cover form a maximumindependent set. This is the largest possible set of nodes such that no twonodes in the set are connected with an edge. Once again, nding a maximumindependent set in a general graph is a NP-hard problem, but in a bipartite graphwe can use Konigs theorem to solve the problem efciently. In the example graph,the maximum independent set is as follows:1234567820.4 Path coversA path cover is a set of paths in a graph such that each node of the graphbelongs to at least one path. It turns out that in directed, acyclic graphs, we canreduce the problem of nding a minimum path cover to the problem of nding amaximum ow in another graph.190",
            "Node-disjoint path coverIn a node-disjoint path cover, each node belongs to exactly one path. As anexample, consider the following graph:1 2 3 45 6 7A minimum node-disjoint path cover of this graph consists of three paths. Forexample, we can choose the following paths:1 2 3 45 6 7Note that one of the paths only contains node 2, so it is possible that a pathdoes not contain any edges.We can nd a minimum node-disjoint path cover by constructing a matchinggraph where each node of the original graph is represented by two nodes: a leftnode and a right node. There is an edge from a left node to a right node if thereis such an edge in the original graph. In addition, the matching graph contains asource and a sink, and there are edges from the source to all left nodes and fromall right nodes to the sink.A maximum matching in the resulting graph corresponds to a minimum node-disjoint path cover in the original graph. For example, the following matchinggraph for the above graph contains a maximum matching of size 4:12345671234567Each edge in the maximum matching of the matching graph corresponds toan edge in the minimum node-disjoint path cover of the original graph. Thus, thesize of the minimum node-disjoint path cover is n  c, where n is the number ofnodes in the original graph and c is the size of the maximum matching.191",
            "General path coverA general path cover is a path cover where a node can belong to more thanone path. A minimum general path cover may be smaller than a minimumnode-disjoint path cover, because a node can be used multiple times in paths.Consider again the following graph:1 2 3 45 6 7The minimum general path cover of this graph consists of two paths. Forexample, the rst path may be as follows:1 2 3 45 6 7And the second path may be as follows:1 2 3 45 6 7A minimum general path cover can be found almost like a minimum node-disjoint path cover. It sufces to add some new edges to the matching graphso that there is an edge a  b always when there is a path from a to b in theoriginal graph (possibly through several edges).The matching graph for the above graph is as follows:12345671234567192",
            "Dilworths theoremAn antichain is a set of nodes of a graph such that there is no path from anynode to another node using the edges of the graph. Dilworths theoremstatesthat in a directed acyclic graph, the size of a minimum general path cover equalsthe size of a maximum antichain.For example, nodes 3 and 7 form an antichain in the following graph:1 2 3 45 6 7This is a maximum antichain, because it is not possible to construct anyantichain that would contain three nodes. We have seen before that the size of aminimum general path cover of this graph consists of two paths.193",
            "194",
            "Part IIIAdvanced topics195",
            "",
            "Chapter 21Number theoryNumber theory is a branch of mathematics that studies integers. Numbertheory is a fascinating eld, because many questions involving integers are verydifcult to solve even if they seem simple at rst glance.As an example, consider the following equation:x3 + y3 + z3 = 33It is easy to nd three real numbers x, y and z that satisfy the equation. Forexample, we can choosex = 3,y =33,z =33.However, it is an open problem in number theory if there are any three integersx, y and z that would satisfy the equation [6].In this chapter, we will focus on basic concepts and algorithms in numbertheory. Throughout the chapter, we will assume that all numbers are integers, ifnot otherwise stated.21.1 Primes and factorsA number a is called a factor or a divisor of a number b if a divides b. If a is afactor of b, we write a | b, and otherwise we write a  b. For example, the factorsof 24 are 1, 2, 3, 4, 6, 8, 12 and 24.A number n > 1 is a prime if its only positive factors are 1 andn. For example,7, 19 and 41 are primes, but 35 is not a prime, because 57 = 35. For every numbern > 1, there is a unique prime factorizationn = p11 p22  pkk ,where p1, p2,..., pk are distinct primes and 1,2,..., k are positive numbers.For example, the prime factorization for 84 is84 = 22 31 71.197",
            "The number of factors of a number n is(n) =ki=1(i +1),because for each prime pi, there are i +1 ways to choose how many times itappears in the factor. For example, the number of factors of 84 is(84) = 322 = 12.The factors are 1, 2, 3, 4, 6, 7, 12, 14, 21, 28, 42 and 84.The sum of factors of n is(n) =ki=1(1+ pi +... + pii ) =ki=1pai+1i 1pi 1 ,where the latter formula is based on the geometric progression formula. Forexample, the sum of factors of 84 is(84) = 23 121  32 131  72 171 = 748 = 224.The product of factors of n is(n) = n(n)/2,because we can form (n)/2 pairs from the factors, each with product n. Forexample, the factors of 84 produce the pairs 1 84, 2 42, 3 28, etc., and theproduct of the factors is (84) = 846 = 351298031616.A number n is called a perfect number if n = (n)n, i.e., n equals the sumof its factors between 1 and n 1. For example, 28 is a perfect number, because28 = 1+2+4+7+14.Number of primesIt is easy to show that there is an innite number of primes. If the number ofprimes would be nite, we could construct a set P = {p1, p2,..., pn} that wouldcontain all the primes. For example, p1 = 2, p2 = 3, p3 = 5, and so on. However,using P, we could form a new primep1 p2  pn +1that is larger than all elements in P. This is a contradiction, and the number ofprimes has to be innite.Density of primesThe density of primes means how often there are primes among the numbers.Let (n) denote the number of primes between 1 and n. For example, (10) = 4,because there are 4 primes between 1 and 10: 2, 3, 5 and 7.It is possible to show that(n)  nlnn,which means that primes are quite frequent. For example, the number of primesbetween 1 and 106 is (106) = 78498, and 106/ln10 6  72382.198",
            "ConjecturesThere are many conjectures involving primes. Most people think that the con-jectures are true, but nobody has been able to prove them. For example, thefollowing conjectures are famous: Goldbachs conjecture: Each even integer n > 2 can be represented as asum n = a +b so that both a and b are primes. Twin prime conjecture: There is an innite number of pairs of the form{p, p +2}, where both p and p +2 are primes. Legendres conjecture: There is always a prime between numbers n2and (n +1)2, where n is any positive integer.Basic algorithmsIf a number n is not prime, it can be represented as a product a b, where a  nor b  n, so it certainly has a factor between 2 andn. Using this observation,we can both test if a number is prime and nd the prime factorization of a numberin O(n) time.The following function prime checks if the given number n is prime. Thefunction attempts to divide n by all numbers between 2 and n, and if none ofthem divides n, then n is prime.bool prime(int n) {if (n < 2) return false;for (int x = 2; x*x <= n; x++) {if (n%x == 0) return false;}return true;}The following function factors constructs a vector that contains the prime factor-ization of n. The function divides n by its prime factors, and adds them to thevector. The process ends when the remaining number n has no factors between 2and n. If n > 1, it is prime and the last factor.vector<int> factors(int n) {vector<int> f;for (int x = 2; x*x <= n; x++) {while (n%x == 0) {f.push_back(x);n /= x;}}if (n > 1) f.push_back(n);return f;}199",
            "Note that each prime factor appears in the vector as many times as it dividesthe number. For example, 24= 23 3, so the result of the function is [2,2,2,3].Sieve of EratosthenesThe sieve of Eratosthenes is a preprocessing algorithm that builds an arrayusing which we can efciently check if a given number between 2... n is primeand, if it is not, nd one prime factor of the number.The algorithm builds an array sieve whose positions 2,3,..., n are used. Thevalue sieve[k] = 0 means that k is prime, and the value sieve[k] = 0 means thatk is not a prime and one of its prime factors is sieve[k].The algorithm iterates through the numbers 2... n one by one. Always when anew prime x is found, the algorithm records that the multiples of x (2x,3x,4x,... )are not primes, because the number x divides them.For example, if n = 20, the array is as follows:0 0 2 0 3 0 2 3 5 0 3 0 7 5 2 0 3 0 52 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20The following code implements the sieve of Eratosthenes. The code assumesthat each element of sieve is initially zero.for (int x = 2; x <= n; x++) {if (sieve[x]) continue;for (int u = 2*x; u <= n; u += x) {sieve[u] = x;}}The inner loop of the algorithm is executedn/x times for each value of x. Thus,an upper bound for the running time of the algorithm is the harmonic sumnx=2n/x = n/2+n/3+n/4++ n/n = O(nlogn).In fact, the algorithm is more efcient, because the inner loop will be executedonly if the number x is prime. It can be shown that the running time of thealgorithm is only O(nloglog n), a complexity very near to O(n).Euclids algorithmThe greatest common divisor of numbers a and b, gcd(a,b), is the greatestnumber that divides both a and b, and the least common multiple of a and b,lcm(a,b), is the smallest number that is divisible by both a and b. For example,gcd(24,36) = 12 and lcm(24,36) = 72.The greatest common divisor and the least common multiple are connected asfollows:lcm(a,b) = abgcd(a,b)200",
            "Euclids algorithm1 provides an efcient way to nd the greatest commondivisor of two numbers. The algorithm is based on the following formula:gcd(a,b) ={a b = 0gcd(b,a mod b) b = 0For example,gcd(24,36) = gcd(36,24) = gcd(24,12) = gcd(12,0) = 12.The algorithm can be implemented as follows:int gcd(int a, int b) {if (b == 0) return a;return gcd(b, a%b);}It can be shown that Euclids algorithm works in O(logn) time, where n =min(a,b). The worst case for the algorithm is the case when a and b are consecu-tive Fibonacci numbers. For example,gcd(13,8) = gcd(8,5) = gcd(5,3) = gcd(3,2) = gcd(2,1) = gcd(1,0) = 1.Eulers totient functionNumbers a and b are coprime if gcd(a,b) = 1. Eulers totient function(n)gives the number of coprime numbers to n between 1 and n. For example,(12) = 4, because 1, 5, 7 and 11 are coprime to 12.The value of (n) can be calculated from the prime factorization of n usingthe formula(n) =ki=1pi1i (pi 1).For example, (12) = 21 (21)30 (31) = 4. Note that (n) = n 1 if n is prime.21.2 Modular arithmeticIn modular arithmetic, the set of numbers is limited so that only numbers0,1,2,..., m 1 are used, where m is a constant. Each number x is representedby the number x mod m: the remainder after dividing x by m. For example, ifm = 17, then 75 is represented by 75 mod 17 = 7.Often we can take remainders before doing calculations. In particular, thefollowing formulas hold:(x + y) mod m = (x mod m + y mod m) mod m(x  y) mod m = (x mod m  y mod m) mod m(x  y) mod m = (x mod m  y mod m) mod mxn mod m = (x mod m)n mod m1Euclid was a Greek mathematician who lived in about 300 BC. This is perhaps the rstknown algorithm in history.201",
            "Modular exponentiationThere is often need to efciently calculate the value of xn mod m. This can bedone in O(logn) time using the following recursion:xn =1 n = 0xn/2  xn/2 n is evenxn1  x n is oddIt is important that in the case of an even n, the value of xn/2 is calculatedonly once. This guarantees that the time complexity of the algorithm is O(logn),because n is always halved when it is even.The following function calculates the value of xn mod m:int modpow(int x, int n, int m) {if (n == 0) return 1%m;long long u = modpow(x,n/2,m);u = (u*u)%m;if (n%2 == 1) u = (u*x)%m;return u;}Fermats theorem and Eulers theoremFermats theoremstates thatxm1 mod m = 1when m is prime and x and m are coprime. This also yieldsxk mod m = xk mod (m1) mod m.More generally, Eulers theoremstates thatx(m) mod m = 1when x and m are coprime. Fermats theorem follows from Eulers theorem,because if m is a prime, then (m) = m 1.Modular inverseThe inverse of x modulo m is a number x1 such thatxx1 mod m = 1.For example, if x = 6 and m = 17, then x1 = 3, because 63 mod 17 = 1.Using modular inverses, we can divide numbers modulo m, because divisionby x corresponds to multiplication by x1. For example, to evaluate the value202",
            "of 36/6 mod 17, we can use the formula 23 mod 17, because 36 mod 17 = 2 and61 mod 17 = 3.However, a modular inverse does not always exist. For example, if x = 2 andm = 4, the equationxx1 mod m = 1cannot be solved, because all multiples of 2 are even and the remainder can neverbe 1 when m = 4. It turns out that the value of x1 mod m can be calculatedexactly when x and m are coprime.If a modular inverse exists, it can be calculated using the formulax1 = x(m)1.If m is prime, the formula becomesx1 = xm2.For example,61 mod 17 = 6172 mod 17 = 3.This formula allows us to efciently calculate modular inverses using themodular exponentation algorithm. The formula can be derived using Eulerstheorem. First, the modular inverse should satisfy the following equation:xx1 mod m = 1.On the other hand, according to Eulers theorem,x(m) mod m = xx(m)1 mod m = 1,so the numbers x1 and x(m)1 are equal.Computer arithmeticIn programming, unsigned integers are represented modulo 2k, where k is thenumber of bits of the data type. A usual consequence of this is that a numberwraps around if it becomes too large.For example, in C++, numbers of type unsigned int are represented mod-ulo 232. The following code declares an unsigned int variable whose value is123456789. After this, the value will be multiplied by itself, and the result is1234567892 mod 232 = 2537071545.unsigned int x = 123456789;cout << x*x << \"\\n\"; // 2537071545203",
            "21.3 Solving equationsDiophantine equationsA Diophantine equation is an equation of the formax +by = c,where a, b and c are constants and the values of x and y should be found. Eachnumber in the equation has to be an integer. For example, one solution for theequation 5x +2y = 11 is x = 3 and y = 2.We can efciently solve a Diophantine equation by using Euclids algorithm.It turns out that we can extend Euclids algorithm so that it will nd numbersxand y that satisfy the following equation:ax +by = gcd(a,b)A Diophantine equation can be solved if c is divisible by gcd(a,b), and other-wise it cannot be solved.As an example, let us nd numbersx and y that satisfy the following equation:39x +15y = 12The equation can be solved, because gcd(39,15) = 3 and 3 | 12. When Euclidsalgorithm calculates the greatest common divisor of 39 and 15, it produces thefollowing sequence of function calls:gcd(39,15) = gcd(15,9) = gcd(9,6) = gcd(6,3) = gcd(3,0) = 3This corresponds to the following equations:39215 = 91519 = 6916 = 3Using these equations, we can derive392+15(5) = 3and by multiplying this by 4, the result is398+15(20) = 12,so a solution to the equation is x = 8 and y = 20.A solution to a Diophantine equation is not unique, because we can form aninnite number of solutions if we know one solution. If a pair ( x, y) is a solution,then also all pairs(x + kbgcd(a,b), y kagcd(a,b))are solutions, where k is any integer.204",
            "Chinese remainder theoremThe Chinese remainder theorem solves a group of equations of the formx = a1 mod m1x = a2 mod m2x = an mod mnwhere all pairs of m1,m2,..., mn are coprime.Let x1m be the inverse of x modulo m, andXk = m1m2  mnmk.Using this notation, a solution to the equations isx = a1 X1 X11m1 +a2 X2 X21m2 ++ an Xn Xn1mn .In this solution, for each k = 1,2,..., n,ak Xk Xk1mk mod mk = ak,becauseXk Xk1mk mod mk = 1.Since all other terms in the sum are divisible by mk, they have no effect on theremainder, and x mod mk = ak.For example, a solution forx = 3 mod 5x = 4 mod 7x = 2 mod 3is3211+4151+2352 = 263.Once we have found a solution x, we can create an innite number of othersolutions, because all numbers of the formx +m1m2  mnare solutions.21.4 Other resultsLagranges theoremLagranges theoremstates that every positive integer can be represented as asum of four squares, i.e., a2 +b2 + c2 +d2. For example, the number 123 can berepresented as the sum 82 +52 +52 +32.205",
            "Zeckendorfs theoremZeckendorfs theoremstates that every positive integer has a unique repre-sentation as a sum of Fibonacci numbers such that no two numbers are equal orconsecutive Fibonacci numbers. For example, the number 74 can be representedas the sum 55+13+5+1.Pythagorean triplesA Pythagorean triple is a triple (a,b, c) that satises the Pythagorean theorema2 +b2 = c2, which means that there is a right triangle with side lengths a, b andc. For example, (3,4,5) is a Pythagorean triple.If (a,b, c) is a Pythagorean triple, all triples of the form (ka,kb,kc) are alsoPythagorean triples where k > 1. A Pythagorean triple is primitive if a, b andc are coprime, and all Pythagorean triples can be constructed from primitivetriples using a multiplier k.Euclids formulacan be used to produce all primitive Pythagorean triples.Each such triple is of the form(n2 m2,2nm,n2 +m2),where 0 < m < n, n and m are coprime and at least one of n and m is even. Forexample, when m = 1 and n = 2, the formula produces the smallest Pythagoreantriple(22 12,221,22 +12) = (3,4,5).Wilsons theoremWilsons theoremstates that a number n is prime exactly when(n 1)! mod n = n 1.For example, the number 11 is prime, because10! mod 11 = 10,and the number 12 is not prime, because11! mod 12 = 0 = 11.Hence, Wilsons theorem can be used to nd out whether a number is prime.However, in practice, the theorem cannot be applied to large values of n, becauseit is difcult to calculate values of (n 1)! when n is large.206",
            "Chapter 22CombinatoricsCombinatorics studies methods for counting combinations of objects. Usually,the goal is to nd a way to count the combinations efciently without generatingeach combination separately.As an example, consider the problem of counting the number of ways torepresent an integer n as a sum of positive integers. For example, there are 8representations for 4: 1 +1+1+1 1 +1+2 1 +2+1 2 +1+1 2 +2 3 +1 1 +3 4A combinatorial problem can often be solved using a recursive function. In thisproblem, we can dene a function f (n) that gives the number of representationsfor n. For example, f (4) = 8 according to the above example. The values of thefunction can be recursively calculated as follows:f (n) ={1 n = 0f (0)+ f (1)++ f (n 1) n > 0The base case is f (0) = 1, because the empty sum represents the number 0. Then,if n > 0, we consider all ways to choose the rst number of the sum. If the rstnumber is k, there are f (nk) representations for the remaining part of the sum.Thus, we calculate the sum of all values of the form f (n k) where k < n.The rst values for the function are:f (0) = 1f (1) = 1f (2) = 2f (3) = 4f (4) = 8Sometimes, a recursive formula can be replaced with a closed-form formula.In this problem,f (n) = 2n1,207",
            "which is based on the fact that there are n1 possible positions for +-signs in thesum and we can choose any subset of them.22.1 Binomial coefcientsThe binomial coefcient(nk)equals the number of ways we can choose a subsetof k elements from a set of n elements. For example,(53)= 10, because the set{1,2,3,4,5} has 10 subsets of 3 elements:{1,2,3},{1,2,4},{1,2,5},{1,3,4},{1,3,5},{1,4,5},{2,3,4},{2,3,5},{2,4,5},{3,4,5}Formula 1Binomial coefcients can be recursively calculated as follows:(nk)=(n 1k 1)+(n 1k)The idea is to x an element x in the set. If x is included in the subset, wehave to choose k 1 elements from n 1 elements, and if x is not included in thesubset, we have to choose k elements from n 1 elements.The base cases for the recursion are(n0)=(nn)= 1,because there is always exactly one way to construct an empty subset and asubset that contains all the elements.Formula 2Another way to calculate binomial coefcients is as follows:(nk)= n!k!(n k)!.There are n! permutations of n elements. We go through all permutationsand always include the rst k elements of the permutation in the subset. Sincethe order of the elements in the subset and outside the subset does not matter,the result is divided by k! and (n k)!PropertiesFor binomial coefcients, (nk)=(nn k),208",
            "because we actually divide a set of n elements into two subsets: the rst containsk elements and the second contains n k elements.The sum of binomial coefcients is(n0)+(n1)+(n2)+... +(nn)= 2n.The reason for the name binomial coefcient can be seen when the binomial(a +b) is raised to the nth power:(a +b)n =(n0)anb0 +(n1)an1b1 +... +(nn 1)a1bn1 +(nn)a0bn.Binomial coefcients also appear in Pascals trianglewhere each valueequals the sum of two above values:11 11 2 11 3 3 11 4 6 4 1... ... ... ... ...Boxes and ballsBoxes and balls is a useful model, where we count the ways to placek balls in nboxes. Let us consider three scenarios:Scenario 1: Each box can contain at most one ball. For example, when n = 5and k = 2, there are 10 solutions:In this scenario, the answer is directly the binomial coefcient(nk).Scenario 2: A box can contain multiple balls. For example, when n = 5 andk = 2, there are 15 solutions:209",
            "The process of placing the balls in the boxes can be represented as a stringthat consists of symbols o and . Initially, assume that we are standing atthe leftmost box. The symbol o means that we place a ball in the current box,and the symbol  means that we move to the next box to the right.Using this notation, each solution is a string that contains k times the symbolo andn 1 times the symbol . For example, the upper-right solution in theabove picture corresponds to the string  o  o . Thus, the number ofsolutions is(k+n1k).Scenario 3: Each box may contain at most one ball, and in addition, no twoadjacent boxes may both contain a ball. For example, when n = 5 and k = 2, thereare 6 solutions:In this scenario, we can assume that k balls are initially placed in boxes andthere is an empty box between each two adjacent boxes. The remaining task isto choose the positions for the remaining empty boxes. There are n 2k +1 suchboxes and k +1 positions for them. Thus, using the formula of scenario 2, thenumber of solutions is(nk+1n2k+1).Multinomial coecientsThe multinomial coefcient(nk1,k2,..., km)= n!k1!k2! km!,equals the number of ways we can divide n elements into subsets of sizesk1,k2,..., km, where k1 +k2 ++ km = n. Multinomial coefcients can be seen asa generalization of binomial cofcients; if m = 2, the above formula correspondsto the binomial coefcient formula.22.2 Catalan numbersThe Catalan number Cn equals the number of valid parenthesis expressionsthat consist of n left parentheses and n right parentheses.For example, C3 = 5, because we can construct the following parenthesisexpressions using three left and right parentheses: ()()() (())() ()(()) ((())) (()())210",
            "Parenthesis expressionsWhat is exactly a valid parenthesis expression? The following rules preciselydene all valid parenthesis expressions: An empty parenthesis expression is valid. If an expression A is valid, then also the expression (A) is valid. If expressions A and B are valid, then also the expression AB is valid.Another way to characterize valid parenthesis expressions is that if we chooseany prex of such an expression, it has to contain at least as many left parenthe-ses as right parentheses. In addition, the complete expression has to contain anequal number of left and right parentheses.Formula 1Catalan numbers can be calculated using the formulaCn =n1i=0CiCni1.The sum goes through the ways to divide the expression into two parts suchthat both parts are valid expressions and the rst part is as short as possible butnot empty. For any i, the rst part contains i +1 pairs of parentheses and thenumber of expressions is the product of the following values: Ci: the number of ways to construct an expression using the parentheses ofthe rst part, not counting the outermost parentheses Cni1: the number of ways to construct an expression using the parenthe-ses of the second partThe base case is C0 = 1, because we can construct an empty parenthesisexpression using zero pairs of parentheses.Formula 2Catalan numbers can also be calculated using binomial coefcients:Cn = 1n +1(2nn)The formula can be explained as follows:There are a total of(2nn)ways to construct a (not necessarily valid) parenthesisexpression that contains n left parentheses and n right parentheses. Let uscalculate the number of such expressions that are not valid.If a parenthesis expression is not valid, it has to contain a prex wherethe number of right parentheses exceeds the number of left parentheses. The211",
            "idea is to reverse each parenthesis that belongs to such a prex. For example,the expression ())()( contains a prex ()), and after reversing the prex, theexpression becomes )((()(.The resulting expression consists of n +1 left parentheses and n 1 rightparentheses. The number of such expressions is( 2nn+1), which equals the numberof non-valid parenthesis expressions. Thus, the number of valid parenthesisexpressions can be calculated using the formula(2nn)(2nn +1)=(2nn) nn +1(2nn)= 1n +1(2nn).Counting treesCatalan numbers are also related to trees: there are Cn binary trees of n nodes there are Cn1 rooted trees of n nodesFor example, for C3 = 5, the binary trees areand the rooted trees are22.3 Inclusion-exclusionInclusion-exclusion is a technique that can be used for counting the size of aunion of sets when the sizes of the intersections are known, and vice versa. Asimple example of the technique is the formula|A B| = |A|+| B|| A B|,where A and B are sets and |X| denotes the size of X. The formula can beillustrated as follows:A BA B212",
            "Our goal is to calculate the size of the union A B that corresponds to thearea of the region that belongs to at least one circle. The picture shows that wecan calculate the area of A B by rst summing the areas of A and B and thensubtracting the area of A B.The same idea can be applied when the number of sets is larger. When thereare three sets, the inclusion-exclusion formula is|A B C| = |A|+| B|+| C|| A B|| A C|| B C|+| A B C|and the corresponding picture isA BCA BA C B CA B CIn the general case, the size of the union X1  X2   Xn can be calcu-lated by going through all possible intersections that contain some of the setsX1, X2,..., Xn. If the intersection contains an odd number of sets, its size is addedto the answer, and otherwise its size is subtracted from the answer.Note that there are similar formulas for calculating the size of an intersectionfrom the sizes of unions. For example,|A B| = |A|+| B|| A B|and|A B C| = |A|+| B|+| C|| A B|| A C|| B C|+| A B C|.DerangementsAs an example, let us count the number ofderangements of elements {1,2,..., n},i.e., permutations where no element remains in its original place. For example,when n = 3, there are two derangements: (2,3,1) and (3,1,2).One approach for solving the problem is to use inclusion-exclusion. Let Xk bethe set of permutations that contain the element k at position k. For example,when n = 3, the sets are as follows:X1 = {(1,2,3),(1,3,2)}X2 = {(1,2,3),(3,2,1)}X3 = {(1,2,3),(2,1,3)}Using these sets, the number of derangements equalsn!| X1  X2  Xn|,213",
            "so it sufces to calculate the size of the union. Using inclusion-exclusion, thisreduces to calculating sizes of intersections which can be done efciently. Forexample, when n = 3, the size of |X1  X2  X3| is|X1|+| X2|+| X3|| X1  X2|| X1  X3|| X2  X3|+| X1  X2  X3|= 2+2+2111+1= 4,so the number of solutions is 3!4 = 2.It turns out that the problem can also be solved without using inclusion-exclusion. Let f (n) denote the number of derangements for {1,2,..., n}. We canuse the following recursive formula:f (n) =0 n = 11 n = 2(n 1)(f (n 2)+ f (n 1)) n > 2The formula can be derived by considering the possibilities how the element 1changes in the derangement. There are n 1 ways to choose an element x thatreplaces the element 1. In each such choice, there are two options:Option 1: We also replace the element x with the element 1. After this, theremaining task is to construct a derangement of n 2 elements.Option 2: We replace the element x with some other element than 1. Now wehave to construct a derangement of n 1 element, because we cannot replace theelement x with the element 1, and all other elements must be changed.22.4 Burnsides lemmaBurnsides lemmacan be used to count the number of combinations so thatonly one representative is counted for each group of symmetric combinations.Burnsides lemma states that the number of combinations isnk=1c(k)n ,where there are n ways to change the position of a combination, and there arec(k) combinations that remain unchanged when the kth way is applied.As an example, let us calculate the number of necklaces of n pearls, whereeach pearl has m possible colors. Two necklaces are symmetric if they are similarafter rotating them. For example, the necklacehas the following symmetric necklaces:214",
            "There are n ways to change the position of a necklace, because we can rotate it0,1,..., n1 steps clockwise. If the number of steps is 0, all mn necklaces remainthe same, and if the number of steps is 1, only the m necklaces where each pearlhas the same color remain the same.More generally, when the number of steps is k, a total ofmgcd(k,n)necklaces remain the same, where gcd(k,n) is the greatest common divisor of kand n. The reason for this is that blocks of pearls of size gcd(k,n) will replaceeach other. Thus, according to Burnsides lemma, the number of necklaces isn1i=0mgcd(i,n)n .For example, the number of necklaces of length 4 with 3 colors is34 +3+32 +34 = 24.22.5 Cayleys formulaCayleys formulastates that there are nn2 labeled trees that contain n nodes.The nodes are labeled 1,2,..., n, and two trees are different if either their struc-ture or labeling is different.For example, when n = 4, the number of labeled trees is 442 = 16:12 3 421 3 431 2 441 2 31 2 3 4 1 2 4 3 1 3 2 41 3 4 2 1 4 2 3 1 4 3 22 1 3 4 2 1 4 3 2 3 1 42 4 1 3 3 1 2 4 3 2 1 4Next we will see how Cayleys formula can be derived using Prfer codes.215",
            "Prfer codeA Prfer code is a sequence of n 2 numbers that describes a labeled tree. Thecode is constructed by following a process that removes n2 leaves from the tree.At each step, the leaf with the smallest label is removed, and the label of its onlyneighbor is added to the code.For example, let us calculate the Prfer code of the following graph:1 23 45First we remove node 1 and add node 4 to the code:23 45Then we remove node 3 and add node 4 to the code:245Finally we remove node 4 and add node 2 to the code:25Thus, the Prfer code of the graph is [4,4,2].We can construct a Prfer code for any tree, and more importantly, the originaltree can be reconstructed from a Prfer code. Hence, the number of labeled treesof n nodes equals nn2, the number of Prfer codes of size n.216",
            "Chapter 23MatricesA matrix is a mathematical concept that corresponds to a two-dimensional arrayin programming. For example,A =6 13 7 47 0 8 29 5 4 18is a matrix of size 34, i.e., it has 3 rows and 4 columns. The notation [i, j] refersto the element in row i and column j in a matrix. For example, in the abovematrix, A[2,3] = 8 and A[3,1] = 9.A special case of a matrix is a vector that is a one-dimensional matrix of sizen 1. For example,V =475is a vector that contains three elements.The transpose AT of a matrix A is obtained when the rows and columns ofA are swapped, i.e., AT[i, j] = A[ j, i]:AT =6 7 913 0 57 8 44 2 18A matrix is a square matrix if it has the same number of rows and columns.For example, the following matrix is a square matrix:S =3 12 45 9 150 2 423.1 OperationsThe sum A +B of matrices A and B is dened if the matrices are of the samesize. The result is a matrix where each element is the sum of the correspondingelements in A and B.217",
            "For example,[6 1 43 9 2]+[4 9 38 1 3]=[6+4 1 +9 4 +33+8 9 +1 2 +3]=[10 10 711 10 5].Multiplying a matrix A by a value x means that each element of A is multi-plied by x. For example,2[6 1 43 9 2]=[26 2 1 2 423 2 9 2 2]=[12 2 86 18 4].Matrix multiplicationThe product AB of matrices A and B is dened if A is of size a n and B is ofsize n b, i.e., the width of A equals the height of B. The result is a matrix ofsize a b whose elements are calculated using the formulaAB[i, j] =nk=1A[i,k]B[k, j].The idea is that each element of AB is a sum of products of elements of A andB according to the following picture:A ABBFor example,1 43 98 6[1 62 9]=11+42 1 6+4931+92 3 6+9981+62 8 6+69=9 4221 9920 102.Matrix multiplication is associative, so A(BC) = (AB)C holds, but it is notcommutative, so AB = BA does not usually hold.An identity matrix is a square matrix where each element on the diagonalis 1 and all other elements are 0. For example, the following matrix is the 33identity matrix:I =1 0 00 1 00 0 1218",
            "Multiplying a matrix by an identity matrix does not change it. For example,1 0 00 1 00 0 11 43 98 6=1 43 98 6 and1 43 98 6[1 00 1]=1 43 98 6.Using a straightforward algorithm, we can calculate the product of two n nmatrices in O(n3) time. There are also more efcient algorithms for matrixmultiplication1, but they are mostly of theoretical interest and such algorithmsare not necessary in competitive programming.Matrix powerThe power Ak of a matrix A is dened if A is a square matrix. The denition isbased on matrix multiplication:Ak = A  A  A  A  k timesFor example,[2 51 4]3=[2 51 4][2 51 4][2 51 4]=[48 16533 114].In addition, A0 is an identity matrix. For example,[2 51 4]0=[1 00 1].The matrix Ak can be efciently calculated in O(n3 logk) time using thealgorithm in Chapter 21.2. For example,[2 51 4]8=[2 51 4]4[2 51 4]4.DeterminantThe determinant det(A) of a matrix A is dened if A is a square matrix. IfA is of size 1 1, then det(A) = A[1,1]. The determinant of a larger matrix iscalculated recursively using the formuladet(A) =nj=1A[1, j]C[1, j],where C[i, j] is the cofactor of A at [i, j]. The cofactor is calculated using theformulaC[i, j] = (1)i+j det(M[i, j]),1The rst such algorithm was Strassens algorithm, published in 1969 [ 63], whose timecomplexity is O(n2.80735); the best current algorithm [27] works in O(n2.37286) time.219",
            "where M[i, j] is obtained by removing row i and column j from A. Due to thecoefcient (1)i+j in the cofactor, every other determinant is positive and negative.For example,det([3 41 6]) = 3641 = 14anddet(2 4 35 1 67 2 4) = 2det([1 62 4])4det([5 67 4])+3det([5 17 2]) = 81.The determinant of A tells us whether there is an inverse matrix A1 suchthat AA1 = I, where I is an identity matrix. It turns out that A1 exists exactlywhen det(A) = 0, and it can be calculated using the formulaA1[i, j] = C[ j, i]det (A).For example,2 4 35 1 67 2 4  A 1818 10 2122 13 33 24 18  A1=1 0 00 1 00 0 1  I.23.2 Linear recurrencesA linear recurrence is a function f (n) whose initial values aref (0), f (1),..., f (k1) and larger values are calculated recursively using the formulaf (n) = c1 f (n 1)+ c2 f (n 2)+... + ck f (n k),where c1, c2,..., ck are constant coefcients.Dynamic programming can be used to calculate any value of f (n) in O(kn)time by calculating all values of f (0), f (1),..., f (n) one after another. However,if k is small, it is possible to calculate f (n) much more efciently in O(k3 logn)time using matrix operations.Fibonacci numbersA simple example of a linear recurrence is the following function that denes theFibonacci numbers:f (0) = 0f (1) = 1f (n) = f (n 1)+ f (n 2)In this case, k = 2 and c1 = c2 = 1.220",
            "To efciently calculate Fibonacci numbers, we represent the Fibonacci formulaas a square matrix X of size 22, for which the following holds:X [ f (i)f (i +1)]=[f (i +1)f (i +2)]Thus, values f (i) and f (i +1) are given as input forX, and X calculates valuesf (i +1) and f (i +2) from them. It turns out that such a matrix isX =[0 11 1].For example,[0 11 1][f (5)f (6)]=[0 11 1][58]=[ 813]=[f (6)f (7)].Thus, we can calculate f (n) using the formula[ f (n)f (n +1)]= X n [f (0)f (1)]=[0 11 1]n[01].The value of X n can be calculated in O(logn) time, so the value of f (n) can alsobe calculated in O(logn) time.General caseLet us now consider the general case where f (n) is any linear recurrence. Again,our goal is to construct a matrix X for whichX f (i)f (i +1)...f (i +k 1)=f (i +1)f (i +2)...f (i +k).Such a matrix isX =0 1 0 0  00 0 1 0  00 0 0 1  0... ... ... ... ... ...0 0 0 0  1ck ck1 ck2 ck3  c1.In the rst k 1 rows, each element is 0 except that one element is 1. These rowsreplace f (i) with f (i +1), f (i +1) with f (i +2), and so on. The last row containsthe coefcients of the recurrence to calculate the new value f (i +k).Now, f (n) can be calculated in O(k3 logn) time using the formulaf (n)f (n +1)...f (n +k 1)= X n f (0)f (1)...f (k 1).221",
            "23.3 Graphs and matricesCounting pathsThe powers of an adjacency matrix of a graph have an interesting property. WhenV is an adjacency matrix of an unweighted graph, the matrix V n contains thenumbers of paths of n edges between the nodes in the graph.For example, for the graph142 35 6the adjacency matrix isV =0 0 0 1 0 01 0 0 0 1 10 1 0 0 0 00 1 0 0 0 00 0 0 0 0 00 0 1 0 1 0.Now, for example, the matrixV4 =0 0 1 1 1 02 0 0 0 2 20 2 0 0 0 00 2 0 0 0 00 0 0 0 0 00 0 1 1 1 0contains the numbers of paths of 4 edges between the nodes. For example,V4[2,5] = 2, because there are two paths of 4 edges from node 2 to node 5:2  1  4  2  5 and 2  6  3  2  5.Shortest pathsUsing a similar idea in a weighted graph, we can calculate for each pair of nodesthe minimum length of a path between them that contains exactly n edges. Tocalculate this, we have to dene matrix multiplication in a new way, so that wedo not calculate the numbers of paths but minimize the lengths of paths.222",
            "As an example, consider the following graph:142 35 64 12 41 2 32Let us construct an adjacency matrix where  means that an edge does notexist, and other values correspond to edge weights. The matrix isV =   4  2    1 2 4     1           3  2 .Instead of the formulaAB[i, j] =nk=1A[i,k]B[k, j]we now use the formulaAB[i, j] =nmink=1A[i,k]+B[k, j]for matrix multiplication, so we calculate a minimum instead of a sum, and asum of elements instead of a product. After this modication, matrix powerscorrespond to shortest paths in the graph.For example, asV4 =  10 11 9 9    8 9 11     8           12 13 11 ,we can conclude that the minimum length of a path of 4 edges from node 2 tonode 5 is 8. Such a path is 2  1  4  2  5.Kirchhos theoremKirchhoffs theoremprovides a way to calculate the number of spanning treesof a graph as a determinant of a special matrix. For example, the graph1 23 4223",
            "has three spanning trees:1 23 41 23 41 23 4To calculate the number of spanning trees, we construct a Laplacean matrix L,where L[i, i] is the degree of node i and L[i, j] = 1 if there is an edge betweennodes i and j, and otherwise L[i, j] = 0. The Laplacean matrix for the abovegraph is as follows:L =3 1 1 11 1 0 01 0 2 11 0 1 2It can be shown that the number of spanning trees equals the determinant ofa matrix that is obtained when we remove any row and any column from L. Forexample, if we remove the rst row and column, the result isdet(1 0 00 2 10 1 2) = 3.The determinant is always the same, regardless of which row and column weremove from L.Note that Cayleys formula in Chapter 22.5 is a special case of Kirchhoffstheorem, because in a complete graph of n nodesdet(n 1 1   11 n 1   1... ... ... ...1 1  n 1) = nn2.224",
            "Chapter 24ProbabilityA probability is a real number between 0 and 1 that indicates how probablean event is. If an event is certain to happen, its probability is 1, and if an eventis impossible, its probability is 0. The probability of an event is denoted P( )where the three dots describe the event.For example, when throwing a dice, the outcome is an integer between 1 and6, and the probability of each outcome is 1/6. For example, we can calculate thefollowing probabilities: P(the outcome is 4)= 1/6 P(the outcome is not 6)= 5/6 P(the outcome is even)= 1/224.1 CalculationTo calculate the probability of an event, we can either use combinatorics orsimulate the process that generates the event. As an example, let us calculatethe probability of drawing three cards with the same value from a shufed deckof cards (for example, 8, 8 and 8).Method 1We can calculate the probability using the formulanumber of desired outcomestotal number of outcomes .In this problem, the desired outcomes are those in which the value of eachcard is the same. There are 13(43)such outcomes, because there are 13 possibilitiesfor the value of the cards and(43)ways to choose 3 suits from 4 possible suits.There are a total of(523)outcomes, because we choose 3 cards from 52 cards.Thus, the probability of the event is13(43)(523) = 1425.225",
            "Method 2Another way to calculate the probability is to simulate the process that generatesthe event. In this example, we draw three cards, so the process consists of threesteps. We require that each step of the process is successful.Drawing the rst card certainly succeeds, because there are no restrictions.The second step succeeds with probability 3/51, because there are 51 cards leftand 3 of them have the same value as the rst card. In a similar way, the thirdstep succeeds with probability 2/50.The probability that the entire process succeeds is1 351  250 = 1425.24.2 EventsAn event in probability theory can be represented as a setA  X,where X contains all possible outcomes and A is a subset of outcomes. Forexample, when drawing a dice, the outcomes areX = {1,2,3,4,5,6}.Now, for example, the event the outcome is even corresponds to the setA = {2,4,6}.Each outcome x is assigned a probability p(x). Then, the probability P(A)of an event A can be calculated as a sum of probabilities of outcomes using theformulaP(A) =xAp(x).For example, when throwing a dice, p(x) = 1/6 for each outcome x, so the proba-bility of the event the outcome is even isp(2)+ p(4)+ p(6) = 1/2.The total probability of the outcomes in X must be 1, i.e., P(X) = 1.Since the events in probability theory are sets, we can manipulate them usingstandard set operations: The complement A means A does not happen. For example, whenthrowing a dice, the complement of A = {2,4,6} is A = {1,3,5}. The union A  B means A or B happen. For example, the union ofA = {2,5} and B = {4,5,6} is A B = {2,4,5,6}. The intersection A B means A and B happen. For example, the inter-section of A = {2,5} and B = {4,5,6} is A B = {5}.226",
            "ComplementThe probability of the complement A is calculated using the formulaP( A) = 1P(A).Sometimes, we can solve a problem easily using complements by solving theopposite problem. For example, the probability of getting at least one six whenthrowing a dice ten times is1(5/6)10.Here 5/6 is the probability that the outcome of a single throw is not six, and(5/6)10 is the probability that none of the ten throws is a six. The complement ofthis is the answer to the problem.UnionThe probability of the union A B is calculated using the formulaP(A B) = P(A)+P(B)P(A B).For example, when throwing a dice, the union of the eventsA = the outcome is evenandB = the outcome is less than 4isA B = the outcome is even or less than 4,and its probability isP(A B) = P(A)+P(B)P(A B) = 1/2+1/21/6 = 5/6.If the events A and B are disjoint, i.e., A B is empty, the probability of theevent A B is simplyP(A B) = P(A)+P(B).Conditional probabilityThe conditional probabilityP(A|B) = P(A B)P(B)is the probability of A assuming that B happens. Hence, when calculating theprobability of A, we only consider the outcomes that also belong to B.Using the previous sets,P(A|B) = 1/3,because the outcomes of B are {1,2,3}, and one of them is even. This is theprobability of an even outcome if we know that the outcome is between 1... 3.227",
            "IntersectionUsing conditional probability, the probability of the intersection A B can becalculated using the formulaP(A B) = P(A)P(B|A).Events A and B are independent ifP(A|B) = P(A) and P(B|A) = P(B),which means that the fact that B happens does not change the probability of A,and vice versa. In this case, the probability of the intersection isP(A B) = P(A)P(B).For example, when drawing a card from a deck, the eventsA = the suit is clubsandB = the value is fourare independent. Hence the eventA B = the card is the four of clubshappens with probabilityP(A B) = P(A)P(B) = 1/41/13 = 1/52.24.3 Random variablesA random variable is a value that is generated by a random process. Forexample, when throwing two dice, a possible random variable isX = the sum of the outcomes.For example, if the outcomes are [4,6] (meaning that we rst throw a four andthen a six), then the value of X is 10.We denote P(X = x) the probability that the value of a random variable Xis x. For example, when throwing two dice, P(X = 10) = 3/36, because the totalnumber of outcomes is 36 and there are three possible ways to obtain the sum 10:[4,6], [5,5] and [6,4].228",
            "Expected valueThe expected value E[X] indicates the average value of a random variable X.The expected value can be calculated as the sumxP(X = x)x,where x goes through all possible values of X.For example, when throwing a dice, the expected outcome is1/61+1/62+1/63+1/64+1/65+1/66 = 7/2.A useful property of expected values is linearity. It means that the sumE[X1 + X2 ++ Xn] always equals the sum E[X1] + E[X2] ++ E[Xn]. Thisformula holds even if random variables depend on each other.For example, when throwing two dice, the expected sum isE[X1 + X2] = E[X1]+E[X2] = 7/2+7/2 = 7.Let us now consider a problem where n balls are randomly placed in n boxes,and our task is to calculate the expected number of empty boxes. Each ball hasan equal probability to be placed in any of the boxes. For example, if n = 2, thepossibilities are as follows:In this case, the expected number of empty boxes is0+0+1+14 = 12.In the general case, the probability that a single box is empty is(n 1n)n,because no ball should be placed in it. Hence, using linearity, the expectednumber of empty boxes isn (n 1n)n.DistributionsThe distribution of a random variable X shows the probability of each valuethat X may have. The distribution consists of values P(X = x). For example,when throwing two dice, the distribution for their sum is:x 2 3 4 5 6 7 8 9 10 11 12P(X = x) 1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36229",
            "In a uniform distribution, the random variable X has n possible valuesa,a+1,..., b and the probability of each value is 1/n. For example, when throwinga dice, a = 1, b = 6 and P(X = x) = 1/6 for each value x.The expected value of X in a uniform distribution isE[X] = a +b2 .In a binomial distribution, n attempts are made and the probability thata single attempt succeeds is p. The random variable X counts the number ofsuccessful attempts, and the probability of a value x isP(X = x) = px(1 p)nx(nx),where px and (1 p)nx correspond to successful and unsuccessful attemps, and(nx)is the number of ways we can choose the order of the attempts.For example, when throwing a dice ten times, the probability of throwing asix exactly three times is (1/6)3(5/6)7(103).The expected value of X in a binomial distribution isE[X] = pn.In a geometric distribution, the probability that an attempt succeeds is p,and we continue until the rst success happens. The random variable X countsthe number of attempts needed, and the probability of a value x isP(X = x) = (1 p)x1 p,where (1 p)x1 corresponds to the unsuccessful attemps and p corresponds tothe rst successful attempt.For example, if we throw a dice until we throw a six, the probability that thenumber of throws is exactly 4 is (5/6)31/6.The expected value of X in a geometric distribution isE[X] = 1p.24.4 Markov chainsA Markov chain is a random process that consists of states and transitionsbetween them. For each state, we know the probabilities for moving to otherstates. A Markov chain can be represented as a graph whose nodes are statesand edges are transitions.As an example, consider a problem where we are in oor 1 in an n oorbuilding. At each step, we randomly walk either one oor up or one oor down,except that we always walk one oor up from oor 1 and one oor down fromoor n. What is the probability of being in oor m after k steps?In this problem, each oor of the building corresponds to a state in a Markovchain. For example, if n = 5, the graph is as follows:230",
            "1 2 3 4 51 1/2 1/2 1/211/21/21/2The probability distribution of a Markov chain is a vector [ p1, p2,..., pn],where pk is the probability that the current state is k. The formula p1 + p2 ++pn = 1 always holds.In the above scenario, the initial distribution is [1,0,0,0,0], because we alwaysbegin in oor 1. The next distribution is [0,1,0,0,0], because we can only movefrom oor 1 to oor 2. After this, we can either move one oor up or one oordown, so the next distribution is [1/2,0,1/2,0,0], and so on.An efcient way to simulate the walk in a Markov chain is to use dynamicprogramming. The idea is to maintain the probability distribution, and at eachstep go through all possibilities how we can move. Using this method, we cansimulate a walk of m steps in O(n2m) time.The transitions of a Markov chain can also be represented as a matrix thatupdates the probability distribution. In the above scenario, the matrix is0 1/2 0 0 01 0 1/2 0 00 1/2 0 1/2 00 0 1/2 0 10 0 0 1/2 0.When we multiply a probability distribution by this matrix, we get the newdistribution after moving one step. For example, we can move from the distribu-tion [1,0,0,0,0] to the distribution [0,1,0,0,0] as follows:0 1/2 0 0 01 0 1/2 0 00 1/2 0 1/2 00 0 1/2 0 10 0 0 1/2 010000=01000.By calculating matrix powers efciently, we can calculate the distributionafter m steps in O(n3 logm) time.24.5 Randomized algorithmsSometimes we can use randomness for solving a problem, even if the problem isnot related to probabilities. A randomized algorithm is an algorithm that isbased on randomness.A Monte Carlo algorithm is a randomized algorithm that may sometimesgive a wrong answer. For such an algorithm to be useful, the probability of awrong answer should be small.231",
            "A Las Vegas algorithm is a randomized algorithm that always gives thecorrect answer, but its running time varies randomly. The goal is to design analgorithm that is efcient with high probability.Next we will go through three example problems that can be solved usingrandomness.Order statisticsThe kth order statistic of an array is the element at position k after sorting thearray in increasing order. It is easy to calculate any order statistic in O(nlogn)time by rst sorting the array, but is it really needed to sort the entire array justto nd one element?It turns out that we can nd order statistics using a randomized algorithmwithout sorting the array. The algorithm, called quickselect1, is a Las Vegasalgorithm: its running time is usually O(n) but O(n2) in the worst case.The algorithm chooses a random element x of the array, and moves elementssmaller than x to the left part of the array, and all other elements to the rightpart of the array. This takes O(n) time when there are n elements. Assume thatthe left part contains a elements and the right part contains b elements. If a = k,element x is the kth order statistic. Otherwise, if a > k, we recursively nd thekth order statistic for the left part, and if a < k, we recursively nd the rth orderstatistic for the right part where r = k a. The search continues in a similar way,until the element has been found.When each element x is randomly chosen, the size of the array about halvesat each step, so the time complexity for nding the kth order statistic is aboutn +n/2+n/4+n/8+ <2n = O(n).The worst case of the algorithm requires stillO(n2) time, because it is possiblethat x is always chosen in such a way that it is one of the smallest or largestelements in the array and O(n) steps are needed. However, the probability forthis is so small that this never happens in practice.Verifying matrix multiplicationOur next problem is to verify if AB = C holds when A, B and C are matrices ofsize n  n. Of course, we can solve the problem by calculating the product ABagain (in O(n3) time using the basic algorithm), but one could hope that verifyingthe answer would by easier than to calculate it from scratch.It turns out that we can solve the problem using a Monte Carlo algorithm2whose time complexity is only O(n2). The idea is simple: we choose a randomvector X of n elements, and calculate the matrices ABX and CX . If ABX = CX ,we report that AB = C, and otherwise we report that AB = C.1In 1961, C. A. R. Hoare published two algorithms that are efcient on average: quicksort[36] for sorting arrays and quickselect [37] for nding order statistics.2R. M. Freivalds published this algorithm in 1977 [26], and it is sometimes called Freivaldsalgorithm.232",
            "The time complexity of the algorithm is O(n2), because we can calculatethe matrices ABX and CX in O(n2) time. We can calculate the matrix ABXefciently by using the representation A(BX ), so only two multiplications of nnand n 1 size matrices are needed.The drawback of the algorithm is that there is a small chance that thealgorithm makes a mistake when it reports that AB = C. For example,[6 81 3]=[8 73 2],but [6 81 3][36]=[8 73 2][36].However, in practice, the probability that the algorithm makes a mistake issmall, and we can decrease the probability by verifying the result using multiplerandom vectors X before reporting that AB = C.Graph coloringGiven a graph that contains n nodes and m edges, our task is to nd a way tocolor the nodes of the graph using two colors so that for at least m/2 edges, theendpoints have different colors. For example, in the graph1 23 45a valid coloring is as follows:1 23 45The above graph contains 7 edges, and for 5 of them, the endpoints have differentcolors, so the coloring is valid.The problem can be solved using a Las Vegas algorithm that generates randomcolorings until a valid coloring has been found. In a random coloring, the color ofeach node is independently chosen so that the probability of both colors is 1/2.In a random coloring, the probability that the endpoints of a single edge havedifferent colors is 1/2. Hence, the expected number of edges whose endpointshave different colors is m/2. Since it is expected that a random coloring is valid,we will quickly nd a valid coloring in practice.233",
            "234",
            "Chapter 25Game theoryIn this chapter, we will focus on two-player games that do not contain randomelements. Our goal is to nd a strategy that we can follow to win the game nomatter what the opponent does, if such a strategy exists.It turns out that there is a general strategy for such games, and we cananalyze the games using the nim theory. First, we will analyze simple gameswhere players remove sticks from heaps, and after this, we will generalize thestrategy used in those games to other games.25.1 Game statesLet us consider a game where there is initially a heap of n sticks. Players A andB move alternately, and player A begins. On each move, the player has to remove1, 2 or 3 sticks from the heap, and the player who removes the last stick wins thegame.For example, if n = 10, the game may proceed as follows: Player A removes 2 sticks (8 sticks left). Player B removes 3 sticks (5 sticks left). Player A removes 1 stick (4 sticks left). Player B removes 2 sticks (2 sticks left). Player A removes 2 sticks and wins.This game consists of states 0,1,2,..., n, where the number of the state corre-sponds to the number of sticks left.Winning and losing statesA winning state is a state where the player will win the game if they playoptimally, and alosing state is a state where the player will lose the game if theopponent plays optimally. It turns out that we can classify all states of a game sothat each state is either a winning state or a losing state.In the above game, state 0 is clearly a losing state, because the player cannotmake any moves. States 1, 2 and 3 are winning states, because we can remove 1,235",
            "2 or 3 sticks and win the game. State 4, in turn, is a losing state, because anymove leads to a state that is a winning state for the opponent.More generally, if there is a move that leads from the current state to a losingstate, the current state is a winning state, and otherwise the current state is alosing state. Using this observation, we can classify all states of a game startingwith losing states where there are no possible moves.The states 0... 15 of the above game can be classied as follows (W denotes awinning state and L denotes a losing state):L W W W L W W W L W W W L W W W0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15It is easy to analyze this game: a state k is a losing state if k is divisible by4, and otherwise it is a winning state. An optimal way to play the game is toalways choose a move after which the number of sticks in the heap is divisible by4. Finally, there are no sticks left and the opponent has lost.Of course, this strategy requires that the number of sticks is not divisible by4 when it is our move. If it is, there is nothing we can do, and the opponent willwin the game if they play optimally.State graphLet us now consider another stick game, where in each state k, it is allowed toremove any number x of sticks such that x is smaller than k and divides k. Forexample, in state 8 we may remove 1, 2 or 4 sticks, but in state 7 the only allowedmove is to remove 1 stick.The following picture shows the states 1 ... 9 of the game as a state graph,whose nodes are the states and edges are the moves between them:1 23456789The nal state in this game is always state 1, which is a losing state, becausethere are no valid moves. The classication of states 1 ... 9 is as follows:L W L W L W L W L1 2 3 4 5 6 7 8 9Surprisingly, in this game, all even-numbered states are winning states, andall odd-numbered states are losing states.236",
            "25.2 Nim gameThe nim game is a simple game that has an important role in game theory,because many other games can be played using the same strategy. First, we focuson nim, and then we generalize the strategy to other games.There are n heaps in nim, and each heap contains some number of sticks.The players move alternately, and on each turn, the player chooses a heap thatstill contains sticks and removes any number of sticks from it. The winner is theplayer who removes the last stick.The states in nim are of the form [x1, x2,..., xn], where xk denotes the numberof sticks in heap k. For example, [10,12,5] is a game where there are three heapswith 10, 12 and 5 sticks. The state [0,0,..., 0] is a losing state, because it is notpossible to remove any sticks, and this is always the nal state.AnalysisIt turns out that we can easily classify any nim state by calculating thenim sums = x1 x2  xn, where  is the xor operation1. The states whose nim sum is0 are losing states, and all other states are winning states. For example, the nimsum of [10,12,5] is 10125 = 3, so the state is a winning state.But how is the nim sum related to the nim game? We can explain this bylooking at how the nim sum changes when the nim state changes.Losing states: The nal state [0,0,..., 0] is a losing state, and its nim sum is 0,as expected. In other losing states, any move leads to a winning state, becausewhen a single value xk changes, the nim sum also changes, so the nim sum isdifferent from 0 after the move.Winning states: We can move to a losing state if there is any heap k for whichxk s < xk. In this case, we can remove sticks from heap k so that it will containxk s sticks, which will lead to a losing state. There is always such a heap, wherexk has a one bit at the position of the leftmost one bit of s.As an example, consider the state [10 ,12,5]. This state is a winning state,because its nim sum is 3. Thus, there has to be a move which leads to a losingstate. Next we will nd out such a move.The nim sum of the state is as follows:10 101012 11005 01013 0011In this case, the heap with 10 sticks is the only heap that has a one bit at theposition of the leftmost one bit of the nim sum:10 101012 11005 01013 00111The optimal strategy for nim was published in 1901 by C. L. Bouton [10].237",
            "The new size of the heap has to be 10 3 = 9, so we will remove just one stick.After this, the state will be [9,12,5], which is a losing state:9 100112 11005 01010 0000Misre gameIn a misre game, the goal of the game is opposite, so the player who removesthe last stick loses the game. It turns out that the misre nim game can beoptimally played almost like the standard nim game.The idea is to rst play the misre game like the standard game, but changethe strategy at the end of the game. The new strategy will be introduced in asituation where each heap would contain at most one stick after the next move.In the standard game, we should choose a move after which there is an evennumber of heaps with one stick. However, in the misre game, we choose a moveso that there is an odd number of heaps with one stick.This strategy works because a state where the strategy changes alwaysappears in the game, and this state is a winning state, because it contains exactlyone heap that has more than one stick so the nim sum is not 0.25.3 SpragueGrundy theoremThe SpragueGrundy theorem2 generalizes the strategy used in nim to allgames that full the following requirements: There are two players who move alternately. The game consists of states, and the possible moves in a state do not dependon whose turn it is. The game ends when a player cannot make a move. The game surely ends sooner or later. The players have complete information about the states and allowed moves,and there is no randomness in the game.The idea is to calculate for each game state a Grundy number that correspondsto the number of sticks in a nim heap. When we know the Grundy numbers of allstates, we can play the game like the nim game.Grundy numbersThe Grundy number of a game state ismex({g1, g2,..., gn}),2The theorem was independently discovered by R. Sprague [61] and P. M. Grundy [31].238",
            "where g1, g2,..., gn are the Grundy numbers of the states to which we can move,and the mex function gives the smallest nonnegative number that is not in theset. For example, mex({0,1,3}) = 2. If there are no possible moves in a state, itsGrundy number is 0, because mex() = 0.For example, in the state graphthe Grundy numbers are as follows:0 1 02 0 2The Grundy number of a losing state is 0, and the Grundy number of a winningstate is a positive number.The Grundy number of a state corresponds to the number of sticks in a nimheap. If the Grundy number is 0, we can only move to states whose Grundynumbers are positive, and if the Grundy number is x > 0, we can move to stateswhose Grundy numbers include all numbers 0,1,..., x 1.As an example, consider a game where the players move a gure in a maze.Each square in the maze is either oor or wall. On each turn, the player has tomove the gure some number of steps left or up. The winner of the game is theplayer who makes the last move.The following picture shows a possible initial state of the game, where @denotes the gure and * denotes a square where it can move.@******The states of the game are all oor squares of the maze. In the above maze,the Grundy numbers are as follows:0 1 0 10 1 20 2 1 03 0 4 10 4 1 3 2239",
            "Thus, each state of the maze game corresponds to a heap in the nim game. Forexample, the Grundy number for the lower-right square is 2, so it is a winningstate. We can reach a losing state and win the game by moving either four stepsleft or two steps up.Note that unlike in the original nim game, it may be possible to move to astate whose Grundy number is larger than the Grundy number of the currentstate. However, the opponent can always choose a move that cancels such a move,so it is not possible to escape from a losing state.SubgamesNext we will assume that our game consists of subgames, and on each turn, theplayer rst chooses a subgame and then a move in the subgame. The game endswhen it is not possible to make any move in any subgame.In this case, the Grundy number of a game is the nim sum of the Grundynumbers of the subgames. The game can be played like a nim game by calculatingall Grundy numbers for subgames and then their nim sum.As an example, consider a game that consists of three mazes. In this game,on each turn, the player chooses one of the mazes and then moves the gure inthe maze. Assume that the initial state of the game is as follows:@ @ @The Grundy numbers for the mazes are as follows:0 1 0 10 1 20 2 1 03 0 4 10 4 1 3 20 1 2 31 0 0 12 0 1 23 1 2 04 0 2 5 30 1 2 3 41 02 13 24 0 1 2 3In the initial state, the nim sum of the Grundy numbers is 233 = 2, so therst player can win the game. One optimal move is to move two steps up in therst maze, which produces the nim sum 0 33 = 0.Grundys gameSometimes a move in a game divides the game into subgames that are indepen-dent of each other. In this case, the Grundy number of the game ismex({g1, g2,..., gn}),240",
            "where n is the number of possible moves andgk = ak,1 ak,2 ... ak,m,where move k generates subgames with Grundy numbers ak,1,ak,2,..., ak,m.An example of such a game is Grundys game. Initially, there is a singleheap that contains n sticks. On each turn, the player chooses a heap and dividesit into two nonempty heaps such that the heaps are of different size. The playerwho makes the last move wins the game.Let f (n) be the Grundy number of a heap that contains n sticks. The Grundynumber can be calculated by going through all ways to divide the heap into twoheaps. For example, when n = 8, the possibilities are 1+7, 2+6 and 3+5, sof (8) = mex({f (1) f (7), f (2) f (6), f (3) f (5)}).In this game, the value of f (n) is based on the values of f (1),..., f (n 1). Thebase cases are f (1) = f (2) = 0, because it is not possible to divide the heaps of 1and 2 sticks. The rst Grundy numbers are:f (1) = 0f (2) = 0f (3) = 1f (4) = 0f (5) = 2f (6) = 1f (7) = 0f (8) = 2The Grundy number for n = 8 is 2, so it is possible to win the game. The winningmove is to create heaps 1+7, because f (1) f (7) = 0.241",
            "242",
            "Chapter 26String algorithmsThis chapter deals with efcient algorithms for string processing. Many stringproblems can be easily solved inO(n2) time, but the challenge is to nd algorithmsthat work in O(n) or O(nlogn) time.For example, a fundamental string processing problem is thepattern match-ing problem: given a string of length n and a pattern of length m, our task is tond the occurrences of the pattern in the string. For example, the pattern ABCoccurs two times in the string ABABCBABC.The pattern matching problem can be easily solved in O(nm) time by a bruteforce algorithm that tests all positions where the pattern may occur in the string.However, in this chapter, we will see that there are more efcient algorithms thatrequire only O(n +m) time.26.1 String terminologyThroughout the chapter, we assume that zero-based indexing is used in strings.Thus, a string s of length n consists of characters s[0],s[1],..., s[n 1]. The set ofcharacters that may appear in strings is called an alphabet. For example, thealphabet {A,B,..., Z} consists of the capital letters of English.A substring is a sequence of consecutive characters in a string. We use thenotation s[a... b] to refer to a substring of s that begins at position a and endsat position b. A string of length n has n(n +1)/2 substrings. For example, thesubstrings of ABCD are A, B, C, D, AB, BC, CD, ABC, BCD and ABCD.A subsequence is a sequence of (not necessarily consecutive) characters in astring in their original order. A string of length n has 2n 1 subsequences. Forexample, the subsequences of ABCD are A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD,ACD, BCD and ABCD.A prex is a substring that starts at the beginning of a string, and a sufxis a substring that ends at the end of a string. For example, the prexes of ABCDare A, AB, ABC and ABCD, and the sufxes of ABCD are D, CD, BCD and ABCD.A rotation can be generated by moving the characters of a string one by onefrom the beginning to the end (or vice versa). For example, the rotations of ABCDare ABCD, BCDA, CDAB and DABC.243",
            "A period is a prex of a string such that the string can be constructed byrepeating the period. The last repetition may be partial and contain only a prexof the period. For example, the shortest period of ABCABCA is ABC.A border is a string that is both a prex and a sufx of a string. For example,the borders of ABACABA are A, ABA and ABACABA.Strings are compared using the lexicographical order (which correspondsto the alphabetical order). It means that x < y if either x = y and x is a prex of y,or there is a position k such that x[i] = y[i] when i < k and x[k] < y[k].26.2 Trie structureA trie is a rooted tree that maintains a set of strings. Each string in the setis stored as a chain of characters that starts at the root. If two strings have acommon prex, they also have a common chain in the tree.For example, consider the following trie:* ***C TANA DL YHEREThis trie corresponds to the set {CANAL,CANDY,THE,THERE}. The character * in anode means that a string in the set ends at the node. Such a character is needed,because a string may be a prex of another string. For example, in the above trie,THE is a prex of THERE.We can check inO(n) time whether a trie contains a string of lengthn, becausewe can follow the chain that starts at the root node. We can also add a string oflength n to the trie in O(n) time by rst following the chain and then adding newnodes to the trie if necessary.Using a trie, we can nd the longest prex of a given string such that theprex belongs to the set. Moreover, by storing additional information in eachnode, we can calculate the number of strings that belong to the set and have agiven string as a prex.A trie can be stored in an arrayint trie[N][A];244",
            "where N is the maximum number of nodes (the maximum total length of thestrings in the set) and A is the size of the alphabet. The nodes of a trie arenumbered 0,1,2,... so that the number of the root is 0, and trie[s][c] is the nextnode in the chain when we move from node s using character c.26.3 String hashingString hashing is a technique that allows us to efciently check whether twostrings are equal1. The idea in string hashing is to compare hash values of stringsinstead of their individual characters.Calculating hash valuesA hash value of a string is a number that is calculated from the characters ofthe string. If two strings are the same, their hash values are also the same, whichmakes it possible to compare strings based on their hash values.A usual way to implement string hashing is polynomial hashing, whichmeans that the hash value of a string s of length n is(s[0]An1 +s[1]An2 ++ s[n 1]A0) mod B,where s[0],s[1],..., s[n1] are interpreted as the codes of the characters of s, andA and B are pre-chosen constants.For example, the codes of the characters of ALLEY are:A L L E Y65 76 76 69 89Thus, if A = 3 and B = 97, the hash value of ALLEY is(6534 +7633 +7632 +6931 +8930) mod 97 = 52.PreprocessingUsing polynomial hashing, we can calculate the hash value of any substring of astring s in O(1) time after an O(n) time preprocessing. The idea is to constructan array h such that h[k] contains the hash value of the prex s[0... k]. The arrayvalues can be recursively calculated as follows:h[0] = s[0]h[k] = (h[k 1]A +s[k]) mod BIn addition, we construct an array p where p[k] = Ak mod B:p[0] = 1p[k] = (p[k 1]A) mod B.1The technique was popularized by the KarpRabin pattern matching algorithm [42].245",
            "Constructing these arrays takes O(n) time. After this, the hash value of anysubstring s[a... b] can be calculated in O(1) time using the formula(h[b]h[a 1]p[b a +1]) mod Bassuming that a > 0. If a = 0, the hash value is simply h[b].Using hash valuesWe can efciently compare strings using hash values. Instead of comparing theindividual characters of the strings, the idea is to compare their hash values. Ifthe hash values are equal, the strings are probably equal, and if the hash valuesare different, the strings are certainly different.Using hashing, we can often make a brute force algorithm efcient. As anexample, consider the pattern matching problem: given a string s and a patternp, nd the positions where p occurs in s. A brute force algorithm goes throughall positions where p may occur and compares the strings character by character.The time complexity of such an algorithm is O(n2).We can make the brute force algorithm more efcient by using hashing,because the algorithm compares substrings of strings. Using hashing, eachcomparison only takes O(1) time, because only hash values of substrings arecompared. This results in an algorithm with time complexity O(n), which is thebest possible time complexity for this problem.By combining hashing and binary search, it is also possible to nd out thelexicographic order of two strings in logarithmic time. This can be done bycalculating the length of the common prex of the strings using binary search.Once we know the length of the common prex, we can just check the nextcharacter after the prex, because this determines the order of the strings.Collisions and parametersAn evident risk when comparing hash values is a collision, which means thattwo strings have different contents but equal hash values. In this case, analgorithm that relies on the hash values concludes that the strings are equal, butin reality they are not, and the algorithm may give incorrect results.Collisions are always possible, because the number of different strings islarger than the number of different hash values. However, the probability of acollision is small if the constants A and B are carefully chosen. A usual way is tochoose random constants near 109, for example as follows:A = 911382323B = 972663749Using such constants, the long long type can be used when calculating hashvalues, because the products AB and BB will t in long long. But is it enough tohave about 109 different hash values?Let us consider three scenarios where hashing can be used:246",
            "Scenario 1: Strings x and y are compared with each other. The probability ofa collision is 1/B assuming that all hash values are equally probable.Scenario 2: A string x is compared with strings y1, y2,..., yn. The probabilityof one or more collisions is1(1 1B)n.Scenario 3: All pairs of strings x1, x2,..., xn are compared with each other.The probability of one or more collisions is1 B (B 1)(B 2) (B n +1)Bn .The following table shows the collision probabilities when n = 106 and thevalue of B varies:constant B scenario 1 scenario 2 scenario 3103 0.001000 1 .000000 1 .000000106 0.000001 0 .632121 1 .000000109 0.000000 0 .001000 1 .0000001012 0.000000 0 .000000 0 .3934691015 0.000000 0 .000000 0 .0005001018 0.000000 0 .000000 0 .000001The table shows that in scenario 1, the probability of a collision is negligiblewhen B  109. In scenario 2, a collision is possible but the probability is stillquite small. However, in scenario 3 the situation is very different: a collision willalmost always happen when B  109.The phenomenon in scenario 3 is known as the birthday paradox: if thereare n people in a room, the probability that some two people have the samebirthday is large even if n is quite small. In hashing, correspondingly, when allhash values are compared with each other, the probability that some two hashvalues are equal is large.We can make the probability of a collision smaller by calculating multiplehash values using different parameters. It is unlikely that a collision wouldoccur in all hash values at the same time. For example, two hash values withparameter B  109 correspond to one hash value with parameter B  1018, whichmakes the probability of a collision very small.Some people use constants B = 232 and B = 264, which is convenient, becauseoperations with 32 and 64 bit integers are calculated modulo 232 and 264. How-ever, this is not a good choice, because it is possible to construct inputs thatalways generate collisions when constants of the form 2x are used [51].26.4 Z-algorithmThe Z-array z of a string s of length n contains for each k = 0,1,..., n 1 thelength of the longest substring of s that begins at position k and is a prex of247",
            "s. Thus, z[k] = p tells us that s[0... p 1] equals s[k... k + p 1]. Many stringprocessing problems can be efciently solved using the Z-array.For example, the Z-array of ACBACDACBACBACDA is as follows:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 7 0 0 2 0 0 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15In this case, for example, z[6] = 5, because the substring ACBAC of length 5 is aprex of s, but the substring ACBACB of length 6 is not a prex of s.Algorithm descriptionNext we describe an algorithm, called the Z-algorithm2, that efciently con-structs the Z-array in O(n) time. The algorithm calculates the Z-array valuesfrom left to right by both using information already stored in the Z-array andcomparing substrings character by character.To efciently calculate the Z-array values, the algorithm maintains a range[x, y] such that s[x... y] is a prex of s and y is as large as possible. Since weknow that s[0... yx] and s[x... y] are equal, we can use this information whencalculating Z-values for positions x +1, x +2,..., y.At each position k, we rst check the value of z[k  x]. If k +z[k  x] < y, weknow that z[k] = z[k  x]. However, if k +z[k  x]  y, s[0... yk] equals s[k... y],and to determine the value of z[k] we need to compare the substrings characterby character. Still, the algorithm works in O(n) time, because we start comparingat positions yk +1 and y+1.For example, let us construct the following Z-array:A C B A C D A C B A C B A C D A ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15After calculating the value z[6] = 5, the current [x, y] range is [6,10]:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 ? ? ? ? ? ? ? ? ?x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Now we can calculate subsequent Z-array values efciently, because we knowthat s[0... 4] and s[6... 10] are equal. First, since z[1] = z[2] = 0, we immediatelyknow that also z[7] = z[8] = 0:2The Z-algorithm was presented in [32] as the simplest known method for linear-time patternmatching, and the original idea was attributed to [50].248",
            "A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15Then, since z[3] = 2, we know that z[9]  2:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15However, we have no information about the string after position 10, so weneed to compare the substrings character by character:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 ? ? ? ? ? ? ?x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15It turns out that z[9] = 7, so the new [x, y] range is [9,15]:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 7 ? ? ? ? ? ?x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15After this, all the remaining Z-array values can be determined by using theinformation already stored in the Z-array:A C B A C D A C B A C B A C D A 0 0 2 0 0 5 0 0 7 0 0 2 0 0 1x y0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15249",
            "Using the Z-arrayIt is often a matter of taste whether to use string hashing or the Z-algorithm.Unlike hashing, the Z-algorithm always works and there is no risk for collisions.On the other hand, the Z-algorithm is more difcult to implement and someproblems can only be solved using hashing.As an example, consider again the pattern matching problem, where our taskis to nd the occurrences of a pattern p in a string s. We already solved thisproblem efciently using string hashing, but the Z-algorithm provides anotherway to solve the problem.A usual idea in string processing is to construct a string that consists of mul-tiple strings separated by special characters. In this problem, we can construct astring p#s, where p and s are separated by a special character # that does notoccur in the strings. The Z-array of p#s tells us the positions where p occurs in s,because such positions contain the length of p.For example, if s =HATTIVATTI and p =ATT, the Z-array is as follows:A T T # H A T T I V A T T I 0 0 0 0 3 0 0 0 0 3 0 0 00 1 2 3 4 5 6 7 8 9 10 11 12 13The positions 5 and 10 contain the value 3, which means that the pattern ATToccurs in the corresponding positions of HATTIVATTI.The time complexity of the resulting algorithm is linear, because it sufces toconstruct the Z-array and go through its values.ImplementationHere is a short implementation of the Z-algorithm that returns a vector thatcorresponds to the Z-array.vector<int> z(string s) {int n = s.size();vector<int> z(n);int x = 0, y = 0;for (int i = 1; i < n; i++) {z[i] = max(0,min(z[i-x],y-i+1));while (i+z[i] < n && s[z[i]] == s[i+z[i]]) {x = i; y = i+z[i]; z[i]++;}}return z;}250",
            "Chapter 27Square root algorithmsA square root algorithm is an algorithm that has a square root in its timecomplexity. A square root can be seen as a poor mans logarithm: the complexityO(n) is better than O(n) but worse than O(logn). In any case, many squareroot algorithms are fast and usable in practice.As an example, consider the problem of creating a data structure that sup-ports two operations on an array: modifying an element at a given position andcalculating the sum of elements in the given range. We have previously solved theproblem using binary indexed and segment trees, that support both operations inO(logn) time. However, now we will solve the problem in another way using asquare root structure that allows us to modify elements inO(1) time and calculatesums in O(n) time.The idea is to divide the array intoblocks of size n so that each block containsthe sum of elements inside the block. For example, an array of 16 elements willbe divided into blocks of 4 elements as follows:5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 221 17 20 13In this structure, it is easy to modify array elements, because it is only neededto update the sum of a single block after each modication, which can be done inO(1) time. For example, the following picture shows how the value of an elementand the sum of the corresponding block change:5 8 6 3 2 5 2 6 7 1 7 5 6 2 3 221 15 20 13Then, to calculate the sum of elements in a range, we divide the range intothree parts such that the sum consists of values of single elements and sums ofblocks between them:5 8 6 3 2 5 2 6 7 1 7 5 6 2 3 221 15 20 13251",
            "Since the number of single elements is O(n) and the number of blocks isalso O(n), the sum query takes O(n) time. The purpose of the block size n isthat it balances two things: the array is divided into n blocks, each of whichcontains n elements.In practice, it is not necessary to use the exact value of n as a parameter,and instead we may use parameters k and n/k where k is different from n.The optimal parameter depends on the problem and input. For example, if analgorithm often goes through the blocks but rarely inspects single elements insidethe blocks, it may be a good idea to divide the array into k < n blocks, each ofwhich contains n/k > n elements.27.1 Combining algorithmsIn this section we discuss two square root algorithms that are based on combiningtwo algorithms into one algorithm. In both cases, we could use either of thealgorithms without the other and solve the problem in O(n2) time. However, bycombining the algorithms, the running time is only O(nn).Case processingSuppose that we are given a two-dimensional grid that contains n cells. Eachcell is assigned a letter, and our task is to nd two cells with the same letterwhose distance is minimum, where the distance between cells (x1, y1) and (x2, y2)is |x1  x2|+| y1  y2|. For example, consider the following grid:ABCACDEFBAGBDFEAIn this case, the minimum distance is 2 between the two E letters.We can solve the problem by considering each letter separately. Using thisapproach, the new problem is to calculate the minimum distance between twocells with a xed letter c. We focus on two algorithms for this:Algorithm 1: Go through all pairs of cells with letter c, and calculate theminimum distance between such cells. This will take O(k2) time where k is thenumber of cells with letter c.Algorithm 2: Perform a breadth-rst search that simultaneously starts ateach cell with letter c. The minimum distance between two cells with letter cwill be calculated in O(n) time.One way to solve the problem is to choose either of the algorithms and useit for all letters. If we use Algorithm 1, the running time is O(n2), because allcells may contain the same letter, and in this casek = n. Also if we use Algorithm2, the running time is O(n2), because all cells may have different letters, and inthis case n searches are needed.252",
            "However, we can combine the two algorithms and use different algorithms fordifferent letters depending on how many times each letter appears in the grid.Assume that a letter c appears k times. If k  n, we use Algorithm 1, and ifk > n, we use Algorithm 2. It turns out that by doing this, the total runningtime of the algorithm is only O(nn).First, suppose that we use Algorithm 1 for a letter c. Since c appears at mostn times in the grid, we compare each cell with letter c O(n) times with othercells. Thus, the time used for processing all such cells is O(nn). Then, supposethat we use Algorithm 2 for a letter c. There are at most n such letters, soprocessing those letters also takes O(nn) time.Batch processingOur next problem also deals with a two-dimensional grid that contains n cells.Initially, each cell except one is white. We performn1 operations, each of whichrst calculates the minimum distance from a given white cell to a black cell, andthen paints the white cell black.For example, consider the following operation:*First, we calculate the minimum distance from the white cell marked with *to a black cell. The minimum distance is 2, because we can move two steps left toa black cell. Then, we paint the white cell black:Consider the following two algorithms:Algorithm 1: Use breadth-rst search to calculate for each white cell thedistance to the nearest black cell. This takes O(n) time, and after the search, wecan nd the minimum distance from any white cell to a black cell in O(1) time.Algorithm 2: Maintain a list of cells that have been painted black, go throughthis list at each operation and then add a new cell to the list. An operation takesO(k) time where k is the length of the list.We combine the above algorithms by dividing the operations into O(n)batches, each of which consists of O(n) operations. At the beginning of eachbatch, we perform Algorithm 1. Then, we use Algorithm 2 to process the opera-tions in the batch. We clear the list of Algorithm 2 between the batches. At each253",
            "operation, the minimum distance to a black cell is either the distance calculatedby Algorithm 1 or the distance calculated by Algorithm 2.The resulting algorithm works in O(nn) time. First, Algorithm 1 is per-formed O(n) times, and each search works in O(n) time. Second, when usingAlgorithm 2 in a batch, the list contains O(n) cells (because we clear the listbetween the batches) and each operation takes O(n) time.27.2 Integer partitionsSome square root algorithms are based on the following observation: if a positiveinteger n is represented as a sum of positive integers, such a sum always containsat most O(n) distinct numbers. The reason for this is that to construct a sumthat contains a maximum number of distinct numbers, we should choose smallnumbers. If we choose the numbers 1,2,..., k, the resulting sum isk(k +1)2 .Thus, the maximum amount of distinct numbers is k = O(n). Next we willdiscuss two problems that can be solved efciently using this observation.KnapsackSuppose that we are given a list of integer weights whose sum is n. Our task is tond out all sums that can be formed using a subset of the weights. For example,if the weights are {1,3,3}, the possible sums are as follows: 0 (empty set) 1 3 1 +3 = 4 3 +3 = 6 1 +3+3 = 7Using the standard knapsack approach (see Chapter 7.4), the problem can besolved as follows: we dene a function possible(x,k) whose value is 1 if the sumx can be formed using the rst k weights, and 0 otherwise. Since the sum of theweights is n, there are at most n weights and all values of the function can becalculated in O(n2) time using dynamic programming.However, we can make the algorithm more efcient by using the fact thatthere are at most O(n) distinct weights. Thus, we can process the weights ingroups that consists of similar weights. We can process each group in O(n) time,which yields an O(nn) time algorithm.The idea is to use an array that records the sums of weights that can be formedusing the groups processed so far. The array contains n elements: element k is 1if the sum k can be formed and 0 otherwise. To process a group of weights, wescan the array from left to right and record the new sums of weights that can beformed using this group and the previous groups.254",
            "String constructionGiven a strings of length n and a set of stringsD whose total length ism, considerthe problem of counting the number of ways s can be formed as a concatenationof strings in D. For example, if s = ABAB and D = {A,B,AB}, there are 4 ways: A +B +A +B AB +A +B A +B +AB AB +ABWe can solve the problem using dynamic programming: Let count(k) denotethe number of ways to construct the prex s[0... k] using the strings in D. Nowcount(n 1) gives the answer to the problem, and we can solve the problem inO(n2) time using a trie structure.However, we can solve the problem more efciently by using string hashingand the fact that there are at most O(m) distinct string lengths in D. First, weconstruct a set H that contains all hash values of the strings in D. Then, whencalculating a value of count(k), we go through all values of p such that there is astring of length p in D, calculate the hash value of s[k  p +1... k] and check if itbelongs to H. Since there are at most O(m) distinct string lengths, this resultsin an algorithm whose running time is O(nm).27.3 Mos algorithmMos algorithm1 can be used in many problems that require processing rangequeries in a static array, i.e., the array values do not change between the queries.In each query, we are given a range [a,b], and we should calculate a value basedon the array elements between positions a and b. Since the array is static, thequeries can be processed in any order, and Mos algorithm processes the queriesin a special order which guarantees that the algorithm works efciently.Mos algorithm maintains an active range of the array, and the answer toa query concerning the active range is known at each moment. The algorithmprocesses the queries one by one, and always moves the endpoints of the activerange by inserting and removing elements. The time complexity of the algorithmis O(nnf (n)) where the array contains n elements, there are n queries and eachinsertion and removal of an element takes O(f (n)) time.The trick in Mos algorithm is the order in which the queries are processed:The array is divided into blocks of k = O(n) elements, and a query [ a1,b1] isprocessed before a query [a2,b2] if either a1/k < a2/k or a1/k = a2/k and b1 < b2.1According to [12], this algorithm is named after Mo Tao, a Chinese competitive programmer,but the technique has appeared earlier in the literature [44].255",
            "Thus, all queries whose left endpoints are in a certain block are processedone after another sorted according to their right endpoints. Using this order, thealgorithm only performs O(nn) operations, because the left endpoint movesO(n) times O(n) steps, and the right endpoint moves O(n) times O(n) steps.Thus, both endpoints move a total of O(nn) steps during the algorithm.ExampleAs an example, consider a problem where we are given a set of queries, each ofthem corresponding to a range in an array, and our task is to calculate for eachquery the number of distinct elements in the range.In Mos algorithm, the queries are always sorted in the same way, but itdepends on the problem how the answer to the query is maintained. In thisproblem, we can maintain an array count where count[x] indicates the numberof times an element x occurs in the active range.When we move from one query to another query, the active range changes.For example, if the current range is4 2 5 4 2 4 3 3 4and the next range is4 2 5 4 2 4 3 3 4there will be three steps: the left endpoint moves one step to the right, and theright endpoint moves two steps to the right.After each step, the array count needs to be updated. After adding an elementx, we increase the value of count[x] by 1, and if count[x] = 1 after this, we alsoincrease the answer to the query by 1. Similarly, after removing an elementx, wedecrease the value of count[x] by 1, and if count[x] = 0 after this, we also decreasethe answer to the query by 1.In this problem, the time needed to perform each step is O(1), so the totaltime complexity of the algorithm is O(nn).256",
            "Chapter 28Segment trees revisitedA segment tree is a versatile data structure that can be used to solve a large num-ber of algorithm problems. However, there are many topics related to segmenttrees that we have not touched yet. Now is time to discuss some more advancedvariants of segment trees.So far, we have implemented the operations of a segment tree by walkingfrom bottom to top in the tree. For example, we have calculated range sums asfollows (Chapter 9.3):int sum(int a, int b) {a += n; b += n;int s = 0;while (a <= b) {if (a%2 == 1) s += tree[a++];if (b%2 == 0) s += tree[b--];a /= 2; b /= 2;}return s;}However, in more advanced segment trees, it is often necessary to implementthe operations in another way, from top to bottom . Using this approach, thefunction becomes as follows:int sum(int a, int b, int k, int x, int y) {if (b < x || a > y) return 0;if (a <= x && y <= b) return tree[k];int d = (x+y)/2;return sum(a,b,2*k,x,d) + sum(a,b,2*k+1,d+1,y);}Now we can calculate any value ofsumq(a,b) (the sum of array values in range[a,b]) as follows:int s = sum(a, b, 1, 0, n-1);257",
            "The parameter k indicates the current position in tree. Initially k equals 1,because we begin at the root of the tree. The range [x, y] corresponds to k and isinitially [0,n 1]. When calculating the sum, if [x, y] is outside [a,b], the sum is0, and if [x, y] is completely inside [a,b], the sum can be found in tree. If [x, y] ispartially inside [a,b], the search continues recursively to the left and right halfof [x, y]. The left half is [x,d] and the right half is [d +1, y] where d = x+y2 .The following picture shows how the search proceeds when calculating thevalue of sumq(a,b). The gray nodes indicate nodes where the recursion stops andthe sum can be found in tree.5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 213 9 9 8 8 12 8 522 17 20 1339 3372a bAlso in this implementation, operations take O(logn) time, because the totalnumber of visited nodes is O(logn).28.1 Lazy propagationUsing lazy propagation, we can build a segment tree that supports both rangeupdates and range queries in O(logn) time. The idea is to perform updates andqueries from top to bottom and perform updateslazily so that they are propagateddown the tree only when it is necessary.In a lazy segment tree, nodes contain two types of information. Like in anordinary segment tree, each node contains the sum or some other value relatedto the corresponding subarray. In addition, the node may contain informationrelated to lazy updates, which has not been propagated to its children.There are two types of range updates: each array value in the range iseither increased by some value or assigned some value. Both operations can beimplemented using similar ideas, and it is even possible to construct a tree thatsupports both operations at the same time.Lazy segment treesLet us consider an example where our goal is to construct a segment tree that sup-ports two operations: increasing each value in [a,b] by a constant and calculating258",
            "the sum of values in [a,b].We will construct a tree where each node has two values s/z: s denotes thesum of values in the range, andz denotes the value of a lazy update, which meansthat all values in the range should be increased by z. In the following tree, z = 0in all nodes, so there are no ongoing lazy updates.5 8 6 3 2 7 2 6 7 1 7 5 6 2 3 213/0 9/0 9/0 8/0 8/0 12/0 8/0 5/022/0 17/0 20/0 13/039/0 33/072/0When the elements in [a,b] are increased by u, we walk from the root towardsthe leaves and modify the nodes of the tree as follows: If the range [x, y] of a nodeis completely inside [a,b], we increase the z value of the node by u and stop. If[x, y] only partially belongs to [a,b], we increase the s value of the node by hu,where h is the size of the intersection of [a,b] and [x, y], and continue our walkrecursively in the tree.For example, the following picture shows the tree after increasing the ele-ments in [a,b] by 2:5 8 6 3 2 9 2 6 7 1 7 5 6 2 3 213/0 9/0 11/0 8/2 8/0 12/0 8/2 5/022/0 23/0 20/2 17/045/0 45/090/0a bWe also calculate the sum of elements in a range [ a,b] by walking in thetree from top to bottom. If the range [x, y] of a node completely belongs to [a,b],we add the s value of the node to the sum. Otherwise, we continue the searchrecursively downwards in the tree.259",
            "Both in updates and queries, the value of a lazy update is always propagatedto the children of the node before processing the node. The idea is that updateswill be propagated downwards only when it is necessary, which guarantees thatthe operations are always efcient.The following picture shows how the tree changes when we calculate thevalue of suma(a,b). The rectangle shows the nodes whose values change, becausea lazy update is propagated downwards.5 8 6 3 2 9 2 6 7 1 7 5 6 2 3 213/0 9/0 11/0 8/2 8/2 12/2 8/2 5/022/0 23/0 28/0 17/045/0 45/090/0a bNote that sometimes it is needed to combine lazy updates. This happens whena node that already has a lazy update is assigned another lazy update. Whencalculating sums, it is easy to combine lazy updates, because the combination ofupdates z1 and z2 corresponds to an update z1 + z2.Polynomial updatesLazy updates can be generalized so that it is possible to update ranges usingpolynomials of the formp(u) = tkuk + tk1uk1 ++ t0.In this case, the update for a value at position i in [a,b] is p(i  a). Forexample, adding the polynomial p(u) = u +1 to [a,b] means that the value atposition a increases by 1, the value at position a +1 increases by 2, and so on.To support polynomial updates, each node is assigned k +2 values, where kequals the degree of the polynomial. The value s is the sum of the elements inthe range, and the values z0, z1,..., zk are the coefcients of a polynomial thatcorresponds to a lazy update.Now, the sum of values in a range [x, y] equalss +yxu=0zkuk + zk1uk1 ++ z0.260",
            "The value of such a sum can be efciently calculated using sum formulas.For example, the term z0 corresponds to the sum (yx +1)z0, and the term z1ucorresponds to the sumz1(0+1++ y x) = z1(y x)(y x +1)2 .When propagating an update in the tree, the indices of p(u) change, becausein each range [x, y], the values are calculated for u = 0,1,..., yx. However, thisis not a problem, because p(u) = p(u +h) is a polynomial of equal degree as p(u).For example, if p(u) = t2u2 + t1u  t0, thenp(u) = t2(u +h)2 + t1(u +h) t0 = t2u2 +(2ht2 + t1)u + t2h2 + t1h  t0.28.2 Dynamic treesAn ordinary segment tree is static, which means that each node has a xedposition in the array and the tree requires a xed amount of memory. In adynamic segment tree, memory is allocated only for nodes that are actuallyaccessed during the algorithm, which can save a large amount of memory.The nodes of a dynamic tree can be represented as structs:struct node {int value;int x, y;node *left, *right;node(int v, int x, int y) : value(v), x(x), y(y) {}};Here value is the value of the node, [x,y] is the corresponding range, and leftand right point to the left and right subtree.After this, nodes can be created as follows:// create new nodenode *x = new node(0, 0, 15);// change valuex->value = 5;Sparse segment treesA dynamic segment tree is useful when the underlying array is sparse, i.e., therange [0,n 1] of allowed indices is large, but most array values are zeros. Whilean ordinary segment tree uses O(n) memory, a dynamic segment tree only usesO(klogn) memory, where k is the number of operations performed.A sparse segment tree initially has only one node [0 ,n 1] whose valueis zero, which means that every array value is zero. After updates, new nodesare dynamically added to the tree. For example, if n = 16 and the elements inpositions 3 and 10 have been modied, the tree contains the following nodes:261",
            "[0,15][0,7][0,3][2,3][3][8,15][8,11][10,11][10]Any path from the root node to a leaf containsO(logn) nodes, so each operationadds at most O(logn) new nodes to the tree. Thus, after k operations, the treecontains at most O(klogn) nodes.Note that if we know all elements to be updated at the beginning of thealgorithm, a dynamic segment tree is not necessary, because we can use anordinary segment tree with index compression (Chapter 9.4). However, this isnot possible when the indices are generated during the algorithm.Persistent segment treesUsing a dynamic implementation, it is also possible to create a persistentsegment tree that stores the modication history of the tree. In such an im-plementation, we can efciently access all versions of the tree that have existedduring the algorithm.When the modication history is available, we can perform queries in anyprevious tree like in an ordinary segment tree, because the full structure of eachtree is stored. We can also create new trees based on previous trees and modifythem independently.Consider the following sequence of updates, where red nodes change andother nodes remain the same:step 1 step 2 step 3After each update, most nodes of the tree remain the same, so an efcient wayto store the modication history is to represent each tree in the history as a262",
            "combination of new nodes and subtrees of previous trees. In this example, themodication history can be stored as follows:step 1 step 2 step 3The structure of each previous tree can be reconstructed by following thepointers starting at the corresponding root node. Since each operation adds onlyO(logn) new nodes to the tree, it is possible to store the full modication historyof the tree.28.3 Data structuresInstead of single values, nodes in a segment tree can also contain data structuresthat maintain information about the corresponding ranges. In such a tree, theoperations take O(f (n)logn) time, where f (n) is the time needed for processing asingle node during an operation.As an example, consider a segment tree that supports queries of the formhow many times does an element x appear in the range [a,b]? For example, theelement 1 appears three times in the following range:3 1 2 3 1 1 1 2To support such queries, we build a segment tree where each node is assigneda data structure that can be asked how many times any element x appears in thecorresponding range. Using this tree, the answer to a query can be calculated bycombining the results from the nodes that belong to the range.For example, the following segment tree corresponds to the above array:31112131111111211 31 12 31 1121 21 11 2 31 1 21 23 11 2 34 2 2263",
            "We can build the tree so that each node contains a map structure. In this case,the time needed for processing each node is O(logn), so the total time complexityof a query isO(log2 n). The tree uses O(nlogn) memory, because there areO(logn)levels and each level contains O(n) elements.28.4 Two-dimensionalityA two-dimensional segment tree supports queries related to rectangular sub-arrays of a two-dimensional array. Such a tree can be implemented as nestedsegment trees: a big tree corresponds to the rows of the array, and each nodecontains a small tree that corresponds to a column.For example, in the array8 5 3 83 9 7 18 7 5 27 6 1 6the sum of any subarray can be calculated from the following segment tree:7 6 1 613 7208 7 5 215 7223 9 7 112 8208 5 3 813 11241513 6 828 1442111410 925 19442627161753 3386The operations of a two-dimensional segment tree takeO(log2 n) time, becausethe big tree and each small tree consist of O(logn) levels. The tree requires O(n2)memory, because each small tree contains O(n) values.264",
            "Chapter 29GeometryIn geometric problems, it is often challenging to nd a way to approach theproblem so that the solution to the problem can be conveniently implementedand the number of special cases is small.As an example, consider a problem where we are given the vertices of aquadrilateral (a polygon that has four vertices), and our task is to calculate itsarea. For example, a possible input for the problem is as follows:One way to approach the problem is to divide the quadrilateral into two trianglesby a straight line between two opposite vertices:After this, it sufces to sum the areas of the triangles. The area of a triangle canbe calculated, for example, using Herons formulas(s a)(s b)(s  c),where a, b and c are the lengths of the triangles sides ands = (a +b + c)/2.This is a possible way to solve the problem, but there is one pitfall: how todivide the quadrilateral into triangles? It turns out that sometimes we cannotjust pick two arbitrary opposite vertices. For example, in the following situation,the division line is outside the quadrilateral:265",
            "However, another way to draw the line works:It is clear for a human which of the lines is the correct choice, but the situation isdifcult for a computer.However, it turns out that we can solve the problem using another methodthat is more convenient to a programmer. Namely, there is a general formulax1 y2  x2 y1 + x2 y3  x3 y2 + x3 y4  x4 y3 + x4 y1  x1 y4,that calculates the area of a quadrilateral whose vertices are ( x1, y1), ( x2, y2),(x3, y3) and (x4, y4). This formula is easy to implement, there are no special cases,and we can even generalize the formula to all polygons.29.1 Complex numbersA complex number is a number of the form x+ yi, where i =1 is the imagi-nary unit. A geometric interpretation of a complex number is that it representsa two-dimensional point (x, y) or a vector from the origin to a point (x, y).For example, 4+2i corresponds to the following point and vector:(4,2)The C++ complex number class complex is useful when solving geometricproblems. Using the class we can represent points and vectors as complexnumbers, and the class contains tools that are useful in geometry.In the following code, C is the type of a coordinate and P is the type of a pointor a vector. In addition, the code denes macros X and Y that can be used to referto x and y coordinates.typedef long long C;typedef complex<C> P;#define X real()#define Y imag()266",
            "For example, the following code denes a point p = (4,2) and prints its x andy coordinates:P p = {4,2};cout << p.X << \" \" << p.Y << \"\\n\"; // 4 2The following code denes vectors v = (3,1) and u = (2,2), and after thatcalculates the sum s = v +u.P v = {3,1};P u = {2,2};P s = v+u;cout << s.X << \" \" << s.Y << \"\\n\"; // 5 3In practice, an appropriate coordinate type is usually long long (integer) orlong double (real number). It is a good idea to use integer whenever possible,because calculations with integers are exact. If real numbers are needed, preci-sion errors should be taken into account when comparing numbers. A safe wayto check if real numbers a and b are equal is to compare them using |a b| <,where  is a small number (for example,  = 109).FunctionsIn the following examples, the coordinate type is long double.The function abs(v) calculates the length |v| of a vector v = (x, y) using theformulax2 + y2. The function can also be used for calculating the distancebetween points (x1, y1) and (x2, y2), because that distance equals the length of thevector (x2  x1, y2  y1).The following code calculates the distance between points (4,2) and (3,1):P a = {4,2};P b = {3,-1};cout << abs(b-a) << \"\\n\"; // 3.16228The function arg(v) calculates the angle of a vector v = (x, y) with respect tothe x axis. The function gives the angle in radians, wherer radians equals 180r/degrees. The angle of a vector that points to the right is 0, and angles decreaseclockwise and increase counterclockwise.The function polar(s,a) constructs a vector whose length is s and that pointsto an angle a. A vector can be rotated by an angle a by multiplying it by a vectorwith length 1 and angle a.The following code calculates the angle of the vector (4 ,2), rotates it 1/2radians counterclockwise, and then calculates the angle again:P v = {4,2};cout << arg(v) << \"\\n\"; // 0.463648v *= polar(1.0,0.5);cout << arg(v) << \"\\n\"; // 0.963648267",
            "29.2 Points and linesThe cross product ab of vectors a = (x1, y1) and b = (x2, y2) is calculated usingthe formula x1 y2  x2 y1. The cross product tells us whether b turns left (positivevalue), does not turn (zero) or turns right (negative value) when it is placeddirectly after a.The following picture illustrates the above cases:aba b = 6aba b = 0aba b = 8For example, in the rst casea = (4,2) and b = (1,2). The following code calculatesthe cross product using the class complex:P a = {4,2};P b = {1,2};C p = (conj(a)*b).Y; // 6The above code works, because the function conj negates the y coordinate ofa vector, and when the vectors (x1,y1) and (x2, y2) are multiplied together, the ycoordinate of the result is x1 y2  x2 y1.Point locationCross products can be used to test whether a point is located on the left or rightside of a line. Assume that the line goes through points s1 and s2, we are lookingfrom s1 to s2 and the point is p.For example, in the following picture, p is on the left side of the line:s1s2pThe cross product (p s1)(p s2) tells us the location of the point p. If thecross product is positive, p is located on the left side, and if the cross product isnegative, p is located on the right side. Finally, if the cross product is zero, pointss1, s2 and p are on the same line.268",
            "Line segment intersectionNext we consider the problem of testing whether two line segments ab and cdintersect. The possible cases are:Case 1: The line segments are on the same line and they overlap each other.In this case, there is an innite number of intersection points. For example, inthe following picture, all points between c and b are intersection points:adcbIn this case, we can use cross products to check if all points are on the sameline. After this, we can sort the points and check whether the line segmentsoverlap each other.Case 2: The line segments have a common vertex that is the only intersectionpoint. For example, in the following picture the intersection point is b = c:ab = cdThis case is easy to check, because there are only four possibilities for theintersection point: a = c, a = d, b = c and b = d.Case 3: There is exactly one intersection point that is not a vertex of any linesegment. In the following picture, the point p is the intersection point:cdabpIn this case, the line segments intersect exactly when both points c and d areon different sides of a line through a and b, and points a and b are on differentsides of a line through c and d. We can use cross products to check this.Point distance from a lineAnother feature of cross products is that the area of a triangle can be calculatedusing the formula|(a  c)(b  c)|2 ,269",
            "where a, b and c are the vertices of the triangle. Using this fact, we can derivea formula for calculating the shortest distance between a point and a line. Forexample, in the following picture d is the shortest distance between the point pand the line that is dened by the points s1 and s2:s1s2pdThe area of the triangle whose vertices are s1, s2 and p can be calculatedin two ways: it is both 12 |s2  s1|d and 12 ((s1  p) (s2  p)). Thus, the shortestdistance isd = (s1  p)(s2  p)|s2 s1| .Point inside a polygonLet us now consider the problem of testing whether a point is located inside oroutside a polygon. For example, in the following picture point a is inside thepolygon and point b is outside the polygon.abA convenient way to solve the problem is to send a ray from the point to anarbitrary direction and calculate the number of times it touches the boundaryof the polygon. If the number is odd, the point is inside the polygon, and if thenumber is even, the point is outside the polygon.For example, we could send the following rays:abThe rays from a touch 1 and 3 times the boundary of the polygon, so a isinside the polygon. Correspondingly, the rays from b touch 0 and 2 times theboundary of the polygon, so b is outside the polygon.270",
            "29.3 Polygon areaA general formula for calculating the area of a polygon, sometimes called theshoelace formula, is as follows:12|n1i=1(pi  pi+1)| =12|n1i=1(xi yi+1  xi+1 yi)|,Here the vertices are p1 = (x1, y1), p2 = (x2, y2), ... , pn = (xn, yn) in such an orderthat pi and pi+1 are adjacent vertices on the boundary of the polygon, and therst and last vertex is the same, i.e., p1 = pn.For example, the area of the polygon(4,1)(7,3)(5,5)(2,4)(4,3)is|(2554)+(5375)+(7143)+(4341)+(4423)|2 = 17/2.The idea of the formula is to go through trapezoids whose one side is a side ofthe polygon, and another side lies on the horizontal line y = 0. For example:(4,1)(7,3)(5,5)(2,4)(4,3)The area of such a trapezoid is(xi+1  xi) yi + yi+12 ,where the vertices of the polygon are pi and pi+1. If xi+1 > xi, the area is positive,and if xi+1 < xi, the area is negative.The area of the polygon is the sum of areas of all such trapezoids, which yieldsthe formula|n1i=1(xi+1  xi) yi + yi+12 | =12|n1i=1(xi yi+1  xi+1 yi)|.Note that the absolute value of the sum is taken, because the value of thesum may be positive or negative, depending on whether we walk clockwise orcounterclockwise along the boundary of the polygon.271",
            "Picks theoremPicks theoremprovides another way to calculate the area of a polygon providedthat all vertices of the polygon have integer coordinates. According to Pickstheorem, the area of the polygon isa +b/21,where a is the number of integer points inside the polygon and b is the numberof integer points on the boundary of the polygon.For example, the area of the polygon(4,1)(7,3)(5,5)(2,4)(4,3)is 6+7/21 = 17/2.29.4 Distance functionsA distance function denes the distance between two points. The usual dis-tance function is the Euclidean distance where the distance between points(x1, y1) and (x2, y2) is (x2  x1)2 +(y2  y1)2.An alternative distance function is the Manhattan distance where the distancebetween points (x1, y1) and (x2, y2) is|x1  x2|+| y1  y2|.For example, consider the following picture:(2,1)(5,2)(2,1)(5,2)Euclidean distance Manhattan distanceThe Euclidean distance between the points is(52)2 +(21)2 =10and the Manhattan distance is|52|+| 21| =4.The following picture shows regions that are within a distance of 1 from thecenter point, using the Euclidean and Manhattan distances:272",
            "Euclidean distance Manhattan distanceRotating coordinatesSome problems are easier to solve if Manhattan distances are used instead ofEuclidean distances. As an example, consider a problem where we are given npoints in the two-dimensional plane and our task is to calculate the maximumManhattan distance between any two points.For example, consider the following set of points:ACBDThe maximum Manhattan distance is 5 between points B and C:ACBDA useful technique related to Manhattan distances is to rotate all coordinates45 degrees so that a point (x, y) becomes (x + y, yx). For example, after rotatingthe above points, the result is:ACBDAnd the maximum distance is as follows:273",
            "ACBDConsider two points p1 = (x1, y1) and p2 = (x2, y2) whose rotated coordinatesare p1 = (x1, y1) and p2 = (x2, y2). Now there are two ways to express the Manhat-tan distance between p1 and p2:|x1  x2|+| y1  y2| =max(|x1  x2|,|y1  y2|)For example, if p1 = (1,0) and p2 = (3,3), the rotated coordinates are p1 =(1,1) and p2 = (6,0) and the Manhattan distance is|13|+| 03| =max(|16|,| 10|) = 5.The rotated coordinates provide a simple way to operate with Manhattandistances, because we can consider x and y coordinates separately. To maximizethe Manhattan distance between two points, we should nd two points whoserotated coordinates maximize the value ofmax(|x1  x2|,|y1  y2|).This is easy, because either the horizontal or vertical difference of the rotatedcoordinates has to be maximum.274",
            "Chapter 30Sweep line algorithmsMany geometric problems can be solved using sweep line algorithms. The ideain such algorithms is to represent an instance of the problem as a set of eventsthat correspond to points in the plane. The events are processed in increasingorder according to their x or y coordinates.As an example, consider the following problem: There is a company that hasn employees, and we know for each employee their arrival and leaving times ona certain day. Our task is to calculate the maximum number of employees thatwere in the ofce at the same time.The problem can be solved by modeling the situation so that each employeeis assigned two events that correspond to their arrival and leaving times. Aftersorting the events, we go through them and keep track of the number of peoplein the ofce. For example, the tableperson arrival time leaving timeJohn 10 15Maria 6 12Peter 14 16Lisa 5 13corresponds to the following events:JohnMariaPeterLisaWe go through the events from left to right and maintain a counter. Always whena person arrives, we increase the value of the counter by one, and when a personleaves, we decrease the value of the counter by one. The answer to the problem isthe maximum value of the counter during the algorithm.In the example, the events are processed as follows:275",
            "JohnMariaPeterLisa+ +  + + 3 12 2 2 01 1The symbols + and  indicate whether the value of the counter increases ordecreases, and the value of the counter is shown below. The maximum value ofthe counter is 3 between Johns arrival and Marias leaving.The running time of the algorithm is O(nlogn), because sorting the eventstakes O(nlogn) time and the rest of the algorithm takes O(n) time.30.1 Intersection pointsGiven a set of n line segments, each of them being either horizontal or vertical,consider the problem of counting the total number of intersection points. Forexample, when the line segments arethere are three intersection points:It is easy to solve the problem in O(n2) time, because we can go through allpossible pairs of line segments and check if they intersect. However, we can solvethe problem more efciently in O(nlogn) time using a sweep line algorithm anda range query data structure.The idea is to process the endpoints of the line segments from left to rightand focus on three types of events:(1) horizontal segment begins(2) horizontal segment ends(3) vertical segment276",
            "The following events correspond to the example:1 21 21 23 3We go through the events from left to right and use a data structure thatmaintains a set of y coordinates where there is an active horizontal segment. Atevent 1, we add the y coordinate of the segment to the set, and at event 2, weremove the y coordinate from the set.Intersection points are calculated at event 3. When there is a vertical segmentbetween points y1 and y2, we count the number of active horizontal segmentswhose y coordinate is between y1 and y2, and add this number to the total numberof intersection points.To store y coordinates of horizontal segments, we can use a binary indexedor segment tree, possibly with index compression. When such structures areused, processing each event takes O(logn) time, so the total running time of thealgorithm is O(nlogn).30.2 Closest pair problemGiven a set of n points, our next problem is to nd two points whose Euclideandistance is minimum. For example, if the points arewe should nd the following points:This is another example of a problem that can be solved in O(nlogn) timeusing a sweep line algorithm1. We go through the points from left to right andmaintain a value d: the minimum distance between two points seen so far. At1Besides this approach, there is also an O(nlogn) time divide-and-conquer algorithm [56] thatdivides the points into two sets and recursively solves the problem for both sets.277",
            "each point, we nd the nearest point to the left. If the distance is less than d, itis the new minimum distance and we update the value of d.If the current point is (x, y) and there is a point to the left within a distance ofless than d, the x coordinate of such a point must be between [x d, x] and the ycoordinate must be between [yd, y+d]. Thus, it sufces to only consider pointsthat are located in those ranges, which makes the algorithm efcient.For example, in the following picture, the region marked with dashed linescontains the points that can be within a distance of d from the active point:ddThe efciency of the algorithm is based on the fact that the region alwayscontains only O(1) points. We can go through those points in O(logn) time bymaintaining a set of points whose x coordinate is between [x d, x], in increasingorder according to their y coordinates.The time complexity of the algorithm is O(nlogn), because we go through npoints and nd for each point the nearest point to the left in O(logn) time.30.3 Convex hull problemA convex hull is the smallest convex polygon that contains all points of a givenset. Convexity means that a line segment between any two vertices of the polygonis completely inside the polygon.For example, for the pointsthe convex hull is as follows:278",
            "Andrews algorithm[3] provides an easy way to construct the convex hullfor a set of points in O(nlogn) time. The algorithm rst locates the leftmostand rightmost points, and then constructs the convex hull in two parts: rst theupper hull and then the lower hull. Both parts are similar, so we can focus onconstructing the upper hull.First, we sort the points primarily according to x coordinates and secondarilyaccording to y coordinates. After this, we go through the points and add eachpoint to the hull. Always after adding a point to the hull, we make sure thatthe last line segment in the hull does not turn left. As long as it turns left, werepeatedly remove the second last point from the hull.The following pictures show how Andrews algorithm works:1 2 3 45 6 7 89 10 11 1213 14 15 1617 18 19 20279",
            "280",
            "Bibliography[1] A. V. Aho, J. E. Hopcroft and J. Ullman. Data Structures and Algorithms ,Addison-Wesley, 1983.[2] R. K. Ahuja and J. B. Orlin. Distance directed augmenting path algorithmsfor maximum ow and parametric maximum ow problems. Naval ResearchLogistics, 38(3):413430, 1991.[3] A. M. Andrew. Another efcient algorithm for convex hulls in two dimensions.Information Processing Letters, 9(5):216219, 1979.[4] B. Aspvall, M. F. Plass and R. E. Tarjan. A linear-time algorithm for testingthe truth of certain quantied boolean formulas. Information ProcessingLetters, 8(3):121123, 1979.[5] R. Bellman. On a routing problem. Quarterly of Applied Mathematics ,16(1):8790, 1958.[6] M. Beck, E. Pine, W. Tarrat and K. Y. Jensen. New integer representationsas the sum of three cubes. Mathematics of Computation, 76(259):16831690,2007.[7] M. A. Bender and M. Farach-Colton. The LCA problem revisited. In LatinAmerican Symposium on Theoretical Informatics, 8894, 2000.[8] J. Bentley. Programming Pearls. Addison-Wesley, 1999 (2nd edition).[9] J. Bentley and D. Wood. An optimal worst case algorithm for reporting inter-sections of rectangles. IEEE Transactions on Computers, C-29(7):571577,1980.[10] C. L. Bouton. Nim, a game with a complete mathematical theory.Annals ofMathematics, 3(1/4):3539, 1901.[11] Croatian Open Competition in Informatics, http://hsin.hr/coci/[12] Codeforces: On Mos algorithm, http://codeforces.com/blog/entry/20032[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest and C. Stein. Introduction toAlgorithms, MIT Press, 2009 (3rd edition).281",
            "[14] E. W. Dijkstra. A note on two problems in connexion with graphs. Nu-merische Mathematik, 1(1):269271, 1959.[15] K. Diks et al. Looking for a Challenge? The Ultimate Problem Set fromthe University of Warsaw Programming Competitions, University of Warsaw,2012.[16] M. Dima and R. Ceterchi. Efcient range minimum queries using binaryindexed trees. Olympiad in Informatics, 9(1):3944, 2015.[17] J. Edmonds. Paths, trees, and owers.Canadian Journal of Mathematics,17(3):449467, 1965.[18] J. Edmonds and R. M. Karp. Theoretical improvements in algorithmic ef-ciency for network ow problems. Journal of the ACM, 19(2):248264, 1972.[19] S. Even, A. Itai and A. Shamir. On the complexity of time table and multi-commodity ow problems. 16th Annual Symposium on Foundations of Com-puter Science, 184193, 1975.[20] D. Fanding. A faster algorithm for shortest-path  SPFA.Journal of South-west Jiaotong University, 2, 1994.[21] P. M. Fenwick. A new data structure for cumulative frequency tables.Soft-ware: Practice and Experience, 24(3):327336, 1994.[22] J. Fischer and V. Heun. Theoretical and practical improvements on theRMQ-problem, with applications to LCA and LCE. In Annual Symposium onCombinatorial Pattern Matching, 3648, 2006.[23] R. W. Floyd Algorithm 97: shortest path. Communications of the ACM ,5(6):345, 1962.[24] L. R. Ford. Network ow theory. RAND Corporation, Santa Monica, Califor-nia, 1956.[25] L. R. Ford and D. R. Fulkerson. Maximal ow through a network. CanadianJournal of Mathematics, 8(3):399404, 1956.[26] R. Freivalds. Probabilistic machines can use less running time. In IFIPcongress, 839842, 1977.[27] F. Le Gall. Powers of tensors and fast matrix multiplication. In Proceedingsof the 39th International Symposium on Symbolic and Algebraic Computation,296303, 2014.[28] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide tothe Theory of NP-Completeness, W. H. Freeman and Company, 1979.[29] Google Code Jam Statistics (2017), https://www.go-hero.net/jam/17282",
            "[30] A. Grnlund and S. Pettie. Threesomes, degenerates, and love triangles.In Proceedings of the 55th Annual Symposium on Foundations of ComputerScience, 621630, 2014.[31] P. M. Grundy. Mathematics and games. Eureka, 2(5):68, 1939.[32] D. Guseld. Algorithms on Strings, Trees and Sequences: Computer Scienceand Computational Biology, Cambridge University Press, 1997.[33] S. Halim and F. Halim. Competitive Programming 3: The New Lower Boundof Programming Contests, 2013.[34] M. Held and R. M. Karp. A dynamic programming approach to sequencingproblems. Journal of the Society for Industrial and Applied Mathematics ,10(1):196210, 1962.[35] C. Hierholzer and C. Wiener. ber die Mglichkeit, einen Linienzug ohneWiederholung und ohne Unterbrechung zu umfahren. Mathematische An-nalen, 6(1), 3032, 1873.[36] C. A. R. Hoare. Algorithm 64: Quicksort. Communications of the ACM ,4(7):321, 1961.[37] C. A. R. Hoare. Algorithm 65: Find. Communications of the ACM, 4(7):321322, 1961.[38] J. E. Hopcroft and J. D. Ullman. A linear list merging algorithm. Technicalreport, Cornell University, 1971.[39] E. Horowitz and S. Sahni. Computing partitions with applications to theknapsack problem. Journal of the ACM, 21(2):277292, 1974.[40] D. A. Huffman. A method for the construction of minimum-redundancycodes. Proceedings of the IRE, 40(9):10981101, 1952.[41] The International Olympiad in Informatics Syllabus, https://people.ksp.sk/~misof/ioi-syllabus/[42] R. M. Karp and M. O. Rabin. Efcient randomized pattern-matching algo-rithms. IBM Journal of Research and Development, 31(2):249260, 1987.[43] P. W. Kasteleyn. The statistics of dimers on a lattice: I. The number of dimerarrangements on a quadratic lattice. Physica, 27(12):12091225, 1961.[44] C. Kent, G. M. Landau and M. Ziv-Ukelson. On the complexity of sparseexon assembly. Journal of Computational Biology, 13(5):10131027, 2006.[45] J. Kleinberg and . Tardos. Algorithm Design, Pearson, 2005.[46] D. E. Knuth. The Art of Computer Programming. Volume 2: SeminumericalAlgorithms, AddisonWesley, 1998 (3rd edition).283",
            "[47] D. E. Knuth. The Art of Computer Programming. Volume 3: Sorting andSearching, AddisonWesley, 1998 (2nd edition).[48] J. B. Kruskal. On the shortest spanning subtree of a graph and the travel-ing salesman problem. Proceedings of the American Mathematical Society,7(1):4850, 1956.[49] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions,and reversals. Soviet physics doklady, 10(8):707710, 1966.[50] M. G. Main and R. J. Lorentz. An O(nlogn) algorithm for nding all repeti-tions in a string. Journal of Algorithms, 5(3):422432, 1984.[51] J. Pachocki and J. Radoszewski. Where to use and how not to use polynomialstring hashing. Olympiads in Informatics, 7(1):90100, 2013.[52] I. Parberry. An efcient algorithm for the Knights tour problem.DiscreteApplied Mathematics, 73(3):251260, 1997.[53] D. Pearson. A polynomial-time algorithm for the change-making problem.Operations Research Letters, 33(3):231234, 2005.[54] R. C. Prim. Shortest connection networks and some generalizations. BellSystem Technical Journal, 36(6):13891401, 1957.[55] 27-Queens Puzzle: Massively Parallel Enumeration and Solution Counting.https://github.com/preusser/q27[56] M. I. Shamos and D. Hoey. Closest-point problems. InProceedings of the 16thAnnual Symposium on Foundations of Computer Science, 151162, 1975.[57] M. Sharir. A strong-connectivity algorithm and its applications in data owanalysis. Computers & Mathematics with Applications, 7(1):6772, 1981.[58] S. S. Skiena. The Algorithm Design Manual, Springer, 2008 (2nd edition).[59] S. S. Skiena and M. A. Revilla. Programming Challenges: The ProgrammingContest Training Manual, Springer, 2003.[60] SZKOpu, https://szkopul.edu.pl/[61] R. Sprague. ber mathematische Kampfspiele. Tohoku Mathematical Jour-nal, 41:438444, 1935.[62] P. Stanczyk. Algorytmika praktyczna w konkursach Informatycznych, MScthesis, University of Warsaw, 2006.[63] V. Strassen. Gaussian elimination is not optimal. Numerische Mathematik,13(4):354356, 1969.[64] R. E. Tarjan. Efciency of a good but not linear set union algorithm.Journalof the ACM, 22(2):215225, 1975.284",
            "[65] R. E. Tarjan. Applications of path compression on balanced trees. Journal ofthe ACM, 26(4):690715, 1979.[66] R. E. Tarjan and U. Vishkin. Finding biconnected componemts and comput-ing tree functions in logarithmic parallel time. In Proceedings of the 25thAnnual Symposium on Foundations of Computer Science, 1220, 1984.[67] H. N. V. Temperley and M. E. Fisher. Dimer problem in statistical mechanics an exact result. Philosophical Magazine, 6(68):10611063, 1961.[68] USA Computing Olympiad, http://www.usaco.org/[69] H. C. von Warnsdorf.Des Rsselsprunges einfachste und allgemeinste Lsung.Schmalkalden, 1823.[70] S. Warshall. A theorem on boolean matrices.Journal of the ACM, 9(1):1112,1962.285",
            "286",
            "Index2SAT problem, 1602SUM problem, 783SAT problem, 1623SUM problem, 79adjacency list, 113adjacency matrix, 114alphabet, 243amortized analysis, 77ancestor, 163and operation, 96Andrews algorithm, 279antichain, 193arithmetic progression, 10backtracking, 50BellmanFord algorithm, 123binary code, 62binary indexed tree, 86binary search, 31binary tree, 139Binets formula, 14binomial coefcient, 208binomial distribution, 230bipartite graph, 112, 122birthday paradox, 247bit representation, 95bit shift, 97bitset, 41border, 244breadth-rst search, 119bubble sort, 25Burnsides lemma, 214Catalan number, 210Cayleys formula, 215child, 133Chinese remainder theorem, 205closest pair, 277codeword, 62cofactor, 219collision, 246coloring, 112, 233combinatorics, 207comparison function, 31comparison operator, 30complement, 12complete graph, 111complex, 266complex number, 266complexity classes, 20component, 110component graph, 157conditional probability, 227conjuction, 13connected graph, 110, 121constant factor, 21constant-time algorithm, 20coprime, 201counting sort, 28cross product, 268cubic algorithm, 20cut, 182cycle, 109, 121, 149, 155cycle detection, 155data compression, 62data structure, 35De Bruijn sequence, 178degree, 111depth-rst search, 117deque, 42derangement, 213determinant, 219diameter, 135287",
            "difference, 12difference array, 93Dijkstras algorithm, 126, 153Dilworths theorem, 193Diophantine equation, 204Diracs theorem, 177directed graph, 110disjunction, 13distance function, 272distribution, 229divisibility, 197divisor, 197dynamic array, 35dynamic programming, 65dynamic segment tree, 261edge, 109edge list, 115edit distance, 74EdmondsKarp algorithm, 184equivalence, 13Euclids algorithm, 200Euclids formula, 206Euclidean distance, 272Euler tour technique, 168Eulers theorem, 202Eulers totient function, 201Eulerian circuit, 174Eulerian path, 173expected value, 229extended Euclids algorithm, 204factor, 197factorial, 14Faulhabers formula, 10Fenwick tree, 86Fermats theorem, 202Fibonacci number, 14, 206, 220oating point number, 7ow, 181Floyds algorithm, 156FloydWarshall algorithm, 129FordFulkerson algorithm, 182Freivalds algoritm, 232functional graph, 154geometric distribution, 230geometric progression, 11geometry, 265Goldbachs conjecture, 199graph, 109greatest common divisor, 200greedy algorithm, 57Grundy number, 238Grundys game, 241Halls theorem, 189Hamiltonian circuit, 177Hamiltonian path, 177Hamming distance, 100harmonic sum, 11, 200hash value, 245hashing, 245heap, 43Herons formula, 265heuristic, 179Hierholzers algorithm, 175Huffman coding, 63identity matrix, 218implication, 13in-order, 139inclusion-exclusion, 212indegree, 111independence, 228independent set, 190index compression, 93input and output, 4integer, 6intersection, 12intersection point, 276inverse matrix, 220inversion, 26iterator, 39Konigs theorem, 189Kadanes algorithm, 23Kirchhoffs theorem, 223knapsack, 72knights tour, 179Kosarajus algorithm, 158Kruskals algorithm, 142Lagranges theorem, 205Laplacean matrix, 224288",
            "Las Vegas algorithm, 231lazy propagation, 258lazy segment tree, 258leaf, 133least common multiple, 200Legendres conjecture, 199Levenshtein distance, 74lexicographical order, 244line segment intersection, 269linear algorithm, 20linear recurrence, 220logarithm, 14logarithmic algorithm, 20logic, 13longest increasing subsequence, 70losing state, 235lowest common ancestor, 167macro, 9Manhattan distance, 272map, 38Markov chain, 230matching, 187matrix, 217matrix multiplication, 218, 232matrix power, 219maximum ow, 181maximum independent set, 190maximum matching, 187maximum query, 83maximum spanning tree, 142maximum subarray sum, 21meet in the middle, 54memoization, 67merge sort, 27mex function, 238minimum cut, 182, 185minimum node cover, 189minimum query, 83minimum spanning tree, 141misre game, 238Mos algorithm, 255modular arithmetic, 6, 201modular inverse, 202Monte Carlo algorithm, 231multinomial coefcient, 210natural logarithm, 15nearest smaller elements, 79negation, 13negative cycle, 125neighbor, 111next_permutation, 49nim game, 237nim sum, 237node, 109node cover, 189not operation, 97NP-hard problem, 20number theory, 197or operation, 96order statistic, 232Ores theorem, 177outdegree, 111pair, 30parent, 133parenthesis expression, 211Pascals triangle, 209path, 109path cover, 190pattern matching, 243perfect matching, 189perfect number, 198period, 243permutation, 49persistent segment tree, 262Picks theorem, 272point, 266polynomial algorithm, 20polynomial hashing, 245post-order, 139Prfer code, 216pre-order, 139predicate, 13prex, 243prex sum array, 84Prims algorithm, 147prime, 197prime decomposition, 197priority queue, 43probability, 225programming language, 3289",
            "Pythagorean triple, 206quadratic algorithm, 20quantier, 13queen problem, 50queue, 43quickselect, 232quicksort, 232random variable, 228random_shuffle, 39randomized algorithm, 231range query, 83regular graph, 111remainder, 6reverse, 39root, 133rooted tree, 133rotation, 243scaling algorithm, 185segment tree, 89, 257set, 12, 37set theory, 12shoelace formula, 271shortest path, 123sieve of Eratosthenes, 200simple graph, 112sliding window, 81sliding window minimum, 81sort, 29, 39sorting, 25spanning tree, 141, 223sparse segment tree, 261sparse table, 85SPFA algorithm, 126SpragueGrundy theorem, 238square matrix, 217square root algorithm, 251stack, 42string, 36, 243string hashing, 245strongly connected component, 157strongly connected graph, 157subsequence, 243subset, 12, 47substring, 243subtree, 133successor graph, 154sufx, 243sum query, 83sweep line, 275time complexity, 17topological sorting, 149transpose, 217tree, 110, 133tree query, 163tree traversal array, 164trie, 244tuple, 30typedef, 8twin prime, 199two pointers method, 77two-dimensional segment tree, 264uniform distribution, 230union, 12union-nd structure, 145universal set, 12vector, 35, 217, 266Warnsdorfs rule, 179weighted graph, 111Wilsons theorem, 206winning state, 235xor operation, 97Z-algorithm, 247Z-array, 247Zeckendorfs theorem, 206290"
        ]
    }
}