{
    "contents": {
        "0": [
            "1\nSoftware Testing with Large Language Models:\nSurvey, Landscape, and Vision\nJunjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing Wang\nAbstract\u2014Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language\nprocessing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide\nrange of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability\nof software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing\ntechniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides\na comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software\ntesting, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for\nwhich LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes\nthe commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs.\nIt also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research\nin this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in\nsoftware testing.\nIndex Terms\u2014Pre-trained Large Language Model, Software Testing, LLM, GPT\n\u2726\n1 I NTRODUCTION\nSoftware testing is a crucial undertaking that serves as\na cornerstone for ensuring the quality and reliability of\nsoftware products. Without the rigorous process of software\ntesting, software enterprises would be reluctant to release\ntheir products into the market, knowing the potential\nconsequences of delivering flawed software to end-users.\nBy conducting thorough and meticulous testing procedures,\nsoftware enterprises can minimize the occurrence of critical\nsoftware failures, usability issues, or security breaches\nthat could potentially lead to financial losses or jeopardize\nuser trust. Additionally, software testing helps to reduce\nmaintenance costs by identifying and resolving issues early\nin the development lifecycle, preventing more significant\ncomplications down the line [1], [2].\nThe significance of software testing has garnered sub-\nstantial attention within the research and industrial com-\nmunities. In the field of software engineering, it stands as\nan immensely popular and vibrant research area. One can\nobserve the undeniable prominence of software testing by\nsimply examining the landscape of conferences and sym-\nposiums focused on software engineering. Amongst these\nevents, topics related to software testing consistently domi-\nnate the submission numbers and are frequently selected for\npublication.\n\u25cf J. Wang,Y. Huang, Z. Liu, Q. Wang are with State Key Laboratory of\nIntelligent Game, Institute of Software Chinese Academy of Sciences, and\nUniversity of Chinese Academy of Sciences, Beijing, China. J. Wang and\nQ. Wang are corresponding authors.\nE-mail: {junjie, yuchao2019, liuzhe2020, wq}@iscas.ac.cn\n\u25cf C. Chen is with Monash University, Melbourne, Australia\nE-mail: chunyang.chen@monash.edu\n\u25cf S. Wang is with York University, Toronto, Canada.\nE-mail: wangsong@yorku.ca\nWhile the field of software testing has gained signifi-\ncant popularity, there remain dozens of challenges that have\nnot been effectively addressed. For example, one such chal-\nlenge is automated unit test case generation. Although var-\nious approaches, including search-based [3], [4], constraint-\nbased [5] or random-based [6] techniques to generate a suite\nof unit tests, the coverage and the meaningfulness of the\ngenerated tests are still far from satisfactory [7], [8]. Simi-\nlarly, when it comes to mobile GUI testing, existing studies\nwith random-/rule-based methods [9], [10], model-based\nmethods [11], [12], and learning-based methods [13] are un-\nable to understand the semantic information of the GUI\npage and often fall short in achieving comprehensive cov-\nerage [14], [15]. Considering these limitations, numerous re-\nsearch efforts are currently underway to explore innovative\ntechniques that can enhance the efficacy of software testing\ntasks, among which large language models are the most\npromising ones.\nLarge language models (LLMs) such as T5 and GPT-3\nhave revolutionized the field of natural language processing\n(NLP) and artificial intelligence (AI). These models, initially\npre-trained on extensive corpora, have exhibited remarkable\nperformance across a wide range of NLP tasks including\nquestion-answering, machine translation, and text genera-\ntion [16]\u2013[19]. In recent years, there has been a significant\nadvancement in LLMs with the emergence of models capa-\nble of handling even larger-scale datasets. This expansion\nin model size has not only led to improved performance\nbut also opened up new possibilities for applying LLMs\nas Artificial General Intelligence. Among these advanced\nLLMs, models like ChatGPT 1 and LLaMA 2 boast billions\n1. https://openai.com/blog/chatgpt\n2. https://ai.meta.com/blog/large-language-model-llama-meta-ai/\narXiv:2307.07221v3  [cs.SE]  4 Mar 2024",
            "2\nof parameters. Such models hold tremendous potential for\ntackling complex practical tasks in domains like code gener-\nation and artistic creation. With their expanded capacity and\nenhanced capabilities, LLMs have become game-changers in\nNLP and AI, and are driving advancements in other fields\nlike coding and software testing.\nLLMs have been used for various coding-related tasks\nincluding code generation and code recommendation [20]\u2013\n[23]. On one hand, in software testing, there are many tasks\nrelated to code generation, such as unit test generation [7],\nwhere the utilization of LLMs is expected to yield good\nperformance. On the other hand, software testing possesses\nunique characteristics that differentiate it from code gener-\nation. For example, code generation primarily focuses on\nproducing a single, correct code snippet, whereas software\ntesting often requires generating diverse test inputs to en-\nsure better coverage of the software under test [1]. The ex-\nistence of these differences introduces new challenges and\nopportunities when employing LLMs for software testing.\nMoreover, people have benefited from the excellent perfor-\nmance of LLMs in generation and inference tasks, leading\nto the emergence of dozens of new practices that use LLMs\nfor software testing.\nThis article presents a comprehensive review of the uti-\nlization of LLMs in software testing. We collect 102 relevant\npapers and conduct a thorough analysis from both software\ntesting and LLMs perspectives, as roughly summarized in\nFigure 1.\nFrom the viewpoint of software testing, our analysis in-\nvolves an examination of the specific software testing tasks\nfor which LLMs are employed. Results show that LLMs are\ncommonly used for test case preparation (including unit test\ncase generation, test oracle generation, and system test input\ngeneration), program debugging, and bug repair, while we\ndo not find the practices for applying LLMs in the tasks of\nearly testing life-cycle (such as test requirement, test plan,\netc). For each test task, we would provide detailed illustra-\ntions showcasing the utilization of LLMs in addressing the\ntask, highlighting commonly-used practices, tracking tech-\nnology evolution trends, and summarizing achieved per-\nformance, so as to facilitate readers in gaining a thorough\noverview of how LLMs are employed across various testing\ntasks.\nFrom the viewpoint of LLMs, our analysis includes\nthe commonly used LLMs in these studies, the types of\nprompt engineering, the input of the LLMs, as well as\nthe accompanied techniques with these LLMs. Results\nshow that about one-third of the studies utilize the LLMs\nthrough pre-training or fine-tuning schema, while the others\nemploy prompt engineering to communicate with LLMs\nto steer their behavior for desired outcomes. For prompt\nengineering, the zero-shot learning and few-shot learning\nstrategies are most commonly used, while other advances\nlike chain-of-thought promoting and self-consistency are\nrarely utilized. Results also show that traditional testing\ntechniques like differential testing and mutation testing\nare usually accompanied by LLMs to help generate more\ndiversified tests.\nFurthermore, we summarize the key challenges and po-\ntential opportunities in this direction. Although software\ntesting with LLMs has undergone significant growth in the\nFig. 1: Structure of the contents in this paper (the numbers\nin bracket indicates the number of involved papers, and a\npaper might involve zero or multiple items)\npast two years, there are still challenges in achieving high\ncoverage of the testing, test oracle problem, rigorous evalu-\nations, and real-world application of LLMs in software test-\ning. Since it is a new emerging field, there are many research\nopportunities, including exploring LLMs in an early stage of\ntesting, exploring LLMs for more types of software and non-\nfunctional testing, exploring advanced prompt engineering,\nas well as incorporating LLMs with traditional techniques.\nThis paper makes the following contributions:\n\u25cf We thoroughly analyze 102 relevant studies that used\nLLMs for software testing, regarding publication\ntrends, distribution of publication venues, etc.\n\u25cf We conduct a comprehensive analysis from the perspec-\ntive of software testing to understand the distribution of\nsoftware testing tasks with LLM and present a thorough\ndiscussion about how these tasks are solved with LLM.\n\u25cf We conduct a comprehensive analysis from the perspec-\ntive of LLMs, and uncover the commonly-used LLMs,\nthe types of prompt engineering, input of the LLMs, as\nwell as the accompanied techniques with these LLMs.\n\u25cf We highlight the challenges in existing studies and\npresent potential opportunities for further studies.\nWe believe that this work will be valuable to both re-\nsearchers and practitioners in the field of software engineer-\ning, as it provides a comprehensive overview of the current\nstate and future vision of using LLMs for software testing.\nFor researchers, this work can serve as a roadmap for future\nresearch in this area, highlighting potential avenues for ex-\nploration and identifying gaps in our current understanding\nof the use of LLMs in software testing. For practitioners, this\nwork can provide insights into the potential benefits and\nlimitations of using LLMs for software testing, as well as\npractical guidance on how to effectively integrate them into",
            "3\nexisting testing processes. By providing a detailed landscape\nof the current state and future vision of using LLMs for\nsoftware testing, this work can help accelerate the adoption\nof this technology in the software engineering community\nand ultimately contribute to improving the quality and reli-\nability of software systems.\n2 B ACKGROUND\n2.1 Large Language Model (LLM)\nRecently, pre-trained language models (PLMs) have been\nproposed by pretraining Transformer-based models over\nlarge-scale corpora, showing strong capabilities in solving\nvarious natural language processing (NLP) tasks [16]\u2013[19].\nStudies have shown that model scaling can lead to improved\nmodel capacity, prompting researchers to investigate the\nscaling effect through further parameter size increases.\nInterestingly, when the parameter scale exceeds a certain\nthreshold, these larger language models demonstrate not\nonly significant performance improvements but also special\nabilities such as in-context learning, which are absent in\nsmaller models such as BERT.\nTo discriminate the language models in different\nparameter scales, the research community has coined\nthe term large language models (LLM) for the PLMs of\nsignificant size. LLMs typically refer to language models\nthat have hundreds of billions (or more) of parameters and\nare trained on massive text data such as GPT-3, PaLM,\nCodex, and LLaMA. LLMs are built using the Transformer\narchitecture, which stacks multi-head attention layers\nin a very deep neural network. Existing LLMs adopt\nsimilar model architectures (Transformer) and pre-training\nobjectives (language modeling) as small language models,\nbut largely scale up the model size, pre-training data,\nand total compute power. This enables LLMs to better\nunderstand natural language and generate high-quality text\nbased on given context or prompts.\nNote that, in existing literature, there is no formal con-\nsensus on the minimum parameter scale for LLMs, since\nthe model capacity is also related to data size and total\ncompute. In a recent survey of LLMs [17], the authors focus\non discussing the language models with a model size larger\nthan 10B. Under their criteria, the first LLM is T5 released\nby Google in 2019, followed by GPT-3 released by OpenAI\nin 2020, and there are more than thirty LLMs released be-\ntween 2021 and 2023 indicating its popularity. In another\nsurvey of unifying LLMs and knowledge graphs [24], the\nauthors categorize the LLMs into three types: encoder-only\n(e.g., BERT), encoder-decoder (e.g., T5), and decoder-only\nnetwork architecture (e.g., GPT-3). In our review, we take\ninto account the categorization criteria of the two surveys\nand only consider the encoder-decoder and decoder-only\nnetwork architecture of pre-training language models, since\nthey can both support generative tasks. We do not consider\nthe encoder-only network architecture because they cannot\nhandle generative tasks, were proposed relatively early (e.g.,\nBERT in 2018), and there are almost no models using this\narchitecture after 2021. In other words, the LLMs discussed\nin this paper not only include models with parameters of\nover 10B (as mentioned in [17]) but also include other mod-\nels that use the encoder-decoder and decoder-only network\narchitecture (as mentioned in [24]), such as BART with 140M\nparameters and GPT-2 with parameter sizes ranging from\n117M to 1.5B. This is also to potentially include more studies\nto demonstrate the landscape of this topic.\n2.2 Software Testing\nSoftware testing is a crucial process in software develop-\nment that involves evaluating the quality of a software prod-\nuct. The primary goal of software testing is to identify de-\nfects or errors in the software system that could potentially\nlead to incorrect or unexpected behavior. The whole life\ncycle of software testing typically includes the following\ntasks (demonstrated in Figure 4):\n\u25cf Requirement Analysis: analyze the software require-\nments and identify the testing objectives, scope, and\ncriteria.\n\u25cf Test Plan: develop a test plan that outlines the testing\nstrategy, test objectives, and schedule.\n\u25cf Test Design and Review: develop and review the test\ncases and test suites that align with the test plan and\nthe requirements of the software application.\n\u25cf Test Case Preparation: the actual test cases are prepared\nbased on the designs created in the previous stage.\n\u25cf Test Execution: execute the tests that were designed in\nthe previous stage. The software system is executed\nwith the test cases and the results are recorded.\n\u25cf Test Reporting: analyze the results of the tests and gen-\nerate reports that summarize the testing process and\nidentify any defects or issues that were discovered.\n\u25cf Bug Fixing and Regression Testing: defects or issues\nidentified during testing are reported to the develop-\nment team for fixing. Once the defects are fixed, regres-\nsion testing is performed to ensure that the changes\nhave not introduced new defects or issues.\n\u25cf Software Release: once the software system has passed\nall of the testing stages and the defects have been fixed,\nthe software can be released to the customer or end\nuser.\nThe testing process is iterative and may involve multiple\ncycles of the above stages, depending on the complexity of\nthe software system and the testing requirements.\nDuring the testing phase, various types of tests may be\nperformed, including unit tests, integration tests, system\ntests, and acceptance tests.\n\u25cf Unit Testing involves testing individual units or com-\nponents of the software application to ensure that they\nfunction correctly.\n\u25cf Integration Testing involves testing different modules\nor components of the software application together to\nensure that they work correctly as a system.\n\u25cf System Testing involves testing the entire software sys-\ntem as a whole, including all the integrated components\nand external dependencies.\n\u25cf Acceptance Testing involves testing the software appli-\ncation to ensure that it meets the business requirements\nand is ready for deployment.\nIn addition, there can be functional testing, performance\ntesting, unit testing, security testing, accessibility testing,\netc, which explores various aspects of the software under\ntest [25].",
            "4\n3.1.1 Automatic \nSearch\n3.1.1 Automatic \nFiltering\n3.1.4 Quality \nAssessment3.1.5 Snowballing\n14,623 \nPapers\n102\nPapers\n1,239 \nPapers\n109 \nPapers\nSTART\n102 \nPapers\nMajor SE Venues\n& AI Venues\n3.1.2 Manual Search\n1,278 \nPapers\n3.1.3 Inclusion and \nExclusion Criteria\nFig. 2: Overview of the paper collection process\n3 P APER SELECTION AND REVIEW SCHEMA\n3.1 Paper Collection Methodology\nFigure 2 shows our paper search and selection process. To\ncollect as much relevant literature as possible, we use both\nautomatic search (from paper repository database) and man-\nual search (from major software engineering and artificial\nintelligence venues). We searched papers from Jan. 2019 to\nJun. 2023 and further conducted the second round of search\nto include the papers from Jul. 2023 to Oct. 2023.\n3.1.1 Automatic Search\nTo ensure that we collect papers from diverse research areas,\nwe conduct an extensive search using four popular scientific\ndatabases: ACM digital library, IEEE Xplore digital library,\narXiv, and DBLP .\nWe search for papers whose title contains keywords re-\nlated to software testing tasks and testing techniques (as shown\nbelow) in the first three databases. In the case of DBLP , we\nuse additional keywords related to LLMs (as shown below)\nto filter out irrelevant studies, as relying solely on testing-\nrelated keywords would result in a large number of can-\ndidate studies. While using two sets of keywords for DBLP\nmay result in overlooking certain related studies, we believe\nit is still a feasible strategy. This is due to the fact that a\nsubstantial number of studies present in this database can\nalready be found in the first three databases, and the fourth\ndatabase only serves as a supplementary source for collect-\ning additional papers.\n\u25cf Keywords related with software testing tasks and tech-\nniques: test OR bug OR issue OR defect OR fault OR\nerror OR failure OR crash OR debug OR debugger OR\nrepair OR fix OR assert OR verification OR validation\nOR fuzz OR fuzzer OR mutation.\n\u25cf Keywords related with LLMs: LLMOR language model\nOR generative model OR large model OR GPT-3 OR\nChatGPT OR GPT-4 OR LLaMA OR PaLM2 OR CodeT5\nOR CodeX OR CodeGen OR Bard OR InstructGPT. Note\nthat, we only list the top ten most popular LLMs (based\non Google search), since they are the search keywords\nfor matching paper titles, rather than matching the pa-\nper content.\nThe above search strategy based on the paper title can\nrecall a large number of papers, and we further conduct the\nautomatic filtering based on the paper content. Specifically,\nwe filter the paper whose content contains \u201cLLM\u201d or \u201clan-\nguage model\u201d or \u201cgenerative model\u201d or \u201clarge model\u201d or\nthe name of the LLMs (using the LLMs in [17], [24] except\nthose in our exclusion criteria). This can help eliminate the\npapers that do not involve the neural models.\n3.1.2 Manual Search\nTo compensate for the potential omissions that may result\nfrom automated searches, we also conduct manual searches.\nIn order to make sure we collect highly relevant papers,\nwe conduct a manual search within the conference proceed-\nings and journal articles from top-tier software engineering\nvenues (listed in Table 2).\nIn addition, given the interdisciplinary nature of this\nwork, we also include the conference proceedings of the\nartificial intelligence field. We select the top ten venues\nbased on the h5 index from Google Scholar, and exclude\nthree computer vision venues, i.e., CVPR, ICCV , ECCV , as\nlisted in Table 2.\n3.1.3 Inclusion and Exclusion Criteria\nThe search conducted on the databases and venue is, by de-\nsign, very inclusive. This allows us to collect as many papers\nas possible in our pool. However, this generous inclusivity\nresults in having papers that are not directly related to the\nscope of this survey. Accordingly, we define a set of specific\ninclusion and exclusion criteria and then we apply them to\neach paper in the pool and remove papers not meeting the\ncriteria. This ensures that each collected paper aligns with\nour scope and research questions.\nInclusion Criteria. We define the following criteria for\nincluding papers:\n\u25cf The paper proposes or improves an approach, study, or\ntool/framework that targets testing specific software or\nsystems with LLMs.\n\u25cf The paper applies LLMs to software testing practice,\nincluding all tasks within the software testing lifecycle\nas demonstrated in Section 2.2.\n\u25cf The paper presents an empirical or experimental study\nabout utilizing LLMs in software testing practice.\n\u25cf The paper involves specific testing techniques (e.g.,\nfuzz testing) employing LLMs.\nIf a paper satisfies any of the following criteria, we will\ninclude it.\nExclusion Criteria. The following studies would be ex-\ncluded during study selection:\n\u25cf The paper does not involve software testing tasks, e.g.,\ncode comment generation.\n\u25cf The paper does not utilize LLMs, e.g., using recurrent\nneural networks.\n\u25cf The paper mentions LLMs only in future work or dis-\ncussions rather than using LLMs in the approach.\n\u25cf The paper utilizes language models with encoder-only\narchitecture, e.g., BERT, which can not directly be uti-\nlized for generation tasks (as demonstrated in Section\n2.1).\n\u25cf The paper focuses on testing the performance of LLMs,\nsuch as fairness, stability, security, etc. [125]\u2013[127].\n\u25cf The paper focuses on evaluating the performance of\nLLM-enabled tools, e.g., evaluating the code quality of\nthe code generation tool Copilot [128]\u2013[130].\nFor the papers collected through automatic search and\nmanual search, we conduct a manual inspection to check\nwhether they satisfy our inclusion criteria and filter those\nfollowing our exclusion criteria. Specifically, the first two\nauthors read each paper to carefully determine whether it",
            "5\nTABLE 1: Details of the collected papers\nID Topic Paper title Year Reference\n1 Unit test case generation Unit Test Case Generation with Transformers and Focal Context 2020 [26]\n2 Unit test case generation Codet: Code Generation with Generated Tests 2022 [27]\n3 Unit test case generation Interactive Code Generation via Test-Driven User-Intent Formalization 2022 [28]\n4 Unit test case generation A3Test: Assertion-Augmented Automated Test Case Generation 2023 [29]\n5 Unit test case generation An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation 2023 [30]\n6 Unit test case generation An Initial Investigation of ChatGPT Unit Test Generation Capability 2023 [31]\n7 Unit test case generation Automated Test Case Generation Using Code Models and Domain Adaptation 2023 [32]\n8 Unit test case generation Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models 2023 [33]\n9 Unit test case generation Can Large Language Models Write Good Property-Based Tests? 2023 [34]\n10 Unit test case generation CAT-LM Training Language Models on Aligned Code And Tests 2023 [35]\n11 Unit test case generation ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation 2023 [8]\n12 Unit test case generation ChatUniTest: a ChatGPT-based Automated Unit Test Generation Tool 2023 [36]\n13 Unit test case generation CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models 2023 [37]\n14 Unit test case generation Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing 2023 [38]\n15 Unit test case generation Exploring the Effectiveness of Large Language Models in Generating Unit Tests 2023 [39]\n16 Unit test case generation How Well does LLM Generate Security Tests? 2023 [40]\n17 Unit test case generation No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation 2023 [7]\n18 Unit test case generation Prompting Code Interpreter to Write Better Unit Tests on Quixbugs Functions 2023 [41]\n19 Unit test case generation Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation 2023 [42]\n20 Unit test case generation Unit Test Generation using Generative AI: A Comparative Performance Analysis of Autogeneration Tools 2023 [43]\n21 Test oracle generation Generating Accurate Assert Statements for Unit Test Cases Using Pretrained Transformers 2022 [44]\n22 Test oracle generation Learning Deep Semantics for Test Completion 2023 [45]\n23 Test oracle generation; Program repairUsing Transfer Learning for Code-Related Tasks 2023 [46]\n24 Test oracle generation; Program repairRetrieval-Based Prompt Selection for Code-Related Few-Shot Learning 2023 [47]\n25 System test input generation Automated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing 2021 [48]\n26 System test input generation Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing 2022 [49]\n27 System test input generation Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors 2022 [50]\n28 System test input generation Slgpt: Using Transfer Learning to Directly Generate Simulink Model Files and Find Bugs in the Simulink Toolchain2021 [51]\n29 System test input generation Augmenting Greybox Fuzzing with Generative AI 2023 [52]\n30 System test input generation Automated Test Case Generation Using T5 and GPT-3 2023 [53]\n31 System test input generation Automating GUI-based Software Testing with GPT-3 2023 [54]\n32 System test input generation AXNav: Replaying Accessibility Tests from Natural Language 2023 [55]\n33 System test input generation Can ChatGPT Advance Software Testing Intelligence? An Experience Report on Metamorphic Testing 2023 [56]\n34 System test input generation Efficient Mutation Testing via Pre-Trained Language Models 2023 [57]\n35 System test input generation Large Language Models are Edge-Case Generators:Crafting Unusual Programs for Fuzzing Deep Learning Libraries2023 [58]\n36 System test input generation Large Language Models are Zero Shot Fuzzers: Fuzzing Deep Learning Libraries via Large Language Models2023 [59]\n37 System test input generation Large Language Models for Fuzzing Parsers (Registered Report) 2023 [60]\n38 System test input generation LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities 2023 [61]\n39 System test input generation Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions2023 [14]\n40 System test input generation PentestGPT: An LLM-empowered Automatic Penetration Testing Tool 2023 [62]\n41 System test input generation SMT Solver Validation Empowered by Large Pre-Trained Language Models 2023 [63]\n42 System test input generation TARGET: Automated Scenario Generation from Traffic Rules for Testing Autonomous Vehicles 2023 [64]\n43 System test input generation Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model2023 [65]\n44 System test input generation Understanding Large Language Model Based Fuzz Driver Generation 2023 [66]\n45 System test input generation Universal Fuzzing via Large Language Models 2023 [67]\n46 System test input generation Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software 2023 [68]\n47 System test input generation White-box Compiler Fuzzing Empowered by Large Language Models 2023 [69]\n48 Bug analysis Itiger: an Automatic Issue Title Generation Tool 2022 [70]\n49 Bug analysis CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace 2023 [71]\n50 Bug analysis Cupid: Leveraging ChatGPT for More Accurate Duplicate Bug Report Detection 2023 [72]\n51 Bug analysis Employing Deep Learning and Structured Information Retrieval to Answer Clarification Questions on Bug Reports2023 [73]\n52 Bug analysis Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation 2022 [74]\n53 Bug analysis Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models 2023 [75]\n54 Bug analysis Still Confusing for Bug-Component Triaging? Deep Feature Learning and Ensemble Setting to Rescue 2023 [76]\n55 Debug Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5 2022 [77]\n56 Debug Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction 2022 [78]\n57 Debug A Preliminary Evaluation of LLM-Based Fault Localization 2023 [79]\n58 Debug Addressing Compiler Errors: Stack Overflow or Large Language Models? 2023 [80]\n59 Debug Can LLMs Demystify Bug Reports? 2023 [81]\n60 Debug Dcc \u2013help: Generating Context-Aware Compiler Error Explanations with Large Language Models 2023 [82]\n61 Debug Explainable Automated Debugging via Large Language Model-driven Scientific Debugging 2023 [83]\n62 Debug Large Language Models for Test-Free Fault Localization 2023 [84]\n63 Debug Large Language Models in Fault Localisation 2023 [85]\n64 Debug LLM4CBI: Taming LLMs to Generate Effective Test Programs for Compiler Bug Isolation 2023 [86]\n65 Debug Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting 2023 [87]\n66 Debug Teaching Large Language Models to Self-Debug 2023 [88]\n67 Debug; Program repair A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair 2023 [89]\n68 Program repair Examining Zero-Shot Vulnerability Repair with Large Language Models 2022 [90]\n69 Program repair Automated Repair of Programs from Large Language Models 2022 [91]\n70 Program repair Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar 2022 [92]\n71 Program repair Practical Program Repair in the Era of Large Pre-trained Language Models 2022 [93]\n72 Program repair Repairing Bugs in Python Assignments Using Large Language Models 2022 [94]\n73 Program repair Towards JavaScript Program Repair with Generative Pre-trained Transformer (GPT-2) 2022 [95]\n74 Program repair An Analysis of the Automatic Bug Fixing Performance of ChatGPT 2023 [96]\n75 Program repair An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair 2023 [97]\n76 Program repair An Evaluation of the Effectiveness of OpenAI\u2019s ChatGPT for Automated Python Program Bug Fixing using QuixBugs2023 [98]\n77 Program repair An Extensive Study on Model Architecture and Program Representation in the Domain of Learning-based Automated Program Repair2023 [99]\n78 Program repair Can OpenAI\u2019s Codex Fix Bugs? An Evaluation on QuixBugs 2022 [100]\n79 Program repair CIRCLE: Continual Repair Across Programming Languages 2022 [101]\n80 Program repair Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback 2023 [102]\n81 Program repair Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair2023 [103]\n82 Program repair Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors 2023 [104]\n83 Program repair Enhancing Genetic Improvement Mutations Using Large Language Models 2023 [105]\n84 Program repair FixEval: Execution-based Evaluation of Program Fixes for Programming Problems 2023 [106]\n85 Program repair Fixing Hardware Security Bugs with Large Language Models 2023 [107]\n86 Program repair Fixing Rust Compilation Errors using LLMs 2023 [108]\n87 Program repair Framing Program Repair as Code Completion 2022 [109]\n88 Program repair Frustrated with Code Quality Issues? LLMs can Help! 2023 [110]\n89 Program repair GPT-3-Powered Type Error Debugging: Investigating the Use of Large Language Models for Code Repair 2023 [111]\n90 Program repair How Effective Are Neural Networks for Fixing Security Vulnerabilities 2023 [112]\n91 Program repair Impact of Code Language Models on Automated Program Repair 2023 [113]\n92 Program repair Inferfix: End-to-end Program Repair with LLMs 2023 [114]\n93 Program repair Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT 2023 [115]\n94 Program repair Neural Program Repair with Program Dependence Analysis and Effective Filter Mechanism 2023 [116]\n95 Program repair Out of Context: How important is Local Context in Neural Program Repair? 2023 [117]\n96 Program repair Pre-trained Model-based Automated Software Vulnerability Repair: How Far are We? 2023 [118]\n97 Program repair RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot 2023 [119]\n98 Program repair RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair 2023 [120]\n99 Program repair STEAM: Simulating the InTeractive BEhavior of ProgrAMmers for Automatic Bug Fixing 2023 [121]\n100 Program repair Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions 2023 [122]\n101 Program repair VulRepair: a T5-based Automated Software Vulnerability Repair 2022 [123]\n102 Program repair What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs? 2023 [124]",
            "6\nTABLE 2: Conference proceedings and journals considered\nfor manual search\nAcronym Venue\nSE Conference\nICSE International Conference on Software EngineeringESEC/FSE Joint European Software Engineering Conference and Symposium on theFoundations of Software EngineeringASE International Conference on Automated Software EngineeringISSTA International Symposium on Software Testing and AnalysisICST International Conference on Software Testing, Verification and ValidationESEM International Symposium on Empirical Software Engineering and Mea-surementMSR International Conference on Mining Software RepositoriesQRS International Conference on Software Quality, Reliability and SecurityICSME International Conference on Software Maintenance and EvolutionISSRE International Symposium on Software Reliability Engineering\nSE Journal\nTSE Transactions on Software EngineeringTOSEM Transactions on Software Engineering and MethodologyEMSE Empirical Software EngineeringASE Automated Software EngineeringJSS Journal of Systems and SoftwareJSEP Journal of Software: Evolution and ProcessSTVR Software Testing, Verification and ReliabilityIEEE SOFTW. IEEE SoftwareIET SOFTW. IET SoftwareIST Information and Software TechnologySQJ Software Quality Journal\nAI Venues\nICLR International Conference on Learning RepresentationsNeurIPS Conference on Neural Information Processing SystemsICML International Conference on Machine LearningAAAI AAAI Conference on Artificial IntelligenceEMNLP Conference on Empirical Methods in Natural Language ProcessingACL Annual Meeting of the Association for Computational LinguisticsIJCAI International Joint Conference on Artificial Intelligence\nshould be included based on the inclusion criteria and exclu-\nsion criteria, and any paper with different decisions will be\nhanded over to the third author to make the final decision.\n3.1.4 Quality Assessment\nIn addition, we establish quality assessment criteria to ex-\nclude low-quality studies as shown below. For each ques-\ntion, the study\u2019s quality is rated as \u201cyes\u201d, \u201cpartial\u201d or \u201cno\u201d\nwhich are assigned values of 1, 0.5, and 0, respectively. Pa-\npers with a score of less than eight will be excluded from\nour study.\n\u25cf Is there a clearly stated research goal related to software\ntesting?\n\u25cf Is there a defined and repeatable technique?\n\u25cf Is there any explicit contribution to software testing?\n\u25cf Is there an explicit description of which LLMs are uti-\nlized?\n\u25cf Is there an explicit explanation about how the LLMs are\nutilized?\n\u25cf Is there a clear methodology for validating the tech-\nnique?\n\u25cf Are the subject projects selected for validation suitable\nfor the research goals?\n\u25cf Are there control techniques or baselines to demon-\nstrate the effectiveness of the proposed technique?\n\u25cf Are the evaluation metrics relevant (e.g., evaluate the\neffectiveness of the proposed technique) to the research\nobjectives?\n\u25cf Do the results presented in the study align with the\nresearch objectives and are they presented in a clear\nand relevant manner?\n3.1.5 Snowballing\nAt the end of searching database repositories and confer-\nence proceedings and journals, and applying inclusion/ex-\nclusion criteria and quality assessment, we obtain the initial\nset of papers. Next, to mitigate the risk of omitting rele-\nvant literature from this survey, we also perform backward\n/uni00000015/uni00000013/uni00000015/uni00000013/uni00000015/uni00000013/uni00000015/uni00000014/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000013/uni00000015/uni00000016/uni00000003\n/uni00000033/uni00000058/uni00000045/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000003c/uni00000048/uni00000044/uni00000055\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000018/uni00000013\n/uni00000015/uni00000013/uni00000013/uni00000006/uni00000003/uni00000033/uni00000058/uni00000045/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056\n/uni00000014/uni00000015\n/uni00000014/uni0000001c\n/uni0000001b/uni00000015\nFig. 3: Trend in the number of papers with year\nsnowballing [131] by inspecting the references cited by the\ncollected papers so far. Note that, this procedure did not in-\nclude new studies, which might because the surveyed topic\nis quite new and the reference studies tend to published pre-\nviously, and we already include a relatively comprehensive\nautomatic and manual search.\n3.2 Collection Results\nAs shown in Figure 2, the collection process started\nwith a total of 14,623 papers retrieved from four\nacademic databases employing keyword searching.\nThen after automated filtering, manual search, applying\ninclusion/exclusion criteria, and quality assessment, we\nfinally collected a total of 102 papers involving software\ntesting with LLMs. Table 1 shows the details of the collected\npapers. Besides, we also use Table 5 (at the end of the\npaper) to provide a more comprehensive overview of these\npapers regarding the specific characteristics which will be\nillustrated in Section 4 and Section 5.\nNote that, there are two studies which are respectively\nthe extension of a previously published paper by the same\nauthors ( [46] and [132], [68] and [133]), and we only keep\nthe extended version to avoid duplicate.\n3.3 General Overview of Collected Paper\nAmong the papers, 47% papers are published in software\nengineering venues, among which 19 papers are from ICSE,\n5 papers are from FSE, 5 papers are from ASE, and 3 pa-\npers are from ISSTA. 2% papers are published in artificial\nintelligence venues such as EMNLP and ICLR, and 5% pa-\npers are published in program analysis or security venues\nlike PLDI and S&P . Besides, 46% of the papers have not\nyet been published via peer-reviewed venues, i.e., they are\ndisclosed on arXiv. This is understandable because this field\nis emerging and many works are just completed and in\nthe process of submission. Although these papers did not\nundergo peer review, we have a quality assessment process\nthat eliminates papers with low quality, which potentially\nensures the quality of this survey.\nFigure 3 demonstrates the trend of our collected papers\nper year. We can see that as the years go by, the number of\npapers in this field is growing almost exponentially. In 2020\nand 2021, there were only 1 and 2 papers, respectively. In\n2022, there were 19 papers, and in 2023, there have been 82",
            "7\nFig. 4: Distribution of testing tasks with LLMs (aligned with software testing life cycle [134]\u2013[136], the number in bracket\nindicates the number of collected studies per task, and one paper might involve multiple tasks)\npapers. It is conceivable that there will be even more papers\nin the future, which indicates the popularity and attention\nthat this field is receiving.\n4 A NALYSIS FROM SOFTWARE TESTING PER-\nSPECTIVE\nThis section presents our analysis from the viewpoint of\nsoftware testing and organizes the collected studies in terms\nof testing tasks. Figure 4 lists the distribution of each in-\nvolved testing task, aligned with the software testing life\ncycle. We first provide a general overview of the distribu-\ntion, followed by further analysis for each task. Note that,\nfor each following subsection, the cumulative total of sub-\ncategories may not always match the total number of papers\nsince a paper might belong to more than one subcategory.\nWe can see that LLMs have been effectively used in both\nthe mid to late stages of the software testing lifecycle. In\nthe test case preparation phase, LLMs have been utilized for\ntasks such as generating unit test cases, test oracle genera-\ntion, and system test input generation. These tasks are cru-\ncial in the mid-phase of software testing to help catch issues\nand prevent further development until issues are resolved.\nFurthermore, in later phases such as the test report/bug re-\nports and bug fix phase, LLMs have been employed for tasks\nsuch as bug analysis, debugging, and repair. These tasks are\ncritical towards the end of the testing phase when software\nbugs need to be resolved to prepare for the product\u2019s release.\n4.1 Unit Test Case Generation\nUnit test case generation involves writing unit test cases to\ncheck individual units/components of the software inde-\npendently and ensure that they work correctly. For a method\nunder test (i.e., often called the focal method), its corre-\nsponding unit test consists of a test prefix and a test oracle.\nIn particular, the test prefix is typically a series of method\ninvocation statements or assignment statements, which aims\nat driving the focal method to a testable state; and then the\ntest oracle serves as the specification to check whether the\ncurrent behavior of the focal method satisfies the expected\none, e.g., the test assertion.\nTo alleviate manual efforts in writing unit tests,\nresearchers have proposed various techniques to facilitate\nautomated unit test generation. Traditional unit test\ngeneration techniques leverage search-based [3], [4],\nconstraint-based [5] or random-based strategies [6] to\ngenerate a suite of unit tests with the main goal of\nmaximizing the coverage in the software under test.\nNevertheless, the coverage and the meaningfulness of the\ngenerated tests are still far from satisfactory.\nSince LLMs have demonstrated promising results in\ntasks such as code generation, and given that both code\ngeneration and unit test case generation involve generating\nsource code, recent research has extended the domain of\ncode generation to encompass unit test case generation.\nDespite initial success, there are nuances that set unit\ntest case generation apart from general code generation,\nsignaling the need for more tailored approaches.\nPre-training or fine-tuning LLMs for unit test case\ngeneration. Due to the limitations of LLMs in their earlier\nstages, a majority of the earlier published studies adopt\nthis pre-training or fine-tuning schema. Moreover, in some\nrecent studies, this schema continues to be employed to\nincrease the LLMs\u2019 familiarity with domain knowledge.\nAlagarsamy et al. [29] first pre-trained the LLM with the\nfocal method and asserted statements to enable the LLM to\nhave a stronger foundation knowledge of assertions, then\nfine-tuned the LLM for the test case generation task where\nthe objective is to learn the relationship between the focal\nmethod and the corresponding test case. Tufano et al. [26]\nutilized a similar schema by pre-training the LLM on a\nlarge unsupervised Java corpus, and supervised fine-tuning\na downstream translation task for generating unit tests.\nHashtroudi et al. [32] leveraged the existing developer-\nwritten tests for each project to generate a project-specific\ndataset for domain adaptation when fine-tuning the LLM,\nwhich can facilitate generating human-readable unit tests.\nRao et al. [35] trained a GPT-style language model by\nutilizing a pre-training signal that explicitly considers the\nmapping between code and test files. Steenhoek et al.\n[42] utilizes reinforcement learning to optimize models by\nproviding rewards based on static quality metrics that can\nbe automatically computed for the generated unit test cases.\nDesigning effective prompts for unit test case genera-\ntion. The advancement of LLMs has allowed them to excel\nat targeted tasks without pre-training or fine-tuning. There-\nfore most later studies typically focus on how to design\nthe prompt, to make the LLM better at understanding the\ncontext and nuances of this task. Xie et al. [36] generated\nunit test cases by parsing the project, extracting essential\ninformation, and creating an adaptive focal context that in-\ncludes a focal method and its dependencies within the pre-\ndefined maximum prompt token limit of the LLM, and in-\ncorporating these context into a prompt to query the LLM.",
            "8\nTABLE 3: Performance of unit test case generation\nDataset Correctness Coverage LLM Paper\n5 Java projects from Defects4J 16.21% 5%-13% (line coverage) BART [26]\n10 Jave projects 40% 89% (line coverage), 90% (branch coverage) ChatGPT [36]\nCodeSearchNet 41% N/A ChatGPT [7]\nHumanEval 78% 87% (line coverage), 92% (branch coverage) Codex [39]\nSF110 2% 2% (line coverage), 1% (branch coverage) Codex [39]\nNote that, [39] experiments with Codex, CodeGen, and ChatGPT, and the best performance was achieved by Codex.\nDakhel et al. [38] introduced MuTAP for improving the ef-\nfectiveness of test cases generated by LLMs in terms of re-\nvealing bugs by leveraging mutation testing. They augment\nprompts with surviving mutants, as those mutants highlight\nthe limitations of test cases in detecting bugs. Zhang et al.\n[40] generated security tests with vulnerable dependencies\nwith LLMs.\nYuan et al. [7] first performed an empirical study to eval-\nuate ChatGPT\u2019s capability of unit test generation with both\na quantitative analysis and a user study in terms of cor-\nrectness, sufficiency, readability, and usability. And results\nshow that the generated tests still suffer from correctness\nissues, including diverse compilation errors and execution\nfailures. They further propose an approach that leveraged\nthe ChatGPT itself to improve the quality of its generated\ntests with an initial test generator and an iterative test re-\nfiner. Specifically, the iterative test refiner iteratively fixed\nthe compilation errors in the tests generated by the initial\ntest generator, which follows a validate-and-fix paradigm to\nprompt the LLM based on the compilation error messages\nand additional code context. Guilherme et al. [31] and Li\net al. [41] respectively evaluated the quality of the gener-\nated unit tests by LLM using different metrics and different\nprompts.\nTest generation with additional documentation.\nVikram et al. [34] went a step further by investigating the\npotential of using LLMs to generate property-based tests\nwhen provided API documentation. They believe that the\ndocumentation of an API method can assist the LLM in\nproducing logic to generate random inputs for that method\nand deriving meaningful properties of the result to check.\nInstead of generating unit tests from the source code, Plein\net al. [33] generated the tests based on user-written bug\nreports.\nLLM and search-based method for unit test generation.\nThe aforementioned studies utilize LLMs for the whole unit\ntest case generation task, while Lemieux et al. [37] focus on\na different direction, i.e., first letting the traditional search-\nbased software testing techniques (e.g., Pynguin [137]) in\ngenerating unit test case until its coverage improvements\nstall, then asking the LLM to provide the example test cases\nfor under-covered functions. These examples can help the\noriginal test generation redirect its search to more useful\nareas of the search space.\nTang et al. [8] conducts a systematic comparison of test\nsuites generated by the LLM and the state-of-the-art search-\nbased software testing tool EvoSuite, by considering the cor-\nrectness, readability, code coverage, and bug detection ca-\npability. Similarly, Bhatia [43] experimentally investigates\nthe quality of unit tests generated by LLM compared to a\ncommonly-used test generator Pynguin.\nPerformance of unit test case generation. Since the\naforementioned studies of unit test case generation are\nbased on different datasets, one can hardly derive a fair\ncomparison and we present the details in Table 3 to let\nthe readers obtain a general view. We can see that in the\nSF110 benchmark, all three evaluated LLMs have quite low\nperformance, i.e., 2% coverage [39]. SF110 is an Evosuite\n(a search-based unit test case generation technique)\nbenchmark consisting of 111 open-source Java projects\nretrieved from SourceForge, containing 23,886 classes, over\n800,000 bytecode-level branches, and 6.6 million lines of\ncode. The authors did not present detailed reasons for the\nlow performance which can be further explored in the\nfuture.\n4.2 Test Oracle Generation\nA test oracle is a source of information about whether the\noutput of a software system (or program or function or\nmethod) is correct or not [138]. Most of the collected studies\nin this category target the test assertion generation, which is\ninside a unit test case. Nevertheless, we opted to treat these\nstudies as separate sections to facilitate a more thorough\nanalysis.\nTest assertion, which is to indicate the potential issues\nin the tested code, is an important aspect that can distin-\nguish the unit test cases from the regular code. This is why\nsome studies specifically focus on the generation of effec-\ntive test assertions. Actually, before using LLMs, researchers\nhave proposed RNN-based approaches that aim at learning\nfrom thousands of unit test methods to generate meaning-\nful assert statements [139], yet only 17% of the generated\nasserts can exactly match with the ground truth asserts. Sub-\nsequently, to improve the performance, several researchers\nutilized the LLMs for this task.\nMastropaolo et al. [46], [132] pre-trained a T5 model on\na dataset composed of natural language English text and\nsource code. Then, it fine-tuned such a model by reusing\ndatasets used in four previous works that used deep learn-\ning techniques (such as RNN as mentioned before) includ-\ning test assertion generation and program repair, etc. Results\nshowed that the extract match rate of the generated test\nassertion is 57%. Tufano et al. [44] proposed a similar ap-\nproach which separately pre-trained the LLM with English\ncorpus and code corpus, and then fine-tuned it on the asserts\ndataset (with test methods, focal methods, and asserts). This\nfurther improved the performance to 62% of the exact match\nrate. Besides the syntax-level data as previous studies, Nie et\nal. [45] fine-tuned the LLMs with six kinds of code semantics\ndata, including the execution result (e.g., types of the local\nvariables) and execution context (e.g., the last called method\nin the test method), which enabled LLMs to learn to under-\nstand the code execution information. The exact match rate",
            "9\nis 17% (note that this paper is based on a different dataset\nfrom all other studies mentioned under this topic).\nThe aforementioned studies utilized the pre-training and\nfine-tuning schema when using LLMs, and with the increas-\ningly powerful capabilities of LLMs, they can perform well\non specific tasks without these specialized pre-training or\nfine-tuning datasets. Subsequently, Nashid et al. [47] uti-\nlized prompt engineering for this task, and proposed a tech-\nnique for prompt creation that automatically retrieves code\ndemonstrations similar to the task, based on embedding\nor frequency analysis. They also present evaluations about\nthe few-shot learning with various numbers (e.g., zero-shot,\none-shot, or n-shot) and forms (e.g., random vs. systematic,\nor with vs. without natural language descriptions) of the\nprompts, to investigate its feasibility on test assertion gen-\neration. With only a few relevant code demonstrations, this\napproach can achieve an accuracy of 76% for exact matches\nin test assertion generation, which is the state-of-the-art per-\nformance for this task.\n4.3 System Test Input Generation\nThis category encompasses the studies related to creating\ntest input of system testing for enabling the automation of\ntest execution. We employ three subsections to present the\nanalysis from three different orthogonal viewpoints, and\neach of the collected studies may be analyzed in one or\nmore of these subsections.\nThe first subsection is input generation in terms of software\ntypes. The generation of system-level test inputs for software\ntesting varies for specific types of software being tested. For\nexample, for mobile applications, the test input generation\nrequires providing a diverse range of text inputs or oper-\nation combinations (e.g., click a button, long press a list)\n[14], [49], which is the key to testing the application\u2019s func-\ntionality and user interface; while for Deep Learning (DL)\nlibraries, the test input is a program which covers diversified\nDL APIs [58], [59]. This subsection will demonstrate how the\nLLMs are utilized to generate inputs for different types of\nsoftware.\nThe second subsection input generation in terms of testing\ntechniques. We have observed that certain approaches serve\nas specific types of testing techniques. For example, dozens\nof our collected studies specifically focus on using LLMs\nfor fuzz testing. Therefore, this subsection would provide\nan analysis of the collected studies in terms of testing tech-\nniques, showcasing how the LLMs are employed to enhance\ntraditional testing techniques.\nThe third subsection input generation in terms of input and\noutput. While most of the collected studies take the source\ncode or the software itself as the input and directly output\nthe software\u2019s test input, there are studies that utilize alter-\nnative forms of input and output. This subsection would\nprovide an analysis of such studies, highlighting different\napproaches and their input-output characteristics.\n4.3.1 Input Generation in Terms of Software Types\nFigure 5 demonstrates the types of software under test in\nour collected studies. It is evident that the most prominent\ncategory is mobile apps, with five studies utilizing LLMs\nfor testing, possibly due to their prevalence and importance\n/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018\n/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n/uni00000030/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000003/uni00000044/uni00000053/uni00000053\n/uni00000027/uni00000048/uni00000048/uni00000053/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000055/uni0000005c\n/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004c/uni0000004f/uni00000048/uni00000055\n/uni00000036/uni00000030/uni00000037/uni00000003/uni00000056/uni00000052/uni0000004f/uni00000059/uni00000048/uni00000055\n/uni00000024/uni00000058/uni00000057/uni00000052/uni00000051/uni00000052/uni00000050/uni00000052/uni00000058/uni00000056/uni00000003/uni00000047/uni00000055/uni0000004c/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050\n/uni00000026/uni0000005c/uni00000045/uni00000048/uni00000055/uni00000003/uni00000053/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000056/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050\n/uni0000002a/uni00000032/uni00000003/uni00000057/uni00000052/uni00000052/uni0000004f/uni00000046/uni0000004b/uni00000044/uni0000004c/uni00000051\n/uni0000002d/uni00000044/uni00000059/uni00000044/uni00000036/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057/uni00000003/uni00000048/uni00000051/uni0000004a/uni0000004c/uni00000051/uni00000048\n/uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni00000058/uni00000050/uni00000003/uni00000046/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000053/uni0000004f/uni00000044/uni00000057/uni00000049/uni00000052/uni00000055/uni00000050\n/uni00000039/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000003/uni0000004a/uni00000044/uni00000050/uni00000048\n/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000003/uni00000037/uni00000048/uni00000056/uni00000057\n/uni00000018\n/uni00000015\n/uni00000015\n/uni00000015\n/uni00000014\n/uni00000014\n/uni00000014\n/uni00000014\n/uni00000014\n/uni00000014\nFig. 5: Distribution of software under test\nin today\u2019s business and daily life. Additionally, there are\nrespectively two studies focusing on testing deep learning\nlibraries, compilers, and SMT solvers. Moreover, LLM-based\ntesting techniques have also been applied to domains such\nas cyber-physical systems, quantum computing platforms,\nand more. This widespread adoption of LLMs demonstrates\ntheir effectiveness in handling diverse test inputs and en-\nhancing testing activities across various software domains.\nA detailed analysis is provided below.\nTest input generation for mobile apps. For mobile app\ntesting, one difficulty is to generate the appropriate text in-\nputs to proceed to the next page, which remains a prominent\nobstacle for testing coverage. Considering the diversity and\nsemantic requirement of valid inputs (e.g., flight departure,\nmovie name), traditional techniques with heuristic-based or\nconstraint-based techniques [10], [140] are far from generat-\ning meaningful text input. Liu et al. [49] employ the LLM\nto intelligently generate the semantic input text according\nto the GUI context. In detail, their proposed QTypist auto-\nmatically extracts the component information related to the\nEditText for generating the prompts, and then inputs the\nprompts into the LLM to generate the input text.\nBesides the text input, there are other forms of input\nfor mobile apps, i.e., operations like \u2018click a button\u2019 and\n\u2018select a list\u2019. To fully test an app, it is required to cover\nmore GUI pages and conduct more meaningful exploration\ntraces through the GUI operations, yet existing studies with\nrandom-/rule-based methods [9], [10], model-based meth-\nods [11], [12], and learning-based methods [13] are unable\nto understand the semantic information of the GUI page\nthus could not conduct the trace planning effectively. Liu et\nal. [14] formulates the test input generation of mobile GUI\ntesting problem as a Q&A task, which asks LLM to chat\nwith the mobile apps by passing the GUI page information\nto LLM to elicit testing scripts (i.e., GUI operation), and\nexecuting them to keep passing the app feedback to LLM, it-\nerating the whole process. The proposed GPTDroid extracts\nthe static context of the GUI page and the dynamic context\nof the iterative testing process, and designs prompts for in-\nputting this information to LLM which enables the LLM to\nbetter understand the GUI page as well as the whole testing\nprocess. It also introduces a functionality-aware memory\nprompting mechanism that equips the LLM with the abil-\nity to retain testing knowledge of the whole process and\nconduct long-term, functionality-based reasoning to guide\nexploration. Similarly, Zimmermann et al. utilize the LLM to",
            "10\ninterpret natural language test cases and programmatically\nnavigate through the application under test [54].\nYu et al. [61] investigate the LLM\u2019s capabilities in the\nmobile app test script generation and migration task, in-\ncluding the scenario-based test generation, and the cross-\nplatform/app test migration.\nTest input generation for DL libraries. The input for\ntesting DL libraries is DL programs, and the difficulty\nin generating the diversified input DL programs is that\nthey need to satisfy both the input language (e.g., Python)\nsyntax/semantics and the API input/shape constraints for\ntensor computations. Traditional techniques with API-level\nfuzzing [141], [142] or model-level fuzzing [143], [144]\nsuffer from the following limitations: 1) lack of diverse API\nsequence thus cannot reveal bugs caused by chained API\nsequences; 2) cannot generate arbitrary code thus cannot\nexplore the huge search space that exists when using the DL\nlibraries. Since LLMs can include numerous code snippets\ninvoking DL library APIs in their training corpora, they\ncan implicitly learn both language syntax/semantics and\nintricate API constraints for valid DL program generation.\nTaken in this sense, Deng et al. [59] used both generative\nand infilling LLMs to generate and mutate valid/diverse\ninput DL programs for fuzzing DL libraries. In detail, it first\nuses a generative LLM (CodeX) to generate a set of seed\nprograms (i.e., code snippets that use the target DL APIs).\nThen it replaces part of the seed program with masked\ntokens using different mutation operators and leverages the\nability of infilling LLM (InCoder) to perform code infilling\nto generate new code that replaces the masked tokens. Their\nfollow-up study [58] goes a step further to prime LLMs to\nsynthesize unusual programs for the fuzzing DL libraries.\nIt is built on the well-known hypothesis that historical\nbug-triggering programs may include rare/valuable code\ningredients important for bug finding and show improved\nbug detection performance.\nTest input generation for other types of software.There\nare also dozens of studies that address testing tasks in vari-\nous other domains, due to space limitations, we will present\na selection of representative studies in these domains.\nFinding bugs in a commercial cyber-physical system\n(CPS) development tool such as Simulink is even more\nchallenging. Given the complexity of the Simulink language,\ngenerating valid Simulink model files for testing is an\nambitious task for traditional machine learning or deep\nlearning techniques. Shrestha et al. [51] employs a small set\nof Simulink-specific training data to fine-tune the LLM for\ngenerating Simulink models. Results show that it can create\nSimulink models quite similar to the open-source models,\nand can find a super-set of the bugs traditional fuzzing\napproaches found.\nSun et al. [63] utilize LLM to generate test formulas for\nfuzzing SMT solvers. It retrains the LLMs on a large corpus\nof SMT formulas to enable them to acquire SMT-specific\ndomain knowledge. Then it further fine-tunes the LLMs\non historical bug-triggering formulas, which are known\nto involve structures that are more likely to trigger bugs\nand solver-specific behaviors. The LLM-based compiler\nfuzzer proposed by Yang et al. [69] adopts a dual-model\nframework: (1) an analysis LLM examines the low-level\noptimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization;\n(2) a generation LLM produces test programs based on the\nsummarized requirements. Ye et al. [48] utilize the LLM\nfor generating the JavaScript programs and then use the\nwell-structured ECMAScript specifications to automatically\ngenerate test data along with the test programs, after that\nthey apply differential testing to expose bugs.\n4.3.2 Input Generation in Terms of Testing Techniques\nBy utilizing system test inputs generated by LLMs, the col-\nlected studies aim to enhance traditional testing techniques\nand make them more effective. Among these techniques,\nfuzz testing is the most commonly involved one. Fuzz test-\ning, as a general concept, revolves around generating in-\nvalid, unexpected, or random data as inputs to evaluate the\nbehavior of software. LLMs play a crucial role in improv-\ning traditional fuzz testing by facilitating the generation of\ndiverse and realistic input data. This enables fuzz testing to\nuncover potential bugs in the software by subjecting it to a\nwide range of input scenarios. In addition to fuzz testing,\nLLMs also contribute to enhancing other testing techniques,\nwhich will be discussed in detail later.\nUniversal fuzzing framework. Xia et al. [67] present\nFuzz4All that can target many different input languages\nand many different features of these languages. The key\nidea behind it is to leverage LLMs as an input generation\nand mutation engine, which enables the approach to\nproduce diverse and realistic inputs for any practically\nrelevant language. To realize this potential, they present\na novel auto-prompting technique, which creates LLM\nprompts that are well-suited for fuzzing, and a novel\nLLM-powered fuzzing loop, which iteratively updates the\nprompt to create new fuzzing inputs. They experiment\nwith six different languages (C, C++, Go, SMT2, Java and\nPython) as inputs and demonstrate higher coverage than\nexisting language-specific fuzzers. Hu et al. [52] propose a\ngreybox fuzzer augmented by the LLM, which picks a seed\nin the fuzzer\u2019s seed pool and prompts the LLM to produce\nthe mutated seeds that might trigger a new code region\nof the software. They experiment with three categories of\ninput formats, i.e., formatted data files (e.g., json, xml),\nsource code in different programming languages (e.g., JS,\nSQL, C), text with no explicit syntax rules (e.g., HTTP\nresponse, md5 checksum). In addition, effective fuzzing\nrelies on the effective fuzz driver, and Zhang et al. [66]\nutilize LLMs on the fuzz driver generation, in which five\nquery strategies are designed and analyzed from basic to\nenhanced.\nFuzzing techniques for specific software. There are\nstudies that focus on the fuzzing techniques tailored to\nspecific software, e.g., the deep learning library [58], [59],\ncompiler [69], SMT solvers [63], input widget of mobile app\n[65], cyber-physical system [51], etc. One key focus of these\nfuzzing techniques is to generate diverse test inputs so as\nto achieve higher coverage. This is commonly achieved\nby combining the mutation technique with LLM-based\ngeneration, where the former produces various candidates\nwhile the latter is responsible for generating the executable\ntest inputs [59], [63]. Another focus of these fuzzing\ntechniques is to generate the risky test inputs that can\ntrigger bugs earlier. To achieve this, a common practice is to",
            "11\ncollect the historical bug-triggering programs to fine-tune\nthe LLM [63] or treat them as the demonstrations when\nquerying the LLM [58], [65].\nOther testing techniques. There are studies that utilize\nLLMs for enhancing GUI testing for generating meaningful\ntext input [49] and functionality-oriented exploration traces\n[14], which has been introduced in Test input generation for\nmobile apps part of Section 4.3.1.\nBesides, Deng et al. [62] leverage the LLMs to carry out\npenetration testing tasks automatically. It involves setting a\npenetration testing goal for the LLM, soliciting it for the\nappropriate operation to execute, implementing it in the\ntesting environment, and feeding the test outputs back to\nthe LLM for next-step reasoning.\n4.3.3 Input Generation in Terms of Input and Output\nOther output format of test generation. Although most\nworks use LLM to generate test cases directly, there are also\nsome works generating indirect inputs like testing code, test\nscenarios, metamorphic relations, etc. Liu et al. [65] pro-\npose InputBlaster which leverages the LLM to automati-\ncally generate unusual text inputs for fuzzing the text input\nwidgets in mobile apps. It formulates the unusual inputs\ngeneration problem as a task of producing a set of test gen-\nerators, each of which can yield a batch of unusual text\ninputs under the same mutation rule. In detail, InputBlaster\nleverages LLM to produce the test generators together with\nthe mutation rules serving as the reasoning chain and uti-\nlizes the in-context learning schema to demonstrate the LLM\nwith examples for boosting the performance. Deng et al.\n[64] use LLM to extract key information related to the test\nscenario from a traffic rule, and represent the extracted in-\nformation in a test scenario schema, then synthesize the\ncorresponding scenario scripts to construct the test scenario.\nLuu et al. [56] examine the effectiveness of LLM in generat-\ning metamorphic relations (MRs) for metamorphic testing.\nTheir results show that ChatGPT can be used to advance\nsoftware testing intelligence by proposing MRs candidates\nthat can be later adapted for implementing tests, but human\nintelligence should still inevitably be involved to justify and\nrectify their correctness.\nOther input format of test generation. The aforemen-\ntioned studies primarily take the source code or the software\nas the input of LLM, yet there are also studies that take\nnatural language description as the input for test generation.\nMathur et al. [53] propose to generate test cases from the\nnatural language described requirements. Ackerman et al.\n[60] generate the instances from natural language described\nrequirements recursively to serve as the seed examples for a\nmutation fuzzer.\n4.4 Bug Analysis\nThis category involves analyzing and categorizing the iden-\ntified software bugs to enhance understanding of the bug,\nand facilitate subsequent debug and bug repair. Mukher-\njee et al. [73] generate relevant answers to follow-up ques-\ntions for deficient bug reports to facilitate bug triage. Su et\nal. [76] transform the bug-component triaging into a multi-\nclassification task and a generation task with LLM, then\nensemble the prediction results from them to improve the\nperformance of bug-component triaging further. Zhang et\nal. [72] first leverage the LLM under the zero-shot setting\nto get essential information on bug reports, then use the\nessential information as the input to detect duplicate bug re-\nports. Mahbub et al. [74] proposes to explain software bugs\nwith LLM, which generates natural language explanations\nfor software bugs by learning from a large corpus of bug-fix\ncommits. Zhang et al. [70] target to automatically generate\nthe bug title from the descriptions of the bug, which aims\nto help developers write issue titles and facilitate the bug\ntriaging and follow-up fixing process.\n4.5 Debug\nThis category refers to the process of identifying and locat-\ning the cause of a software problem (i.e., bug). It involves\nanalyzing the code, tracing the execution flow, collecting\nerror information to understand the root cause of the issue,\nand fixing the issue. Some studies concentrate on the com-\nprehensive debug process, while others delve into specific\nsub-activities within the process.\nOverall debug framework. Bui et al. [77] proposes a uni-\nfied Detect-Localize-Repair framework based on the LLM\nfor debugging, which first determines whether a given code\nsnippet is buggy or not, then identifies the buggy lines, and\ntranslates the buggy code to its fixed version. Kang et al.\n[83] proposes automated scientific debugging, a technique\nthat given buggy code and a bug-revealing test, prompts\nLLMs to automatically generate hypotheses, uses debuggers\nto actively interact with buggy code, and thus automati-\ncally reaches conclusions prior to patch generation. Chen\net al. [88] demonstrate that self-debugging can teach the\nLLM to perform rubber duck debugging; i.e., without any\nhuman feedback on the code correctness or error messages,\nthe model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in nat-\nural language. Cao et al. [89] conducts a study of LLM\u2019s de-\nbugging ability for deep learning programs, including fault\ndetection, fault localization and program repair.\nBug localization. Wu et al. [85] compare the two LLMs\n(ChatGPT and GPT-4) with the existing fault localization\ntechniques, and investigate the consistency of LLMs in fault\nlocalization, as well as how prompt engineering and the\nlength of code context affect the results. Kang et al. [79]\npropose AutoFL, an automated fault localization technique\nthat only requires a single failing test, and during its fault\nlocalization process, it also generates an explanation about\nwhy the given test fails. Yang et al. [84] propose LLMAO to\novercome the left-to-right nature of LLMs by fine-tuning a\nsmall set of bidirectional adapter layers on top of the rep-\nresentations learned by LLMs, which can locate buggy lines\nof code without any test coverage information. Tu et al. [86]\npropose LLM4CBI to tame LLMs to generate effective test\nprograms for finding suspicious files.\nBug reproduction. There are also studies focusing on a\nsub-phase of the debugging process. For example, Kang et\nal. [78] and Plein et al. [81] respectively propose the frame-\nwork to harness the LLM to reproduce bugs, and suggest\nbug reproducing test cases to the developer for facilitating\ndebugging. Li et al. [87] focus on a similar aspect of finding\nthe failure-inducing test cases whose test input can trigger",
            "12\nthe software\u2019s fault. It synergistically combines LLM and\ndifferential testing to do that.\nThere are also studies focusing on the bug reproduc-\ntion of mobile apps to produce the replay script. Feng et\nal. [75] propose AdbGPT, a new lightweight approach to\nautomatically reproduce the bugs from bug reports through\nprompt engineering, without any training and hard-coding\neffort. It leverages few-shot learning and chain-of-thought\nreasoning to elicit human knowledge and logical reasoning\nfrom LLMs to accomplish the bug replay in a manner similar\nto a developer. Huang et al. [71] propose CrashTranslator to\nautomatically reproduce bugs directly from the stack trace.\nIt accomplishes this by leveraging the LLM to predict the\nexploration steps for triggering the crash, and designing a\nreinforcement learning based technique to mitigate the in-\naccurate prediction and guide the search holistically. Taeb et\nal. [55] convert the manual accessibility test instructions into\nreplayable, navigable videos by using LLM and UI element\ndetection models, which can also help reveal accessibility\nissues.\nError explanation. Taylor et al. [82] integrates the LLM\ninto the Debugging C Compiler to generate unique, novice-\nfocused explanations tailored to each error. Widjojo et al.\n[80] study the effectiveness of Stack Overflow and LLMs at\nexplaining compiler errors.\n4.6 Program Repair\nThis category denotes the task of fixing the identified\nsoftware bugs. The high frequency of repair-related studies\ncan be attributed to the close relationship between this\ntask and the source code. With their advanced natural\nlanguage processing and understanding capabilities, LLM\nare well-equipped to process and analyze source code,\nmaking them an ideal tool for performing code-related\ntasks such as fixing bugs.\nThere have been template-based [145], heuristic-based\n[146], and constraint-based [147], [148] automatic program\nrepair techniques. And with the development of deep\nlearning techniques in the past few years, there have been\nseveral studies employing deep learning techniques for\nprogram repair. They typically adopt deep learning models\nto take a buggy software program as input and generate a\npatched program. Based on the training data, they would\nbuild a neural network model that learns the relations\nbetween the buggy code and the corresponding fixed code.\nNevertheless, these techniques still fail to fix a large portion\nof bugs, and they typically have to generate hundreds to\nthousands of candidate patches and take hours to validate\nthese patches to fix enough bugs. Furthermore, the deep\nlearning based program repair models need to be trained\nwith huge amounts of labeled training data (typically\npairs of buggy and fixed code), which is time- and effort-\nconsuming to collect the high-quality dataset. Subsequently,\nwith the popularity and demonstrated capability of the\nLLMs, researchers begin to explore the LLMs for program\nrepair.\nPatch single-line bugs. In the early era of program re-\npair, the focus was mainly on addressing defects related to\nsingle-line code errors, which are relatively simple and did\nnot require the repair of complex program logic. Lajk \u00b4o et\nal. [95] propose to fine-tune the LLM with JavaScript code\nsnippets to serve as the purpose for the JavaScript program\nrepair. Zhang et al. [116] employs program slicing to extract\ncontextual information directly related to the given buggy\nstatement as repair ingredients from the corresponding pro-\ngram dependence graph, which makes the fine-tuning more\nfocused on the buggy code. Zhang et al. [121] propose a\nstage-wise framework STEAM for patching single-line bugs,\nwhich simulates the interactive behavior of multiple pro-\ngrammers involved in bug management, e.g., bug reporting,\nbug diagnosis, patch generation, and patch verification.\nSince most real-world bugs would involve multiple lines\nof code, and later studies explore these more complex situa-\ntions (although some of them can also patch the single-line\nbugs).\nPatch multiple-lines bugs. The studies in this category\nwould input a buggy function to the LLM, and the goal is to\noutput the patched function, which might involve complex\nsemantic understanding, code hunk modification, as well\nas program refactoring. Earlier studies typically employ the\nfine-tuning strategy to enable the LLM to better understand\nthe code semantics. Fu et al. [123] fine-tune the LLM by\nemploying BPE tokenization to handle Out-Of-Vocabulary\n(OOV) issues which makes the approach generate new to-\nkens that never appear in a training function but are newly\nintroduced in the repair. Wang et. al. [120] train the LLM\nbased on both buggy input and retrieved bug-fix examples\nwhich are retrieved in terms of the lexical and semantical\nsimilarities. The aforementioned studies (including the ones\nin patching single-line bugs) would predict the fixed pro-\ngrams directly, and Hu et al. [92] utilize a different setup\nthat predicts the scripts that can fix the bugs when executed\nwith the delete and insert grammar. For example, it predicts\nwhether an original line of code should be deleted, and what\ncontent should be inserted.\nNevertheless, fine-tuning may face limitations in terms\nof its reliance on abundant high-quality labeled data,\nsignificant computational resources, and the possibility of\noverfitting. To approach the program repair problem more\neffectively, later studies focus on how to design an effective\nprompt for program repair. Several studies empirically\ninvestigate the effectiveness of prompt variants of the latest\nLLMs for program repair under different repair settings\nand commonly-used benchmarks (which will be explored\nin depth later), while other studies focus on proposing\nnew techniques. Ribeiro et al. [109] take advantage of\nLLM to conduct the code completion in a buggy line for\npatch generation, and elaborate on how to circumvent the\nopen-ended nature of code generation to appropriately\nfit the new code in the original program. Xia et al. [115]\npropose the conversation-driven program repair approach\nthat interleaves patch generation with instant feedback\nto perform the repair in a conversational style. They first\nfeed the LLM with relevant test failure information to start\nwith, and then learns from both failures and successes\nof earlier patching attempts of the same bug for more\npowerful repair. For earlier patches that failed to pass\nall tests, they combine the incorrect patches with their\ncorresponding relevant test failure information to construct\na new prompt for the LLM to generate the next patch,\nin order to avoid making the same mistakes. For earlier",
            "13\nTABLE 4: Performance of program repair\nDataset % Correct patches LLM Paper\nDefects4J v1.2, Defects4J\nv2.0, QuixBugs,\nHumanEval-Java\n22/40 Jave bugs (QuixBugs dataset, with InCoder-6B, correct\ncode infilling setting)\nPLBART, CodeT5, CodeGen, In-\nCoder (each with variant pa-\nrameters, 10 LLMs in total)\n[113]\nQuixBugs 23/40 Python bugs, 14/40 Java bugs (complete function genera-\ntion setting)\nCodex-12B [100]\nDefects4J v1.2, Defects4J\nv2.0, QuixBugs, Many-\nBugs\n39/40 Python bugs, 34/40 Java bugs (QuixBugs dataset, with\nCodex-12B, correct code infilling setting); 37/40 Python bugs,\n32/40 Java bugs (QuixBugs dataset, with Codex-12B, complete\nfunction generation setting)\nCodex, GPT-Neo, CodeT5, In-\nCoder (each with variant pa-\nrameters, 9 LLMs in total)\n[93]\nQuixBugs 31/40 Python bugs (completion function generation setting) ChatGPT-175B [96]\nDL programs from Stack-\nOverflow\n16/72 Python bugs (complete function generation setting) ChatGPT-175B [89]\nNote that, for studies with multiple datasets or LLMs, we only present the best performance or in the most commonly utilized dataset.\npatches that passed all the tests (i.e., plausible patches),\nthey further ask the LLM to generate alternative variations\nof the original plausible patches. This can further build on\nand learn from earlier successes to generate more plausible\npatches to increase the chance of having correct patches.\nZhang et al. [94] propose a similar approach design by\nleveraging multimodal prompts (e.g., natural language\ndescription, error message, input-output-based test cases),\niterative querying, test-case-based few-shot selection to\nproduce repairs. Moon et al. [102] propose for bug fixing\nwith feedback. It consists of a critic model to generate\nfeedback, an editor to edit codes based on the feedback,\nand a feedback selector to choose the best possible feedback\nfrom the critic.\nWei et. al. [103] propose Repilot to copilot the AI \u201ccopi-\nlots\u201d (i.e., LLMs) by synthesizing more valid patches during\nthe repair process. Its key insight is that many LLMs pro-\nduce outputs autoregressively (i.e., token by token), and by\nresembling human writing programs, the repair can be sig-\nnificantly boosted and guided through a completion engine.\nBrownlee et al. [105] propose to use the LLM as mutation\noperators for the search-based techniques of program repair.\nRepair with static code analyzer. Most of the program\nrepair studies would suppose the bug has been detected,\nwhile Jin et al. [114] propose a program repair framework\npaired with a static analyzer to first detect the bugs, and\nthen fix them. In detail, the static analyzer first detects an\nerror (e.g., null pointer dereference) and the context infor-\nmation provided by the static analyzer will be sent into the\nLLM for querying the patch for this specific error. Wadhwa\net al. [110] focus on a similar task, and additionally employ\nan LLM as the ranker to assess the likelihood of acceptance\nof generated patches which can effectively catch plausible\nbut incorrect fixes and reduce developer burden.\nRepair for specific bugs. The aforementioned studies\nall consider the buggy code as the input for the automatic\nprogram repair, while other studies conduct program re-\npairing in terms of other types of bug descriptions, specific\ntypes of bugs, etc. Fakhoury et al. [122] focus on program\nrepair from natural language issue descriptions, i.e., gen-\nerating the patch with the bug and fix-related information\ndescribed in the issue reports. Garg et al. [119] aim at re-\npairing performance issues, in which they first retrieve a\nprompt instruction from a pre-constructed knowledge-base\nof previous performance bug fixes and then generate a re-\npair prompt using the retrieved instruction. There are stud-\nies focusing on the bug fixing of Rust programs [108] or\nOCaml programs (an industrial-strength programming lan-\nguage) [111].\nEmpirical study about program repair.There are several\nstudies related to the empirical or experimental evaluation\nof the various LLMs on program repair, and we summa-\nrize the performance in Table 4. Jiang et al. [113], Xia et al.\n[93], and Zhang et. al. [118] respectively conduct compre-\nhensive experimental evaluations with various LLMs and\non different automated program repair benchmarks, while\nother researchers [89], [96], [98], [100] focus on a specific\nLLM and on one dataset, e.g., QuixBugs. In addition, Gao\net al. [124] empirically investigate the impact of in-context\ndemonstrations for bug fixing, including the selection, or-\nder, and number of demonstration examples. Prenner et al.\n[117] empirically study how the local context (i.e., code that\ncomes before or after the bug location) affects the repair per-\nformance. Horv \u00b4ath et al. [99] empirically study the impact\nof program representation and model architecture on the\nrepair performance.\nThere are two commonly-used repair settings when us-\ning LLMs to generate patches: 1) complete function gen-\neration (i.e., generating the entire patch function), 2) cor-\nrect code infilling (i.e., filling in a chunk of code given the\nprefix and suffix), and different studies might utilize differ-\nent settings which are marked in Table 4. The commonly-\nused datasets are QuixBugs, Defects4J, etc. These datasets\nonly involve the fundamental functionalities such as sorting\nalgorithms, each program\u2019s average number of lines rang-\ning from 13 to 22, implementing one functionality, and in-\nvolving few dependencies. To tackle this, Cao et al. [89]\nconducts an empirical study on a more complex dataset\nwith DL programs collected from StackOverflow. Every pro-\ngram contains about 46 lines of code on average, imple-\nmenting several functionalities including data preprocess-\ning, DL model construction, model training, and evaluation.\nAnd the dataset involves more than 6 dependencies for each\nprogram, including TensorFlow, Keras, and Pytorch. Their\nresults demonstrate a much lower rate of correct patches\nthan in other datasets, which again reveals the potential\ndifficulty of this task. Similarly, Haque et al. [106] introduce\na dataset comprising of buggy code submissions and their\ncorresponding fixes collected from online judge platforms,\nin which it offers an extensive collection of unit tests to\nenable the evaluations about the correctness of fixes and fur-\nther information regarding time, memory constraints, and\nacceptance based on a verdict.",
            "14\nChatGPT, 36\n25%\nCodex, 23\n16%\nCodeT5, 18\n13% GPT-4, 14\n10%\nGPT-3, 7\n5%\nCodeGen, 64%\nInCoder, 54%\nPLBART, 54%\nT5, 5\n4%\nCodeGPT, 4\n3%\nGPT-2, 4\n3%\nBART, 3\n2%\nStarCoder, 3\n2%\nUniXCoder, 2\n1%\nOthers, 7\n5%\nFig. 6: LLMs used in the collected papers\n5 A NALYSIS FROM LLM PERSPECTIVE\nThis section discusses the analysis based on the viewpoints\nof LLM, specifically, it\u2019s unfolded from the viewpoints of\nutilized LLMs, types of prompt engineering, input of the\nLLMs, as well as the accompanied techniques when utilizing\nLLM.\n5.1 LLM Models\nAs shown in Figure 6, the most commonly utilized LLM\nin software testing tasks is ChatGPT, which was released\non Nov. 2022 by OpenAI. It is trained on a large corpus\nof natural language text data, and primarily designed for\nnatural language processing and conversation. ChatGPT is\nthe most widely recognized and popular LLM up until now,\nknown for its exceptional performance across various tasks.\nTherefore, it comes as no surprise that it ranks in the top\nposition in terms of our collected studies.\nCodex, an LLM based on GPT-3, is the second most com-\nmonly used LLM in our collected studies. It is trained on a\nmassive code corpus containing examples from many pro-\ngramming languages such as JavaScript, Python, C/C++,\nand Java. Codex was released on Sep. 2021 by OpenAI and\npowers GitHub Copilot\u2013 an AI pair programmer that gener-\nates whole code snippets, given a natural language descrip-\ntion as a prompt. Since a large portion of our collected stud-\nies involve the source code (e.g., repair, unit test case gen-\neration), it is not surprising that researchers choose Codex\nas the LLM in assisting them in accomplishing the coding-\nrelated tasks.\nThe third-ranked LLM is CodeT5, which is an open-\nsourced LLM developed by salesforce 3. Thanks to its open\nsource, researchers can easily conduct the pre-training and\nfine-tuning with domain-specific data to achieve better\nperformance. Similarly, CodeGen is also open-sourced and\nranked relatively higher. Besides, for CodeT5 and CodeGen,\nthere are more than half of the related studies involve the\nempirical evaluations (which employ multiple LLMs), e.g.,\nprogram repair [112], [113], unit test case generation [39].\n3. https://blog.salesforceairesearch.com/codet5/\nThere are already 14 studies that utilize GPT-4, ranking\nat the fourth place, which is launched on March 2023. Sev-\neral studies directly utilize this state-of-the-art LLM of Ope-\nnAI, since it demonstrates excellent performance across a\nwide range of generation and reasoning tasks. For example,\nXie et al. utilize GPT-4 to generate fuzzing inputs [67], while\nVikram et al. employ it to generate property-based tests with\nthe assistance of API documentation [34]. In addition, some\nstudies conduct experiments using both GPT-4 and Chat-\nGPT or other LLMs to provide a more comprehensive evalu-\nation of these models\u2019 performance. In their proposed LLM-\nempowered automatic penetration testing technique, Deng\net al. find that GPT-4 surpasses ChatGPT and LaMDA from\nGoogle [62]. Similarly, Zhang et al. find that GPT-4 shows\nits performance superiority over ChatGPT when generat-\ning the fuzz drivers with both the basic query strategies\nand enhanced query strategies [66]. Furthermore, GPT-4, as\na multi-modal LLM, sets itself apart from the other men-\ntioned LLMs by showcasing additional capabilities such as\ngenerating image narratives and answering questions based\non images [149]. Yet we have not come across any studies\nthat explore the utilization of GPT-4\u2019s image-related features\n(e.g., UI screenshots, programming screencasts) in software\ntesting tasks.\n5.2 Types of Prompt Engineering\nAs shown in Figure 7, among our collected studies, 38\nstudies utilize the LLMs through pre-training or fine-\ntuning schema, while 64 studies employ the prompt\nengineering to communicate with LLMs to steer its\nbehavior for desired outcomes without updating the model\nweights. When using the early LLMs, their performances\nmight not be as impressive, so researchers often use\npre-training or fine-tuning techniques to adjust the models\nfor specific domains and tasks in order to improve their\nperformance. Then with the upgrading of LLM technology,\nespecially with the introduction of GPT-3 and later\nLLMs, the knowledge contained within the models and\ntheir understanding/inference capability has increased\nsignificantly. Therefore, researchers will typically rely on\nprompt engineering to consider how to design appropriate\nprompts to stimulate the model\u2019s knowledge.\nAmong the 64 studies with prompt engineering, 51 stud-\nies involve zero-shot learning, and 25 studies involve few-\nshot learning (a study may involve multiple types). There\nare also studies involving the chain-of-though (7 studies),\nself-consistency (1 study), and automatic prompt (1 study).\nZero-shot learning is to simply feed the task text to the\nmodel and ask for results. Many of the collected studies em-\nploy the Codex, CodeT5, and CodeGen (as shown in Section\n5.1), which is already trained on source code. Hence, for the\ntasks dealing with source code like unit test case generation\nand program repair as demonstrated in previous sections,\ndirectly querying the LLM with prompts is the common\npractice. There are generally two types of manners of zero-\nshot learning, i.e., with and without instructions. For exam-\nple, Xie et al. [36] would provide the LLMs with the instruc-\ntions as \u201cplease help me generate a JUnit test for a specific\nJava method ...\u201d to facilitate the unit test case generation.\nIn contrast, Siddiq et al. [39] only provide the code header",
            "15\nFig. 7: Distribution about how LLM is used (Note that, a study can involve multiple types of prompt engineering)\nof the unit test case (e.g., \u201cclass $ {className}${suffix}Test\n{\u201d), and the LLMs would carry out the unit test case gener-\nation automatically. Generally speaking, prompts with clear\ninstructions will yield more accurate results, while prompts\nwithout instructions are typically suitable for very specific\nsituations.\nFew-shot learning presents a set of high-quality demon-\nstrations, each consisting of both input and desired output,\non the target task. As the model first sees the examples,\nit can better understand human intention and criteria for\nwhat kinds of answers are wanted, which is especially im-\nportant for tasks that are not so straightforward or intuitive\nto the LLM. For example, when conducting the automatic\ntest generation from general bug reports, Kang et al. [78]\nprovide examples of bug reports (questions) and the corre-\nsponding bug reproducing tests (answers) to the LLM, and\ntheir results show that two examples can achieve the highest\nperformance than no examples or other number of exam-\nples. Another example of test assertion generation, Nashid\net al. [47] provide demonstrations of the focal method, the\ntest method containing an <AssertPlaceholder>, and the ex-\npected assertion, which enables the LLMs to better under-\nstand the task.\nChain-of-thought (CoT) prompting generates a\nsequence of short sentences to describe reasoning logics\nstep by step (also known as reasoning chains or rationales)\nto the LLMs for generating the final answer. For example,\nfor program repair from the natural language issue\ndescriptions [122], given the buggy code and issue report,\nthe authors first ask the LLM to localize the bug, and then\nthey ask it to explain why the localized lines are buggy,\nfinally, they ask the LLM to fix the bug. Another example is\nfor generating unusual programs for fuzzing deep learning\nlibraries, Deng et al. [58] first generate a possible \u201cbug\u201d (bug\ndescription) before generating the actual \u201cbug-triggering\u201d\ncode snippet that invokes the target API. The predicted\nbug description provides an additional hint to the LLM,\nindicating that the generated code should try to cover\nspecific potential buggy behavior.\nSelf-consistency involves evaluating the coherence and\nconsistency of the LLM\u2019s responses on the same input in\ndifferent contexts. There is one study with this prompt\ntype, and it is about debugging. Kang et al. [83] employ a\nhypothesize-observe-conclude loop, which first generates\na hypothesis about what the bug is and constructs an\nexperiment to verify, using an LLM, then decide whether\nthe hypothesis is correct based on the experiment result\n(with a debugger or code execution) using an LLM, after\nthat, depending on the conclusion, it either starts with a\nnew hypothesis or opts to terminate the debugging process\nand generate a fix.\nAutomatic prompt aims to automatically generate and\nselect the appropriate instruction for the LLMs, instead of\nrequiring the user to manually engineer a prompt. Xia et\nal. [67] introduce an auto-prompting step that automatically\ndistils all user-provided inputs into a concise and effective\nprompt for fuzzing. Specifically, they first generate a list of\ncandidate prompts by incorporating the user inputs and\nauto prompting instruction while setting the LLM at high\ntemperature, then a small-scale fuzzing experiment is con-\nducted to evaluate each candidate prompt, and the best one\nis selected.\nNote that there are fourteen studies that apply the it-\nerative prompt design when using zero-shot or few-shot\nlearning, in which the approach continuously refines the\nprompts with the running information of the testing task,\ne.g., the test failure information. For example, for program\nrepair, Xia et al. [115] interleave patch generation with test\nvalidation feedback to prompt future generation iteratively.\nIn detail, they incorporate various information from a failing\ntest including its name, the relevant code line(s) triggering\nthe test failure, and the error message produced in the next\nround of prompting which can help the model understand\nthe failure reason and provide guidance towards generating\nthe correct fix. Another example is for mobile GUI testing,\nLiu et al. [14] iteratively query the LLM about the operation\n(e.g., click a button, enter a text) to be conducted in the\nmobile app, and at each iteration, they would provide the\nLLM with current context information like which GUI pages\nand widgets have just explored.\nMapping between testing tasks and how LLMs are\nused. Figure 8 demonstrates the mapping between the test-\ning tasks (mentioned in Section 4) and how LLMs are used\n(as introduced in this subsection). The unit test case gen-\neration and program repair share similar patterns of com-\nmunicating with the LLMs, since both tasks are closely re-\nlated to the source code. Typically, researchers utilize pre-\ntraining and/or fine-tuning and zero-shot learning methods\nfor these two tasks. Zero-shot learning is suitable because\nthese tasks are relatively straightforward and can be easily\nunderstood by LLMs. Moreover, since the training data for\nthese two tasks can be automatically collected from source\ncode repositories, pre-training and/or fine-tuning methods",
            "16\nFig. 8: Mapping between testing tasks and how LLMs are\nused\nCode, 78\n68%\nBug description, 1210%\nError information, 7\n6%\nView hierarchy file of UI, 6\n5%\nOthers, 12\n10%\nFig. 9: Input of LLM\nare widely employed for these two tasks, which can enhance\nLLMs\u2019 understanding of domain-specific knowledge.\nIn comparison, for system test input generation, zero-\nshot learning and few-shot learning methods are commonly\nused. This might be because this task often involves gener-\nating specific types of inputs, and demonstrations in few-\nshot learning can assist the LLMs in better understanding\nwhat should be generated. Besides, for this task, the uti-\nlization of pre-training and/or fine-tuning methods are not\nas widespread as in unit test case generation and program\nrepair. This might be attributed to the fact that training data\nfor system testing varies across different software and is\nrelatively challenging to collect automatically.\n5.3 Input of LLM\nWe also find that different testing tasks or software under\ntest might involve diversified input when querying the\nLLM, as demonstrated in Figure 9.\nThe most commonly utilized input is the source code\nsince a large portion of collected studies relate to program\nrepair or unit test case generation whose input are source\ncode. For unit test case generation, typical code-related in-\nformation would be (i) the complete focal method, including\nthe signature and body; (ii) the name of the focal class (i.e.,\nthe class that the focal method belongs to); (iii) the field in\nthe focal class; and (iv) the signatures of all methods defined\nin the focal class [7], [26]. For program repair, there can be\ndifferent setups and involve different inputs, including (i)\ninputting a buggy function with the goal of outputting the\npatched function, (ii) inputting the buggy location with the\ngoal of generating the correct replacement code (can be a\nsingle line change) given the prefix and suffix of the buggy\nfunction [93]. Besides, there can be variations for the buggy\nlocation input, i.e., (i) does not contain the buggy lines (but\nthe bug location is still known), (ii) give the buggy lines as\nlines of comments.\nThere are also 12 studies taking the bug description as\ninput for the LLM. For example, Kang et al. [78] take the\nbug description as input when querying LLM and let the\nLLM generate the bug-reproducing test cases. Fakhoury et\nal. [122] input the natural language descriptions of bugs to\nthe LLM, and generate the correct code fixes.\nThere are 7 studies that would provide the intermedi-\nate error information , e.g., test failure information, to the\nLLM, and would conduct the iterative prompt (as described\nin Section 5.2) to enrich the context provided to the LLM.\nThese studies are related to the unit test case generation\nand program repair, since in these scenarios, the running\ninformation can be acquired easily.\nWhen testing mobile apps, since the utilized LLM could\nnot understand the image of the GUI page, the view hierar-\nchy file which represents the details of the GUI page usually\nacts as the input to LLMs. Nevertheless, with the emergence\nof GPT-4 which is a multimodal model and accepts both\nimage and text inputs for model input, the GUI screenshots\nmight be directly utilized for LLM\u2019s input.\n5.4 Incorporating Other Techniques with LLM\nThere are divided opinions on whether LLM has reached\nan all-powerful status that requires no other techniques. As\nshown in Figure 10, among our collected studies, 67 of them\nutilize LLMs to address the entire testing task, while 35 stud-\nies incorporate additional techniques. These techniques in-\nclude mutation testing, differential testing, syntactic check-\ning, program analysis, statistical analysis, etc. .\nThe reason why researchers still choose to combine\nLLMs with other techniques might be because, despite\nexhibiting enormous potential in various tasks, LLMs still\npossess limitations such as comprehending code semantics\nand handling complex program structures. Therefore,\ncombining LLMs with other techniques optimizes their\nstrengths and weaknesses to achieve better outcomes in\nspecific scenarios. In addition, it is important to note that\nwhile LLMs are capable of generating correct code, they\nmay not necessarily produce sufficient test cases to check\nfor edge cases or rare scenarios. This is where mutation\nand other testing techniques come into play, as they allow\nfor the generation of more diverse and complex code that\ncan better simulate real-world scenarios. Taken in this\nsense, a testing approach can incorporate a combination\nof different techniques, including both LLMs and other\ntesting strategies, to ensure comprehensive coverage and\neffectiveness.\nLLM + statistical analysis. As LLMs can often generate\na multitude of outputs, manually sifting through and iden-\ntifying the correct output can be overwhelmingly laborious.\nAs such, researchers have turned to statistical analysis tech-\nniques like ranking and clustering [28], [45], [78], [93], [116]",
            "17\nFig. 10: Distribution about other techniques incorporated with LLMs (Note that, a study can involve multiple types)\nto efficiently filter through LLM\u2019s outputs and ultimately\nobtain more accurate results.\nLLM + program analysis. When utilizing LLMs to\naccomplish tasks such as generating unit test cases and\nrepairing software code, it is important to consider that\nsoftware code inherently possesses structural information,\nwhich may not be fully understood by LLMs. Hence,\nresearchers often utilize program analysis techniques,\nincluding code abstract syntax trees (ASTs) [74], to\nrepresent the structure of code more effectively and increase\nthe LLM\u2019s ability to comprehend the code accurately.\nResearchers also perform the structure-based subsetting\nof code lines to narrow the focus for LLM [94], or extract\nadditional code context from other code files [7], to enable\nthe models to focus on the most task-relevant information\nin the codebase and lead to more accurate predictions.\nLLM + mutation testing. It is mainly targeting at gener-\nating more diversified test inputs. For example, Deng et al.\n[59] first use LLM to generate the seed programs (e.g., code\nsnippets using a target DL API) for fuzzing deep learning\nlibraries. To enrich the pool of these test programs, they\nreplace parts of the seed program with masked tokens using\nmutation operators (e.g., replaces the API call arguments\nwith the span token) to produce masked inputs, and again\nutilize the LLMs to perform code infilling to generate new\ncode that replaces the masked tokens.\nLLM + syntactic checking. Although LLMs have shown\nremarkable performance in various natural language pro-\ncessing tasks, the generated code from these models can\nsometimes be syntactically incorrect, leading to potential er-\nrors and reduced usability. Therefore, researchers have pro-\nposed to leverage syntax checking to identify and correct\nerrors in the generated code. For example, in their work for\nunit test case generation, Alagarsamy et al. [29] addition-\nally introduce a verification method to check and repair the\nnaming consistency (i.e., revising the test method name to\nbe consistent with the focal method name) and the test sig-\nnatures (i.e., adding missing keywords like public, void, or\n@test annotations). Xie et al. [36] also validates the generated\nunit test case and employs rule-based repair to fix syntactic\nand simple compile errors.\nLLM + differential testing. Differential testing is well-\nsuited to find semantic or logic bugs that do not exhibit\nexplicit erroneous behaviors like crashes or assertion\nfailures. In this category of our collected studies, the LLM\nis mainly responsible for generating valid and diversified\ninputs, while the differential testing helps to determine\nwhether there is a triggered bug based on the software\u2019s\noutput. For example, Ye et al. [48] first uses LLM to\nproduce random JavaScript programs, and leverages the\nlanguage specification document to generate test data, then\nconduct the differential testing on JavaScript engines such\nas JavaScriptCore, ChakraCore, SpiderMonkey, QuickJS,\netc. There are also studies utilizing the LLMs to generate\ntest inputs and then conduct differential testing for fuzzing\nDL libraries [58], [59] and SAT solvers [63]. Li et al. [87]\nemploys the LLM in finding the failure-inducing test cases.\nIn detail, given a program under test, they first request the\nLLM to infer the intention of the program, then request the\nLLM to generate programs that have the same intention,\nwhich are alternative implementations of the program, and\nare likely free of the program\u2019s bug. Then they perform\nthe differential testing with the program under test and the\ngenerated programs to find the failure-inducing test cases.\n6 C HALLENGES AND OPPORTUNITIES\nBased on the above analysis from the viewpoints of soft-\nware testing and LLM, we summarize the challenges and\nopportunities when conducting software testing with LLM.\n6.1 Challenges\nAs indicated by this survey, software testing with LLMs\nhas undergone significant growth in the past two years.\nHowever, it is still in its early stages of development, and\nnumerous challenges and open questions need to be ad-\ndressed.\n6.1.1 Challenges for Achieving High Coverage\nExploring the diverse behaviors of the software under test\nto achieve high coverage is always a significant concern\nin software testing. In this context, test generation differs\nfrom code generation, as code generation primarily focuses\non producing a single, correct code snippet, whereas soft-\nware testing requires generating diverse test inputs to en-\nsure better coverage of the software. Although setting a high\ntemperature can facilitate the LLMs in generating different\noutputs, it remains challenging for LLMs to directly achieve\nthe required diversity. For example, for unit test case gen-\neration, in SF110 dataset, the line coverage is merely 2%\nand the branch coverage is merely 1% [39]. For system test\ninput generation, in terms of fuzzing DL libraries, the API\ncoverage for TensorFlow is reported to be 66% (2215/3316)\n[59].",
            "18\nFrom our collected studies, we observe that the\nresearchers often utilize mutation testing together with the\nLLMs to generate more diversified outputs. For example,\nwhen fuzzing a DL library, instead of directly generating\nthe code snippet with LLM, Deng et al. [59] replace parts\nof the selected seed (code generated by LLM) with masked\ntokens using different mutation operators to produce\nmasked inputs. They then leverage the LLM to perform\ncode infilling to generate new code that replaces the masked\ntokens, which can significantly increase the diversity of the\ngenerated tests. Liu et al. [65] leverage LLM to produce the\ntest generators (each of which can yield a batch of unusual\ntext inputs under the same mutation rule) together with the\nmutation rules for text-oriented fuzzing, which reduces the\nhuman effort required for designing mutation rules.\nA potential research direction could involve utilizing\ntesting-specific data to train or fine-tune a specialized LLM\nthat is specifically designed to understand the nature of\ntesting. By doing so, the LLM can inherently acknowledge\nthe requirements of testing and autonomously generate\ndiverse outputs.\n6.1.2 Challenges in Test Oracle Problem\nThe oracle problem has been a longstanding challenge in\nvarious testing applications, e.g., testing machine learning\nsystems [150] and testing deep learning libraries [59]. To\nalleviate the oracle problem to the overall testing activities,\na common practice in our collected studies is to transform it\ninto a more easily derived form, often by utilizing differen-\ntial testing [63] or focusing on only identifying crash bugs\n[14].\nThere are successful applications of differential testing\nwith LLMs, as shown in Figure 10. For instance, when\ntesting the SMT solvers, Sun et al. adopt differential testing\nwhich involves comparing the results of multiple SMT\nsolvers (i.e., Z3, cvc5, and Bitwuzla) on the same generated\ntest formulas by LLM [63]. However, this approach is\nlimited to systems where counterpart software or running\nenvironment can easily be found, potentially restricting\nits applicability. Moreover, to mitigate the oracle problem,\nother studies only focus on the crash bugs which are easily\nobserved automatically. This is particularly the case for\nmobile applications testing, in which the LLMs guide the\ntesting in exploring more diversified pages, conducting\nmore complex operational actions, and covering more\nmeaningful operational sequences [14]. However, this\nsignificantly restricts the potential of utilizing the LLMs for\nuncovering various types of software bugs.\nExploring the use of LLMs to derive other types of\ntest oracles represents an interesting and valuable research\ndirection. Specifically, metamorphic testing is also widely\nused in software testing practices to help mitigate the oracle\nproblem, yet in most cases, defining metamorphic relations\nrelies on human ingenuity. Luu et al. [56] have examined the\neffectiveness of LLM in generating metamorphic relations,\nyet they only experiment with straightforward prompts by\ndirectly querying ChatGPT. Further exploration, potentially\nincorporating human-computer interaction or domain\nknowledge, is highly encouraged. Another promising\navenue is exploring the capability of LLMs to automatically\ngenerate test cases based on metamorphic relations,\ncovering a wide range of inputs.\nThe advancement of multi-model LLMs like GPT-4 may\nopen up possibilities for exploring their ability to detect\nbugs in software user interfaces and assist in deriving test\noracles. By leveraging the image understanding and reason-\ning capabilities of these models, one can investigate their\npotential to automatically identify inconsistencies, errors, or\nusability issues in user interfaces.\n6.1.3 Challenges for Rigorous Evaluations\nThe lack of benchmark datasets and the potential data leak-\nage issues associated with LLM-based techniques present\nchallenges in conducting rigorous evaluations and compre-\nhensive comparisons of proposed methods.\nFor program repair, there are only two well-known and\ncommonly-used benchmarks, i.e., Defect4J and QuixBugs,\nas demonstrated in Table 4. Furthermore, these datasets are\nnot specially designed for testing the LLMs. For example, as\nreported by Xia et al. [93], 39 out of 40 Python bugs in the\nQuixBugs dataset can be fixed by Codex, yet in real-world\npractice, the successful fix rate can be nowhere near as high.\nFor unit test case generation, there are no widely recognized\nbenchmarks, and different studies would utilize different\ndatasets for performance evaluation, as demonstrated in Ta-\nble 3. This indicates the need to build more specialized and\ndiversified benchmarks.\nFurthermore, the LLMs may have seen the widely-used\nbenchmarks in their pre-training data, i.e., data leakage\nissues. Jiang et al. [113] check the CodeSearchNet and\nBigQuery, which are the data sources of common LLMs,\nand the results show that four repositories used by the\nDefect4J benchmark are also in CodeSearchNet, and the\nwhole Defects4J repository is included by BigQuery.\nTherefore, it is very likely that existing program repair\nbenchmarks are seen by the LLMs during pre-training. This\ndata leakage issue has also been investigated in machine\nlearning-related studies. For example, Tu et al. [151] focus\non the data leakage in issue tracking data, and results show\nthat information leaked from the \u201cfuture\u201d makes prediction\nmodels misleadingly optimistic. This reminds us that the\nperformance of LLMs on software testing tasks may not be\nas good as reported in previous studies. It also suggests\nthat we need more specialized datasets that are not seen by\nLLMs to serve as benchmarks. One way is to collect it from\nspecialized sources, e.g., user-generated content from niche\nonline communities.\n6.1.4 Challenges in Real-world Application of LLMs in Soft-\nware Testing\nAs we mentioned in Section 5.2, in the early days of us-\ning LLMs, pre-training and fine-tuning are commonly used\npractice, considering the model parameters are relatively\nfew resulting in weaker model capabilities (e.g., T5). As time\nprogressed, the number of model parameters increased sig-\nnificantly, leading to the emergence of models with greater\ncapabilities (e.g., ChatGPT). And in recent studies, prompt\nengineering has become a common approach. However, due\nto concerns regarding data privacy, when considering real-\nworld practice, most software organizations tend to avoid",
            "19\nusing commercial LLMs and would prefer to adopt open-\nsource ones with training or fine-tuning using organization-\nspecific data. Furthermore, some companies also consider\nthe current limitations in terms of computational power or\npay close attention to energy consumption, they tend to\nfine-tune medium-sized models. It is quite challenging for\nthese models to achieve similar performance to what our\ncollected papers have reported. For instance, in the widely-\nused QuixBugs dataset, it has been reported that 39 out of\n40 Python bugs and 34 out of 40 Java bugs can be automat-\nically fixed [93]. However, when it comes to DL programs\ncollected from Stack Overflow, which represent real-world\ncoding practice, only 16 out of 72 Python bugs can be auto-\nmatically fixed [89].\nRecent research has highlighted the importance of high-\nquality training data in improving the performance of mod-\nels for code-related tasks [152], yet manually building high-\nquality organization-specific datasets for training or fine-\ntuning is time-consuming and labor-intensive. To address\nthis, one is encouraged to utilize the automated techniques\nof mining software repositories to build the datasets, for\nexample, techniques like key information extraction tech-\nniques from Stack Overflow [153] offer potential solutions\nfor automatically gathering relevant data.\nIn addition, exploring the methodology for better fine-\ntuning the LLMs with software-specific data is worth con-\nsidering because software-specific data differs from natural\nlanguage data as it contains more structural information,\nsuch as data flow and control flow. Previous research on\ncode representations has shown the benefits of incorporat-\ning data flow, which captures the semantic-level structure\nof code and represents the relationship between variables in\nterms of \u201cwhether-value-comes-from\u201d [154]. These insights\ncan provide valuable guidance for effectively fine-tuning\nLLMs with software-specific data.\n6.2 Opportunities\nThere are also many research opportunities in software test-\ning with LLMs, which can greatly benefit developers, users,\nand the research community. While not necessarily chal-\nlenges, these opportunities contribute to advancements in\nsoftware testing, benefiting practitioners and the wider re-\nsearch community.\n6.2.1 Exploring LLMs in the Early Stage of Testing\nAs shown in Figure 4, LLMs have not been used in the early\nstage of testing, e.g., test requirements, and test planning.\nThere might be two main reasons behind that. The first is\nthe subjectivity in early-stage testing tasks. Many tasks in\nthe early stages of testing, such as requirements gathering,\ntest plan creation, and design reviews, may involve subjec-\ntive assessments that require significant input from human\nexperts. This could make it less suitable for LLMs that rely\nheavily on data-driven approaches. The second might be the\nlack of open-sourced data in the early stages. Unlike in later\nstages of testing, there may be limited data available online\nduring early-stage activities. This could mean that LLMs\nmay not have seen much of this type of data, and therefore\nmay not perform well on these tasks.\nAdopting a human-computer interaction schema for\ntackling early-stage testing tasks would harness the domain-\nspecific knowledge of human developers and leverage the\ngeneral knowledge embedded in LLMs. Additionally, it is\nhighly encouraged for software development companies\nto record and provide access to early-stage testing data,\nallowing for improved training and performance of LLMs\nin these critical testing activities.\n6.2.2 Exploring LLMs in Other Testing Phases\nWe have analyzed the distribution of testing phases for the\ncollected studies. As shown in Fig 11, we can observe that\nLLMs are most commonly used in unit testing, followed by\nsystem testing. However, there is still no research on the use\nof LLMs in integration testing and acceptance testing.\n/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018\n/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n/uni00000038/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057\n/uni0000002c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000036/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000024/uni00000046/uni00000046/uni00000048/uni00000053/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000056\n/uni00000015/uni00000017\n/uni00000013\n/uni00000015/uni00000015\n/uni00000013\nFig. 11: Distribution of testing phases (note that we omit the\nstudies which do not explicitly specify the testing phases,\ne.g., program repair)\nFor integration testing, it involves testing the interfaces\nbetween different software modules. In some software or-\nganizations, integration testing might be merged with unit\ntesting, which can be a possible reason why LLM is rarely\nutilized in integration testing. Another reason might be that\nthe size and complexity of the input data in this circum-\nstance may exceed the capacity of the LLM to process and\nanalyze (e.g., the source code of all involved software mod-\nules), which can lead to errors or unreliable results. To tackle\nthis, a potential reference can be found in Section 4.1, where\nXie et al. [36] design a method to organize the necessary\ninformation into the pre-defined maximum prompt token\nlimit of the LLM. Furthermore, integration testing requires\ndiversified data to be generated to sufficiently test the in-\nterface among multiple modules. As mentioned in Section\n4.3, previous work has demonstrated the LLM\u2019s capability\nin generating diversified test input for system testing, in\nconjunction with mutation testing techniques [48], [59]. And\nthese can provide insights about generating the diversified\ninterface data for integration testing.\nAcceptance testing is usually conducted by business an-\nalysts or end-users to validate the system\u2019s functionality\nand usability, which requires more non-technical language\nand domain-specific knowledge, thus making it challenging\nto apply LLM effectively. Since acceptance testing involves\nhumans, it is well-suited for the use of human-in-the-loop\nschema with LLMs. This has been studied in traditional\nmachine learning [155], but has not yet been explored with\nLLMs. Specifically, the LLMs can be responsible for auto-\nmatically generating test cases, evaluating test coverage, etc,\nwhile human testers are responsible for checking the pro-\ngram\u2019s behavior and verifying test oracle.",
            "20\n6.2.3 Exploring LLMs for More Types of Software\nWe analyze what types of software have been explored in\nthe collected studies, as shown in Figure 5. Note that, since\na large portion of studies are focused on unit testing or\nprogram repair, they are conducted on publicly available\ndatasets and do not involve specific software types.\nFrom the analysis in Section 4.3, the LLM can generate\nnot only the source code for testing DL libraries but also\nthe textual input for testing mobile apps, even the models\nfor testing CPS. Overall, the LLM provides a flexible and\npowerful framework for generating test inputs for a wide\nrange of applications. Its versatility would make it useful\nfor testing the software in other domains.\nFrom one point of view, some proposed techniques can\nbe applied to other types of software. For example, in the\npaper proposed for testing deep learning libraries [58], since\nit proposes techniques for generating diversified, compli-\ncated, and human-like DL programs, the authors state that\nthe approach can be easily extended to test software systems\nfrom other application domains, e.g., interpreters, database\nsystems, and other popular libraries. More than that, there\nare already studies that focus on universal fuzzing tech-\nniques [52], [67] which are designed to be adaptable and\napplicable to different types of test inputs and software.\nFrom another point of view, other types of software can\nalso benefit from the capabilities of LLMs to design the test-\ning techniques that are better suited to their specific do-\nmain and characteristics. For instance, the metaverse, with\nits immersive virtual environments and complex interac-\ntions, presents unique challenges for software testing. LLMs\ncan be leveraged to generate diverse and realistic inputs that\nmimic user behavior and interactions within the metaverse,\nwhich are never explored.\n6.2.4 Exploring LLMs for Non-functional Testing\nIn our collected studies, LLMs are primarily used for func-\ntional testing, and no practice in performance testing, usabil-\nity testing or others. One possible reason for the prevalence\nof LLM-based solutions in functional testing is that they\ncan convert functional testing problems into code gener-\nation or natural language generation problems [14], [59],\nwhich LLMs are particularly adept at solving.\nOn the other hand, performance testing and usability\ntesting may require more specialized models that are de-\nsigned to detect and analyze specific types of data, handle\ncomplex statistical analyses, or determine the buggy criteria.\nMoreover, there have been dozens of performance testing\ntools (e.g., LoadRunner [156]) that can generate a workload\nthat simulates real-world usage scenarios and achieve rela-\ntively satisfactory performance.\nThe potential opportunities might let the LLM integrate\nthe performance testing tools and acts like the LangChain\n[157], to better simulate different types of workloads based\non real user behavior. Furthermore, the LLMs can identify\nthe parameter combinations and values that have the high-\nest potential to trigger performance problems. It is essen-\ntially a way to rank and prioritize different parameter set-\ntings based on their impact on performance and improve\nthe efficiency of performance testing.\n6.2.5 Exploring Advanced Prompt Engineering\nThere are a total of 11 commonly used prompt engineering\ntechniques as listed in a popular prompt engineering guide\n[158], as shown in Figure 12. Currently, in our collected\nstudies, only the first five techniques are being utilized. The\nmore advanced techniques have not been employed yet, and\ncan be explored in the future for prompt design.\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013\n/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n/uni0000003d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000029/uni00000048/uni0000005a/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000026/uni0000004b/uni00000044/uni0000004c/uni00000051/uni00000010/uni00000052/uni00000049/uni00000010/uni00000057/uni0000004b/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000057\n/uni00000036/uni00000048/uni0000004f/uni00000049/uni00000010/uni00000046/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni00000024/uni00000058/uni00000057/uni00000052/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000004e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni00000037/uni00000055/uni00000048/uni00000048/uni00000010/uni00000052/uni00000049/uni00000010/uni00000057/uni0000004b/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000057/uni00000056\n/uni00000024/uni00000046/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000010/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni00000027/uni0000004c/uni00000055/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000057/uni0000004c/uni00000050/uni00000058/uni0000004f/uni00000058/uni00000056/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni00000035/uni00000048/uni00000024/uni00000046/uni00000057/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000050/uni00000052/uni00000047/uni00000044/uni0000004f/uni00000003/uni00000046/uni0000004b/uni00000044/uni0000004c/uni00000051/uni00000010/uni00000052/uni00000049/uni00000010/uni00000057/uni0000004b/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000057\n/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000003/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057\n/uni00000024/uni00000058/uni00000057/uni00000052/uni00000050/uni00000044/uni00000057/uni0000004c/uni00000046/uni00000003/uni00000055/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni00000044/uni00000051/uni00000047/uni00000003/uni00000057/uni00000052/uni00000052/uni0000004f/uni00000010/uni00000058/uni00000056/uni00000048\n/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni00000028/uni00000051/uni0000004a/uni0000004c/uni00000051/uni00000048/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a\n/uni00000018/uni00000014\n/uni00000015/uni00000018\n/uni0000001a\n/uni00000014\n/uni00000014\n/uni00000013\n/uni00000013\n/uni00000013\n/uni00000013\n/uni00000013\n/uni00000013\n/uni00000013\n/uni00000013\nFig. 12: List of advanced prompt engineering practices and\nthose utilized in the collected papers\nFor instance, multimodal chain of thought prompting in-\nvolves using diverse sensory and cognitive cues to stimulate\nthinking and creativity in LLMs [159]. By providing images\n(e.g., GUI screenshots) or audio recordings related to the\nsoftware under test can help the LLM better understand\nthe software\u2019s context and potential issues. Besides, try to\nprompt the LLM to imagine itself in different roles, such\nas a developer, user, or quality assurance specialist. This\nperspective-shifting exercise enables the LLM to approach\nsoftware testing from multiple viewpoints and uncover dif-\nferent aspects that might require attention or investigation.\nGraph prompting [160] involves the representation of\ninformation using graphs or visual structures to facilitate\nunderstanding and problem-solving. Graph prompting can\nbe a natural match with software engineering, consider\nit involves various dependencies, control flow, data flow,\nstate transitions, or other relevant graph structure. Graph\nprompting can be beneficial in analyzing this structural\ninformation, and enabling the LLMs to comprehend the\nsoftware under test effectively. For instance, testers can use\ngraph prompts to visualize test coverage, identify untested\nareas or paths, and ensure adequate test execution.\n6.2.6 Incorporating LLMs with Traditional Techniques\nThere is currently no clear consensus on the extent to which\nLLMs can solve software testing problems. From the analy-\nsis in Section 5.4, we have seen some promising results from\nstudies that have combined LLMs with traditional software\ntesting techniques. This implies the LLMs are not the sole\nsilver bullet for software testing. Considering the availabil-\nity of many mature software testing techniques and tools,\nand the limited capabilities of LLMs, it is necessary to ex-\nplore other better ways to combine LLMs with traditional\ntesting or program analysis techniques and tools for better\nsoftware testing.",
            "21\nBased on the collected studies, the LLMs have been suc-\ncessfully utilized together with various techniques such as\ndifferential testing (e.g., [63]), mutation testing (e.g., [59]),\nprogram analysis (e.g., [104], as shown in Figure 10. From\none perspective, future studies can explore improved in-\ntegration of these traditional techniques with LLMs. Take\nmutation testing as an example, current practices mainly\nrely on the human-designed mutation rules to mutate the\ncandidate tests, and let the LLMs re-generate new tests [38],\n[59], [67], while Liu et al. directly utilize the LLMs for pro-\nducing the mutation rules alongside the mutated tests [65].\nFurther explorations in this direction are of great interest.\nFrom another point of view, more traditional techniques\ncan be incorporated in LLMs for software testing. For in-\nstance, besides the aforementioned traditional techniques,\nthe LLMs have been combined with formal verification for\nself-healing software detection in the field of software se-\ncurity [161]. More attempts are encouraged. Moreover, con-\nsidering the existence of numerous mature software testing\ntools, one can explore the integration of LLMs with these\ntools, allowing them to act as a \u201cLangChain\u201d to better ex-\nplore the potential of these tools.\n7 R ELATED WORK\nThe systematic literature review is a crucial manner for gain-\ning insights into the current trends and future directions\nwithin a particular field. It enables us to understand and\nstay updated on the developments in that domain.\nWang et al. surveyed the machine learning and deep\nlearning techniques for software engineering [162]. Yang et\nal. and Watson et al. respectively carried out surveys about\nthe use of deep learning in software engineering domain\n[163], [164]. Bajammal et al. surveyed the utilization of com-\nputer vision techniques to improve software engineering\ntasks [165]. Zhang et al. provided a survey of techniques\nfor testing machine learning systems [150]\nWith the advancements of artificial intelligence and\nLLMs, researchers also conduct systematic literature\nreviews about LLMs, and their applications in various\nfields (e.g., software engineering). Zhao et al. [17] reviewed\nrecent advances in LLMs by providing an overview of their\nbackground, key findings, and mainstream techniques.\nThey focused on four major aspects of LLMs, namely\npre-training, adaptation tuning, utilization, and capacity\nevaluation. Additionally, they summarized the available\nresources for developing LLMs and discuss the remaining\nissues for future directions. Hou et al. conducted a\nsystematic literature review on using LLMs for software\nengineering, with a particular focus on understanding\nhow LLMs can be exploited to optimize processes and\noutcomes [166]. Fan et al. conducted a survey of LLMs for\nsoftware engineering, and set out open research challenges\nfor the application of LLMs to technical problems faced by\nsoftware engineers [167]. Zan et al. conducted a survey of\nexisting LLMs for NL2Code task (i.e., generating code from\na natural language description), and reviewed benchmarks\nand metrics [168].\nWhile these studies either targeted the broader software\nengineering domain (with a limited focus on software test-\ning tasks) or focused on other software development tasks\n(excluding software testing), this paper specifically focuses\non the use of LLMs for software testing. It surveys related\nstudies, summarizes key challenges and potential opportu-\nnities, and serves as a roadmap for future research in this\narea.\n8 C ONCLUSION\nThis paper provides a comprehensive review of the use\nof LLMs in software testing. We have analyzed relevant\nstudies that have utilized LLMs in software testing from\nboth the software testing and LLMs perspectives. This paper\nalso highlights the challenges and potential opportunities\nin this direction. Results of this review demonstrate that\nLLMs have been successfully applied in a wide range\nof testing tasks, including unit test case generation, test\noracle generation, system test input generation, program\ndebugging, and program repair. However, challenges still\nexist in achieving high testing coverage, addressing the\ntest oracle problem, conducting rigorous evaluations, and\napplying LLMs in real-world scenarios. Additionally, it is\nobserved that LLMs are commonly used in only a subset of\nthe entire testing lifecycle, for example, they are primarily\nutilized in the middle and later stages of testing, only\nserving the unit and system testing phases, and only for\nfunctional testing. This highlights the research opportunities\nfor exploring the uncovered areas. Regarding how the LLMs\nare utilized, we find that various pre-training/fine-tuning\nand prompt engineering methods have been developed\nto enhance the capabilities of LLMs in addressing testing\ntasks. However, more advanced techniques in prompt\ndesign have yet to be explored and can be an avenue for\nfuture research.\nIt can serve as a roadmap for future research in this area,\nidentifying gaps in our current understanding of the use of\nLLMs in software testing and highlighting potential avenues\nfor exploration. We believe that the insights provided in this\npaper will be valuable to both researchers and practition-\ners in the field of software engineering, assisting them in\nleveraging LLMs to improve software testing practices and\nultimately enhance the quality and reliability of software\nsystems.\nREFERENCES\n[1] G. J. Myers, The art of software testing (2. ed.) . Wiley,\n2004. [Online]. Available: http://eu.wiley.com/WileyCDA/\nWileyTitle/productCd-0471469122.html\n[2] M. Pezz `e and M. Young, Software testing and analysis - process,\nprinciples and techniques. Wiley, 2007.\n[3] M. Harman and P . McMinn, \u201cA theoretical and empirical study\nof search-based testing: Local, global, and hybrid search,\u201d vol. 36,\nno. 2, 2010, pp. 226\u2013247.\n[4] P . Delgado-P \u00b4erez, A. Ram \u00b4\u0131rez, K. J. Valle-G \u00b4omez, I. Medina-\nBulo, and J. R. Romero, \u201cInterevo-tr: Interactive evolutionary\ntest generation with readability assessment,\u201d IEEE Trans. Software\nEng., vol. 49, no. 4, pp. 2580\u20132596, 2023.\n[5] X. Xiao, S. Li, T. Xie, and N. Tillmann, \u201cCharacteristic studies\nof loop problems for structural test generation via symbolic\nexecution,\u201d in 2013 28th IEEE/ACM International Conference on\nAutomated Software Engineering, ASE 2013, Silicon Valley, CA, USA,\nNovember 11-15, 2013 , E. Denney, T. Bultan, and A. Zeller, Eds.\nIEEE, 2013, pp. 246\u2013256.\n[6] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, \u201cFeedback-\ndirected random test generation,\u201d in 29th International Conference\non Software Engineering (ICSE 2007), Minneapolis, MN, USA, May\n20-26, 2007. IEEE Computer Society, 2007, pp. 75\u201384.",
            "22\n[7] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng,\n\u201cNo more manual tests? evaluating and improving chatgpt for\nunit test generation,\u201d arXiv preprint arXiv:2305.04207, 2023.\n[8] Y. Tang, Z. Liu, Z. Zhou, and X. Luo, \u201cChatgpt vs SBST:\nA comparative assessment of unit test suite generation,\u201d\nCoRR, vol. abs/2307.00588, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2307.00588\n[9] A. Developers, \u201cUi/application exerciser monkey,\u201d 2012.\n[10] Y. Li, Z. Yang, Y. Guo, and X. Chen, \u201cDroidbot: a lightweight ui-\nguided test input generator for android,\u201d in ICSE. IEEE, 2017.\n[11] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, and\nZ. Su, \u201cGuided, stochastic model-based gui testing of android\napps,\u201d in Proceedings of the 2017 11th Joint Meeting on Foundations\nof Software Engineering, 2017, pp. 245\u2013256.\n[12] Z. Dong, M. B \u00a8ohme, L. Cojocaru, and A. Roychoudhury, \u201cTime-\ntravel testing of android apps,\u201d in ICSE. IEEE, 2020.\n[13] M. Pan, A. Huang, G. Wang, T. Zhang, and X. Li, \u201cReinforcement\nlearning based curiosity-driven testing of android applications,\u201d\nin Proceedings of the 29th ACM SIGSOFT International Symposium\non Software Testing and Analysis, 2020, pp. 153\u2013164.\n[14] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che, D. Wang,\nand Q. Wang, \u201cMake LLM a testing expert: Bringing human-\nlike interaction to mobile GUI testing via functionality-aware\ndecisions,\u201d CoRR, vol. abs/2310.15780, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.15780\n[15] T. Su, J. Wang, and Z. Su, \u201cBenchmarking automated GUI testing\nfor android against real-world bugs,\u201d in ESEC/FSE \u201921: 29th ACM\nJoint European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering, Athens, Greece, August 23-\n28, 2021. ACM, 2021, pp. 119\u2013130.\n[16] M. Shanahan, \u201cTalking about large language models,\u201d\nCoRR, vol. abs/2212.03551, 2022. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2212.03551\n[17] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou,\nY. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang,\nY. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu,\nP . Liu, J. Nie, and J. Wen, \u201cA survey of large language\nmodels,\u201d CoRR, vol. abs/2303.18223, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2303.18223\n[18] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and\nY. Iwasawa, \u201cLarge language models are zero-\nshot reasoners,\u201d in NeurIPS, 2022. [Online]. Avail-\nable: http://papers.nips.cc/paper files/paper/2022/hash/\n8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html\n[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter,\nF. Xia, E. H. Chi, Q. V . Le, and D. Zhou,\n\u201cChain-of-thought prompting elicits reasoning in large\nlanguage models,\u201d in NeurIPS, 2022. [Online]. Avail-\nable: http://papers.nips.cc/paper files/paper/2022/hash/\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html\n[20] J. Li, G. Li, Y. Li, and Z. Jin, \u201cStructured chain-of-thought\nprompting for code generation,\u201d 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:258615421\n[21] J. Li, Y. Li, G. Li, Z. Jin, Y. Hao, and X. Hu, \u201cSkcoder: A\nsketch-based approach for automatic code generation,\u201d in 2023\nIEEE/ACM 45th International Conference on Software Engineering\n(ICSE), 2023, pp. 2124\u20132135.\n[22] J. Li, Y. Zhao, Y. Li, G. Li, and Z. Jin, \u201cAcecoder: Utilizing existing\ncode to enhance code generation,\u201d 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:257901190\n[23] Y. Dong, X. Jiang, Z. Jin, and G. Li, \u201cSelf-collaboration\ncode generation via chatgpt,\u201d CoRR, vol. abs/2304.07590, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2304.07590\n[24] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu,\n\u201cUnifying large language models and knowledge graphs: A\nroadmap,\u201d CoRR, vol. abs/2306.08302, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2306.08302\n[25] G. J. Myers, T. Badgett, T. M. Thomas, and C. Sandler, The art of\nsoftware testing. Wiley Online Library, 2004, vol. 2.\n[26] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sun-\ndaresan, \u201cUnit test case generation with transformers and focal\ncontext,\u201d arXiv preprint arXiv:2009.05617, 2020.\n[27] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and\nW. Chen, \u201cCodet: Code generation with generated tests,\u201d arXiv\npreprint arXiv:2207.10397, 2022.\n[28] S. K. Lahiri, A. Naik, G. Sakkas, P . Choudhury, C. von Veh,\nM. Musuvathi, J. P . Inala, C. Wang, and J. Gao, \u201cInteractive\ncode generation via test-driven user-intent formalization,\u201d arXiv\npreprint arXiv:2208.05950, 2022.\n[29] S. Alagarsamy, C. Tantithamthavorn, and A. Aleti, \u201cA3test:\nAssertion-augmented automated test case generation,\u201d arXiv\npreprint arXiv:2302.10352, 2023.\n[30] M. Sch \u00a8afer, S. Nadi, A. Eghbali, and F. Tip, \u201cAn empirical eval-\nuation of using large language models for automated unit test\ngeneration,\u201d IEEE Transactions on Software Engineering , pp. 1\u201321,\n2023.\n[31] V . Guilherme and A. Vincenzi, \u201cAn initial investigation\nof chatgpt unit test generation capability,\u201d in 8th Brazilian\nSymposium on Systematic and Automated Software Testing, SAST\n2023, Campo Grande, MS, Brazil, September 25-29, 2023 , A. L.\nFont\u02dcao, D. M. B. Paiva, H. Borges, M. I. Cagnin, P . G.\nFernandes, V . Borges, S. M. Melo, V . H. S. Durelli, and E. D.\nCanedo, Eds. ACM, 2023, pp. 15\u201324. [Online]. Available:\nhttps://doi.org/10.1145/3624032.3624035\n[32] S. Hashtroudi, J. Shin, H. Hemmati, and S. Wang,\n\u201cAutomated test case generation using code models and\ndomain adaptation,\u201d CoRR, vol. abs/2308.08033, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2308.08033\n[33] L. Plein, W. C. Ou \u00b4edraogo, J. Klein, and T. F. Bissyand \u00b4e,\n\u201cAutomatic generation of test cases based on bug reports:\na feasibility study with large language models,\u201d CoRR, vol.\nabs/2310.06320, 2023. [Online]. Available: https://doi.org/10.\n48550/arXiv.2310.06320\n[34] V . Vikram, C. Lemieux, and R. Padhye, \u201cCan large\nlanguage models write good property-based tests?\u201d CoRR,\nvol. abs/2307.04346, 2023. [Online]. Available: https://doi.org/\n10.48550/arXiv.2307.04346\n[35] N. Rao, K. Jain, U. Alon, C. L. Goues, and V . J. Hellendoorn,\n\u201cCAT-LM training language models on aligned code and\ntests,\u201d in 38th IEEE/ACM International Conference on Automated\nSoftware Engineering, ASE 2023, Luxembourg, September 11-\n15, 2023 . IEEE, 2023, pp. 409\u2013420. [Online]. Available:\nhttps://doi.org/10.1109/ASE56229.2023.00193\n[36] Z. Xie, Y. Chen, C. Zhi, S. Deng, and J. Yin, \u201cChatunitest: a\nchatgpt-based automated unit test generation tool,\u201darXiv preprint\narXiv:2305.04764, 2023.\n[37] C. Lemieux, J. P . Inala, S. K. Lahiri, and S. Sen, \u201cCodamosa:\nEscaping coverage plateaus in test generation with pre-trained\nlarge language models,\u201d in International conference on software\nengineering (ICSE), 2023.\n[38] A. M. Dakhel, A. Nikanjam, V . Majdinasab, F. Khomh,\nand M. C. Desmarais, \u201cEffective test generation using\npre-trained large language models and mutation testing,\u201d\nCoRR, vol. abs/2308.16557, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2308.16557\n[39] M. L. Siddiq, J. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and V . C.\nLopes, \u201cExploring the effectiveness of large language models in\ngenerating unit tests,\u201d arXiv preprint arXiv:2305.00418, 2023.\n[40] Y. Zhang, W. Song, Z. Ji, D. Yao, and N. Meng, \u201cHow well does\nLLM generate security tests?\u201d CoRR, vol. abs/2310.00710, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2310.00710\n[41] V . Li and N. Doiron, \u201cPrompting code interpreter to write better\nunit tests on quixbugs functions,\u201d CoRR, vol. abs/2310.00483,\n2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.\n00483\n[42] B. Steenhoek, M. Tufano, N. Sundaresan, and A. Svyatkovskiy,\n\u201cReinforcement learning from automatic feedback for high-\nquality unit test generation,\u201d 2023.\n[43] S. Bhatia, T. Gandhi, D. Kumar, and P . Jalote, \u201cUnit test generation\nusing generative ai : A comparative performance analysis of\nautogeneration tools,\u201d 2023.\n[44] M. Tufano, D. Drain, A. Svyatkovskiy, and N. Sundaresan,\n\u201cGenerating accurate assert statements for unit test cases using\npretrained transformers,\u201d in Proceedings of the 3rd ACM/IEEE\nInternational Conference on Automation of Software Test , 2022, pp.\n54\u201364.\n[45] P . Nie, R. Banerjee, J. J. Li, R. J. Mooney, and M. Gligoric,\n\u201cLearning deep semantics for test completion,\u201d arXiv preprint\narXiv:2302.10166, 2023.\n[46] A. Mastropaolo, N. Cooper, D. Nader-Palacio, S. Scalabrino,\nD. Poshyvanyk, R. Oliveto, and G. Bavota, \u201cUsing transfer\nlearning for code-related tasks,\u201d IEEE Trans. Software Eng. ,\nvol. 49, no. 4, pp. 1580\u20131598, 2023. [Online]. Available:\nhttps://doi.org/10.1109/TSE.2022.3183297",
            "23\n[47] N. Nashid, M. Sintaha, and A. Mesbah, \u201cRetrieval-based prompt\nselection for code-related few-shot learning,\u201d in Proceedings of\nthe 45th International Conference on Software Engineering (ICSE\u201923) ,\n2023.\n[48] G. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian,\nH. Wang, and Z. Wang, \u201cAutomated conformance testing for\njavascript engines via deep compiler fuzzing,\u201d in Proceedings of\nthe 42nd ACM SIGPLAN international conference on programming\nlanguage design and implementation, 2021, pp. 435\u2013450.\n[49] Z. Liu, C. Chen, J. Wang, X. Che, Y. Huang, J. Hu, and Q. Wang,\n\u201cFill in the blank: Context-aware automated text input generation\nfor mobile gui testing,\u201d arXiv preprint arXiv:2212.04732, 2022.\n[50] M. R. Taesiri, F. Macklon, Y. Wang, H. Shen, and C.-P . Bezemer,\n\u201cLarge language models are pretty good zero-shot video game\nbug detectors,\u201d arXiv preprint arXiv:2210.02506, 2022.\n[51] S. L. Shrestha and C. Csallner, \u201cSlgpt: using transfer learning\nto directly generate simulink model files and find bugs in the\nsimulink toolchain,\u201d in Evaluation and Assessment in Software\nEngineering, 2021, pp. 260\u2013265.\n[52] J. Hu, Q. Zhang, and H. Yin, \u201cAugmenting greybox fuzzing\nwith generative AI,\u201d CoRR, vol. abs/2306.06782, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2306.06782\n[53] A. Mathur, S. Pradhan, P . Soni, D. Patel, and R. Regunathan,\n\u201cAutomated test case generation using t5 and gpt-3,\u201d in 2023 9th\nInternational Conference on Advanced Computing and Communication\nSystems (ICACCS), vol. 1, 2023, pp. 1986\u20131992.\n[54] D. Zimmermann and A. Koziolek, \u201cAutomating gui-based soft-\nware testing with gpt-3,\u201d in 2023 IEEE International Conference\non Software Testing, Verification and Validation Workshops (ICSTW),\n2023, pp. 62\u201365.\n[55] M. Taeb, A. Swearngin, E. Schoop, R. Cheng, Y. Jiang, and\nJ. Nichols, \u201cAxnav: Replaying accessibility tests from natural\nlanguage,\u201d CoRR, vol. abs/2310.02424, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.02424\n[56] Q. Luu, H. Liu, and T. Y. Chen, \u201cCan chatgpt advance software\ntesting intelligence? an experience report on metamorphic\ntesting,\u201d CoRR, vol. abs/2310.19204, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.19204\n[57] A. Khanfir, R. Degiovanni, M. Papadakis, and Y. L. Traon, \u201cEf-\nficient mutation testing via pre-trained language models,\u201d arXiv\npreprint arXiv:2301.03543, 2023.\n[58] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n\u201cLarge language models are edge-case fuzzers: Testing deep\nlearning libraries via fuzzgpt,\u201d arXiv preprint arXiv:2304.02014 ,\n2023.\n[59] \u2014\u2014, \u201cLarge language models are zero shot fuzzers: Fuzzing\ndeep learning libraries via large language models,\u201d arXiv preprint\narXiv:2209.11515, 2023.\n[60] J. Ackerman and G. Cybenko, \u201cLarge language models for\nfuzzing parsers (registered report),\u201d in Proceedings of the\n2nd International Fuzzing Workshop, FUZZING 2023, Seattle,\nWA, USA, 17 July 2023 , M. B \u00a8ohme, Y. Noller, B. Ray, and\nL. Szekeres, Eds. ACM, 2023, pp. 31\u201338. [Online]. Available:\nhttps://doi.org/10.1145/3605157.3605173\n[61] S. Yu, C. Fang, Y. Ling, C. Wu, and Z. Chen, \u201cLLM for\ntest script generation and migration: Challenges, capabilities,\nand opportunities,\u201d CoRR, vol. abs/2309.13574, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2309.13574\n[62] G. Deng, Y. Liu, V . M. Vilches, P . Liu, Y. Li, Y. Xu,\nT. Zhang, Y. Liu, M. Pinzger, and S. Rass, \u201cPentestgpt:\nAn llm-empowered automatic penetration testing tool,\u201d\nCoRR, vol. abs/2308.06782, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2308.06782\n[63] M. Sun, Y. Yang, Y. Wang, M. Wen, H. Jia, and Y. Zhou,\n\u201cSMT solver validation empowered by large pre-trained\nlanguage models,\u201d in 38th IEEE/ACM International Conference on\nAutomated Software Engineering, ASE 2023, Luxembourg, September\n11-15, 2023 . IEEE, 2023, pp. 1288\u20131300. [Online]. Available:\nhttps://doi.org/10.1109/ASE56229.2023.00180\n[64] Y. Deng, J. Yao, Z. Tu, X. Zheng, M. Zhang, and T. Zhang,\n\u201cTarget: Automated scenario generation from traffic rules\nfor testing autonomous vehicles,\u201d 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:258588387\n[65] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che,\nD. Wang, and Q. Wang, \u201cTesting the limits: Unusual text inputs\ngeneration for mobile app crash detection with large language\nmodel,\u201d CoRR, vol. abs/2310.15657, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.15657\n[66] C. Zhang, M. Bai, Y. Zheng, Y. Li, X. Xie, Y. Li, W. Ma, L. Sun,\nand Y. Liu, \u201cUnderstanding large language model based fuzz\ndriver generation,\u201d CoRR, vol. abs/2307.12469, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2307.12469\n[67] C. Xia, M. Paltenghi, J. Tian, M. Pradel, and L. Zhang,\n\u201cUniversal fuzzing via large language models,\u201d ArXiv,\nvol. abs/2308.04748, 2023. [Online]. Available: https://api.\nsemanticscholar.org/CorpusID:260735598\n[68] C. Tsigkanos, P . Rani, S. M \u00a8uller, and T. Kehrer, \u201cVariable\ndiscovery with large language models for metamorphic testing\nof scientific software,\u201d in Computational Science - ICCS 2023 -\n23rd International Conference, Prague, Czech Republic, July 3-5,\n2023, Proceedings, Part I , ser. Lecture Notes in Computer\nScience, J. Mikyska, C. de Mulatier, M. Paszynski, V . V .\nKrzhizhanovskaya, J. J. Dongarra, and P . M. A. Sloot, Eds.,\nvol. 14073. Springer, 2023, pp. 321\u2013335. [Online]. Available:\nhttps://doi.org/10.1007/978-3-031-35995-8 23\n[69] C. Yang, Y. Deng, R. Lu, J. Yao, J. Liu, R. Jabbarvand, and\nL. Zhang, \u201cWhite-box compiler fuzzing empowered by large\nlanguage models,\u201d CoRR, vol. abs/2310.15991, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2310.15991\n[70] T. Zhang, I. C. Irsan, F. Thung, D. Han, D. Lo, and L. Jiang,\n\u201citiger: an automatic issue title generation tool,\u201d in Proceedings\nof the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering , 2022, pp.\n1637\u20131641.\n[71] Y. Huang, J. Wang, Z. Liu, Y. Wang, S. Wang, C. Chen,\nY. Hu, and Q. Wang, \u201cCrashtranslator: Automatically\nreproducing mobile application crashes directly from stack\ntrace,\u201d CoRR, vol. abs/2310.07128, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.07128\n[72] T. Zhang, I. C. Irsan, F. Thung, and D. Lo, \u201cCupid:\nLeveraging chatgpt for more accurate duplicate bug report\ndetection,\u201d CoRR, vol. abs/2308.10022, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2308.10022\n[73] U. Mukherjee and M. M. Rahman, \u201cEmploying deep\nlearning and structured information retrieval to answer\nclarification questions on bug reports,\u201d 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:259501524\n[74] P . Mahbub, O. Shuvo, and M. M. Rahman, \u201cExplaining software\nbugs leveraging code structures in neural machine translation,\u201d\narXiv preprint arXiv:2212.04584, 2022.\n[75] S. Feng and C. Chen, \u201cPrompting is all your need:\nAutomated android bug replay with large language models,\u201d\nCoRR, vol. abs/2306.01987, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2306.01987\n[76] Y. Su, Z. Han, Z. Gao, Z. Xing, Q. Lu, and X. Xu, \u201cStill\nconfusing for bug-component triaging? deep feature learning\nand ensemble setting to rescue,\u201d in 31st IEEE/ACM International\nConference on Program Comprehension, ICPC 2023, Melbourne,\nAustralia, May 15-16, 2023 . IEEE, 2023, pp. 316\u2013327. [Online].\nAvailable: https://doi.org/10.1109/ICPC58990.2023.00046\n[77] N. D. Bui, Y. Wang, and S. Hoi, \u201cDetect-localize-repair: A unified\nframework for learning to debug with codet5,\u201d arXiv preprint\narXiv:2211.14875, 2022.\n[78] S. Kang, J. Yoon, and S. Yoo, \u201cLarge language models are few-shot\ntesters: Exploring llm-based general bug reproduction,\u201d arXiv\npreprint arXiv:2209.11515, 2022.\n[79] S. Kang, G. An, and S. Yoo, \u201cA preliminary evaluation of\nllm-based fault localization,\u201d CoRR, vol. abs/2308.05487, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2308.05487\n[80] P . Widjojo and C. Treude, \u201cAddressing compiler errors: Stack\noverflow or large language models?\u201d CoRR, vol. abs/2307.10793,\n2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.\n10793\n[81] L. Plein and T. F. Bissyand \u00b4e, \u201cCan llms demystify bug\nreports?\u201d CoRR, vol. abs/2310.06310, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.06310\n[82] A. Taylor, A. Vassar, J. Renzella, and H. A. Pearce, \u201cDcc\n\u2013help: Generating context-aware compiler error explanations\nwith large language models,\u201d 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:261076439\n[83] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, \u201cExplainable automated\ndebugging via large language model-driven scientific debug-\nging,\u201d arXiv preprint arXiv:2304.02195, 2023.",
            "24\n[84] A. Z. H. Yang, R. Martins, C. L. Goues, and V . J.\nHellendoorn, \u201cLarge language models for test-free fault\nlocalization,\u201d CoRR, vol. abs/2310.01726, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2310.01726\n[85] Y. Wu, Z. Li, J. M. Zhang, M. Papadakis, M. Harman,\nand Y. Liu, \u201cLarge language models in fault localisation,\u201d\nCoRR, vol. abs/2308.15276, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2308.15276\n[86] H. Tu, Z. Zhou, H. Jiang, I. N. B. Yusuf, Y. Li, and L. Jiang,\n\u201cLLM4CBI: taming llms to generate effective test programs\nfor compiler bug isolation,\u201d CoRR, vol. abs/2307.00593, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2307.00593\n[87] T.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, S.-C. Cheung,\nand J. Kramer, \u201cNuances are the key: Unlocking chatgpt to\nfind failure-inducing tests with differential prompting,\u201d in 2023\n38th IEEE/ACM International Conference on Automated Software\nEngineering (ASE), 2023, pp. 14\u201326.\n[88] X. Chen, M. Lin, N. Sch \u00a8arli, and D. Zhou, \u201cTeaching large\nlanguage models to self-debug,\u201dCoRR, vol. abs/2304.05128, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2304.05128\n[89] J. Cao, M. Li, M. Wen, and S.-c. Cheung, \u201cA study on prompt\ndesign, advantages and limitations of chatgpt for deep learning\nprogram repair,\u201d arXiv preprint arXiv:2304.08191, 2023.\n[90] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt,\n\u201cExamining zero-shot vulnerability repair with large language\nmodels,\u201d in 2023 IEEE Symposium on Security and Privacy (SP) .\nIEEE Computer Society, 2022, pp. 1\u201318.\n[91] Z. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, \u201cAutomated\nrepair of programs from large language models,\u201d arXiv preprint\narXiv:2205.10583, 2022.\n[92] Y. Hu, X. Shi, Q. Zhou, and L. Pike, \u201cFix bugs with trans-\nformer through a neural-symbolic edit grammar,\u201d arXiv preprint\narXiv:2204.06643, 2022.\n[93] C. S. Xia, Y. Wei, and L. Zhang, \u201cPractical program repair in\nthe era of large pre-trained language models,\u201d arXiv preprint\narXiv:2210.14179, 2022.\n[94] J. Zhang, J. Cambronero, S. Gulwani, V . Le, R. Piskac, G. Soares,\nand G. Verbruggen, \u201cRepairing bugs in python assignments\nusing large language models,\u201d arXiv preprint arXiv:2209.14876 ,\n2022.\n[95] M. Lajk \u00b4o, V . Csuvik, and L. Vid\u00b4acs, \u201cTowards javascript program\nrepair with generative pre-trained transformer (gpt-2),\u201d in Pro-\nceedings of the Third International Workshop on Automated Program\nRepair, 2022, pp. 61\u201368.\n[96] D. Sobania, M. Briesch, C. Hanna, and J. Petke, \u201cAn analysis of\nthe automatic bug fixing performance of chatgpt,\u201d arXiv preprint\narXiv:2301.08653, 2023.\n[97] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li,\nand Y. Zhang, \u201cAn empirical study on fine-tuning large\nlanguage models of code for automated program repair,\u201d\nin 38th IEEE/ACM International Conference on Automated\nSoftware Engineering, ASE 2023, Luxembourg, September 11-\n15, 2023 . IEEE, 2023, pp. 1162\u20131174. [Online]. Available:\nhttps://doi.org/10.1109/ASE56229.2023.00181\n[98] M. C. Wuisang, M. Kurniawan, K. A. Wira Santosa, A. Agung\nSantoso Gunawan, and K. E. Saputra, \u201cAn evaluation of the\neffectiveness of openai\u2019s chatgpt for automated python program\nbug fixing using quixbugs,\u201d in2023 International Seminar on Appli-\ncation for Technology of Information and Communication (iSemantic) ,\n2023, pp. 295\u2013300.\n[99] D. Horv \u00b4ath, V . Csuvik, T. Gyim \u00b4othy, and L. Vid \u00b4acs,\n\u201cAn extensive study on model architecture and program\nrepresentation in the domain of learning-based automated\nprogram repair,\u201d in IEEE/ACM International Workshop on\nAutomated Program Repair, APR@ICSE 2023, Melbourne, Australia,\nMay 16, 2023 . IEEE, 2023, pp. 31\u201338. [Online]. Available:\nhttps://doi.org/10.1109/APR59189.2023.00013\n[100] J. A. Prenner, H. Babii, and R. Robbes, \u201cCan openai\u2019s codex fix\nbugs? an evaluation on quixbugs,\u201d in Proceedings of the Third\nInternational Workshop on Automated Program Repair, 2022, pp. 69\u2013\n75.\n[101] W. Yuan, Q. Zhang, T. He, C. Fang, N. Q. V . Hung, X. Hao, and\nH. Yin, \u201cCircle: continual repair across programming languages,\u201d\nin Proceedings of the 31st ACM SIGSOFT International Symposium\non Software Testing and Analysis, 2022, pp. 678\u2013690.\n[102] S. Moon, Y. Song, H. Chae, D. Kang, T. Kwon, K. T. iunn Ong,\nS. won Hwang, and J. Yeo, \u201cCoffee: Boost your code llms by\nfixing bugs with feedback,\u201d 2023.\n[103] Y. Wei, C. S. Xia, and L. Zhang, \u201cCopiloting the copilots:\nFusing large language models with completion engines for\nautomated program repair,\u201d in Proceedings of the 31st ACM Joint\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, ESEC/FSE 2023, San Francisco,\nCA, USA, December 3-9, 2023 , S. Chandra, K. Blincoe, and\nP . Tonella, Eds. ACM, 2023, pp. 172\u2013184. [Online]. Available:\nhttps://doi.org/10.1145/3611643.3616271\n[104] Y. Peng, S. Gao, C. Gao, Y. Huo, and M. R. Lyu, \u201cDomain\nknowledge matters: Improving prompts with fix templates for\nrepairing python type errors,\u201d CoRR, vol. abs/2306.01394, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2306.01394\n[105] A. E. I. Brownlee, J. Callan, K. Even-Mendoza, A. Geiger,\nC. Hanna, J. Petke, F. Sarro, and D. Sobania, \u201cEnhancing\ngenetic improvement mutations using large language models,\u201d\nin Search-Based Software Engineering - 15th International\nSymposium, SSBSE 2023, San Francisco, CA, USA, December\n8, 2023, Proceedings , ser. Lecture Notes in Computer\nScience, P . Arcaini, T. Yue, and E. M. Fredericks, Eds.,\nvol. 14415. Springer, 2023, pp. 153\u2013159. [Online]. Available:\nhttps://doi.org/10.1007/978-3-031-48796-5 13\n[106] M. M. A. Haque, W. U. Ahmad, I. Lourentzou, and C. Brown,\n\u201cFixeval: Execution-based evaluation of program fixes for\nprogramming problems,\u201d in IEEE/ACM International Workshop on\nAutomated Program Repair, APR@ICSE 2023, Melbourne, Australia,\nMay 16, 2023 . IEEE, 2023, pp. 11\u201318. [Online]. Available:\nhttps://doi.org/10.1109/APR59189.2023.00009\n[107] B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce, \u201cFixing\nhardware security bugs with large language models,\u201d arXiv\npreprint arXiv:2302.01215, 2023.\n[108] P . Deligiannis, A. Lal, N. Mehrotra, and A. Rastogi, \u201cFixing rust\ncompilation errors using llms,\u201d CoRR, vol. abs/2308.05177, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2308.05177\n[109] F. Ribeiro, R. Abreu, and J. Saraiva, \u201cFraming program repair\nas code completion,\u201d in Proceedings of the Third International\nWorkshop on Automated Program Repair, 2022, pp. 38\u201345.\n[110] N. Wadhwa, J. Pradhan, A. Sonwane, S. P . Sahu, N. Natarajan,\nA. Kanade, S. Parthasarathy, and S. K. Rajamani, \u201cFrustrated with\ncode quality issues? llms can help!\u201d CoRR, vol. abs/2309.12938,\n2023. [Online]. Available: https://doi.org/10.48550/arXiv.2309.\n12938\n[111] F. Ribeiro, J. N. C. de Macedo, K. Tsushima, R. Abreu,\nand J. Saraiva, \u201cGpt-3-powered type error debugging:\nInvestigating the use of large language models for code\nrepair,\u201d in Proceedings of the 16th ACM SIGPLAN International\nConference on Software Language Engineering, SLE 2023, Cascais,\nPortugal, October 23-24, 2023 , J. Saraiva, T. Degueule, and\nE. Scott, Eds. ACM, 2023, pp. 111\u2013124. [Online]. Available:\nhttps://doi.org/10.1145/3623476.3623522\n[112] Y. Wu, N. Jiang, H. V . Pham, T. Lutellier, J. Davis, L. Tan,\nP . Babkin, and S. Shah, \u201cHow effective are neural networks for\nfixing security vulnerabilities,\u201d arXiv preprint arXiv:2305.18607 ,\n2023.\n[113] N. Jiang, K. Liu, T. Lutellier, and L. Tan, \u201cImpact of code\nlanguage models on automated program repair,\u201d arXiv preprint\narXiv:2302.05020, 2023.\n[114] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan,\nand A. Svyatkovskiy, \u201cInferfix: End-to-end program repair with\nllms,\u201d arXiv preprint arXiv:2303.07263, 2023.\n[115] C. S. Xia and L. Zhang, \u201cKeep the conversation going: Fixing\n162 out of 337 bugs for $0.42 each using chatgpt,\u201d arXiv preprint\narXiv:2304.00385, 2023.\n[116] Y. Zhang, G. Li, Z. Jin, and Y. Xing, \u201cNeural program repair with\nprogram dependence analysis and effective filter mechanism,\u201d\narXiv preprint arXiv:2305.09315, 2023.\n[117] J. A. Prenner and R. Robbes, \u201cOut of context: How important is\nlocal context in neural program repair?\u201d 2023.\n[118] Q. Zhang, C. Fang, B. Yu, W. Sun, T. Zhang, and Z. Chen,\n\u201cPre-trained model-based automated software vulnerability\nrepair: How far are we?\u201d CoRR, vol. abs/2308.12533, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2308.12533\n[119] S. Garg, R. Z. Moghaddam, and N. Sundaresan, \u201cRapgen:\nAn approach for fixing code inefficiencies in zero-shot,\u201d",
            "25\nCoRR, vol. abs/2306.17077, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2306.17077\n[120] W. Wang, Y. Wang, S. Joty, and S. C. H. Hoi, \u201cRap-\ngen: Retrieval-augmented patch generation with codet5 for\nautomatic program repair,\u201d in Proceedings of the 31st ACM Joint\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, ESEC/FSE 2023, San Francisco,\nCA, USA, December 3-9, 2023 , S. Chandra, K. Blincoe, and\nP . Tonella, Eds. ACM, 2023, pp. 146\u2013158. [Online]. Available:\nhttps://doi.org/10.1145/3611643.3616256\n[121] Y. Zhang, Z. Jin, Y. Xing, and G. Li, \u201cSTEAM: simulating\nthe interactive behavior of programmers for automatic bug\nfixing,\u201d CoRR, vol. abs/2308.14460, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2308.14460\n[122] S. Fakhoury, S. Chakraborty, M. Musuvathi, and S. K. Lahiri,\n\u201cTowards generating functionally correct code edits from natu-\nral language issue descriptions,\u201d arXiv preprint arXiv:2304.03816,\n2023.\n[123] M. Fu, C. Tantithamthavorn, T. Le, V . Nguyen, and D. Phung,\n\u201cVulrepair: a t5-based automated software vulnerability repair,\u201d\nin Proceedings of the 30th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineer-\ning, 2022, pp. 935\u2013947.\n[124] S. Gao, X. Wen, C. Gao, W. Wang, H. Zhang, and\nM. R. Lyu, \u201cWhat makes good in-context demonstrations\nfor code intelligence tasks with llms?\u201d in 38th IEEE/ACM\nInternational Conference on Automated Software Engineering, ASE\n2023, Luxembourg, September 11-15, 2023 . IEEE, 2023, pp. 761\u2013\n773. [Online]. Available: https://doi.org/10.1109/ASE56229.\n2023.00109\n[125] C. Treude and H. Hata, \u201cShe elicits requirements and he\ntests: Software engineering gender bias in large language\nmodels,\u201d CoRR, vol. abs/2303.10131, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2303.10131\n[126] R. Kocielnik, S. Prabhumoye, V . Zhang, R. M. Alvarez, and\nA. Anandkumar, \u201cAutobiastest: Controllable sentence generation\nfor automated and open-ended social bias testing in language\nmodels,\u201d CoRR, vol. abs/2302.07371, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2302.07371\n[127] M. Ciniselli, L. Pascarella, and G. Bavota, \u201cTo what extent do\ndeep learning-based code recommenders generate predictions\nby cloning code from the training set?\u201d in 19th IEEE/ACM\nInternational Conference on Mining Software Repositories, MSR 2022,\nPittsburgh, P A, USA, May 23-24, 2022. ACM, 2022, pp. 167\u2013178.\n[Online]. Available: https://doi.org/10.1145/3524842.3528440\n[128] D. Erhabor, S. Udayashankar, M. Nagappan, and S. Al-Kiswany,\n\u201cMeasuring the runtime performance of code produced with\ngithub copilot,\u201d CoRR, vol. abs/2305.06439, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2305.06439\n[129] R. Wang, R. Cheng, D. Ford, and T. Zimmermann, \u201cInvestigating\nand designing for trust in ai-powered code generation\ntools,\u201d CoRR, vol. abs/2305.11248, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2305.11248\n[130] B. Yetistiren, I. \u00a8Ozsoy, M. Ayerdem, and E. T \u00a8uz \u00a8un, \u201cEvaluating\nthe code quality of ai-assisted code generation tools: An\nempirical study on github copilot, amazon codewhisperer, and\nchatgpt,\u201d CoRR, vol. abs/2304.10778, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2304.10778\n[131] C. Wohlin, \u201cGuidelines for snowballing in systematic literature\nstudies and a replication in software engineering,\u201d in\n18th International Conference on Evaluation and Assessment\nin Software Engineering, EASE \u201914, London, England, United\nKingdom, May 13-14, 2014 , M. J. Shepperd, T. Hall, and\nI. Myrtveit, Eds. ACM, 2014, pp. 38:1\u201338:10. [Online]. Available:\nhttps://doi.org/10.1145/2601248.2601268\n[132] A. Mastropaolo, S. Scalabrino, N. Cooper, D. Nader-Palacio,\nD. Poshyvanyk, R. Oliveto, and G. Bavota, \u201cStudying the usage\nof text-to-text transfer transformer to support code-related tasks,\u201d\nin 43rd IEEE/ACM International Conference on Software Engineering,\nICSE 2021, Madrid, Spain, 22-30 May 2021 . IEEE, 2021, pp. 336\u2013\n347.\n[133] C. Tsigkanos, P . Rani, S. M \u00a8uller, and T. Kehrer, \u201cLarge\nlanguage models: The next frontier for variable discovery\nwithin metamorphic testing?\u201d in IEEE International Conference\non Software Analysis, Evolution and Reengineering, SANER 2023,\nTaipa, Macao, March 21-24, 2023 , T. Zhang, X. Xia, and\nN. Novielli, Eds. IEEE, 2023, pp. 678\u2013682. [Online]. Available:\nhttps://doi.org/10.1109/SANER56733.2023.00070\n[134] G. J. Myers, The art of software testing (2. ed.) . Wiley,\n2004. [Online]. Available: http://eu.wiley.com/WileyCDA/\nWileyTitle/productCd-0471469122.html\n[135] P . Farrell-Vinay,Manage software testing. Auerbach Publ., 2008.\n[136] A. Mili and F. Tchier, Software testing: Concepts and operations .\nJohn Wiley & Sons, 2015.\n[137] S. Lukasczyk and G. Fraser, \u201cPynguin: Automated unit\ntest generation for python,\u201d in 44th IEEE/ACM International\nConference on Software Engineering: Companion Proceedings,\nICSE Companion 2022, Pittsburgh, P A, USA, May 22-24,\n2022. ACM/IEEE, 2022, pp. 168\u2013172. [Online]. Available:\nhttps://doi.org/10.1145/3510454.3516829\n[138] E. T. Barr, M. Harman, P . McMinn, M. Shahbaz, and S. Yoo, \u201cThe\noracle problem in software testing: A survey,\u201d IEEE transactions\non software engineering, vol. 41, no. 5, pp. 507\u2013525, 2014.\n[139] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk,\n\u201cOn learning meaningful assert statements for unit test cases,\u201d\nin ICSE \u201920: 42nd International Conference on Software Engineering,\nSeoul, South Korea, 27 June - 19 July, 2020, G. Rothermel and D. Bae,\nEds. ACM, 2020, pp. 1398\u20131409.\n[140] Y. He, L. Zhang, Z. Yang, Y. Cao, K. Lian, S. Li, W. Yang, Z. Zhang,\nM. Yang, Y. Zhang, and H. Duan, \u201cTextexerciser: Feedback-driven\ntext input exercising for android applications,\u201d in 2020 IEEE\nSymposium on Security and Privacy, SP 2020, San Francisco, CA,\nUSA, May 18-21, 2020. IEEE, 2020, pp. 1071\u20131087.\n[141] A. Wei, Y. Deng, C. Yang, and L. Zhang, \u201cFree lunch for test-\ning: Fuzzing deep-learning libraries from open source,\u201d in 44th\nIEEE/ACM 44th International Conference on Software Engineering,\nICSE 2022, Pittsburgh, P A, USA, May 25-27, 2022 . ACM, 2022,\npp. 995\u20131007.\n[142] D. Xie, Y. Li, M. Kim, H. V . Pham, L. Tan, X. Zhang, and M. W.\nGodfrey, \u201cDocter: documentation-guided fuzzing for testing\ndeep learning API functions,\u201d in ISSTA \u201922: 31st ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis, Virtual\nEvent, South Korea, July 18 - 22, 2022 , S. Ryu and Y. Smaragdakis,\nEds. ACM, 2022, pp. 176\u2013188.\n[143] Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, X. Li, and C. Shen,\n\u201cAudee: Automated testing for deep learning frameworks,\u201d in\n35th IEEE/ACM International Conference on Automated Software\nEngineering, ASE 2020, Melbourne, Australia, September 21-25, 2020.\nIEEE, 2020, pp. 486\u2013498.\n[144] Z. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang, \u201cDeep learning\nlibrary testing via effective model generation,\u201d in ESEC/FSE\n\u201920: 28th ACM Joint European Software Engineering Conference\nand Symposium on the Foundations of Software Engineering, Virtual\nEvent, USA, November 8-13, 2020 , P . Devanbu, M. B. Cohen, and\nT. Zimmermann, Eds. ACM, 2020, pp. 788\u2013799.\n[145] J. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, \u201cShaping\nprogram repair space with existing patches and similar code,\u201d in\nProceedings of the 27th ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis , ser. ISSTA 2018. New York, NY,\nUSA: Association for Computing Machinery, 2018, p. 298\u2013309.\n[Online]. Available: https://doi.org/10.1145/3213846.3213871\n[146] M. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, \u201cContext-\naware patch generation for better automated program repair,\u201d\nin Proceedings of the 40th International Conference on Software\nEngineering, ser. ICSE \u201918. New York, NY, USA: Association\nfor Computing Machinery, 2018, p. 1\u201311. [Online]. Available:\nhttps://doi.org/10.1145/3180155.3180233\n[147] Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and\nL. Zhang, \u201cPrecise condition synthesis for program repair,\u201d in\n2017 IEEE/ACM 39th International Conference on Software Engineer-\ning (ICSE), 2017, pp. 416\u2013426.\n[148] J. Xuan, M. Martinez, F. DeMarco, M. Cl \u00b4ement, S. L. Marcote,\nT. Durieux, D. Le Berre, and M. Monperrus, \u201cNopol: Automatic\nrepair of conditional statement bugs in java programs,\u201d IEEE\nTransactions on Software Engineering, vol. 43, no. 1, pp. 34\u201355, 2017.\n[149] S. Song, X. Li, and S. Li, \u201cHow to bridge the gap between modal-\nities: A comprehensive survey on multimodal large language\nmodel,\u201d CoRR, vol. abs/2311.07594, 2023.\n[150] J. M. Zhang, M. Harman, L. Ma, and Y. Liu, \u201cMachine learning\ntesting: Survey, landscapes and horizons,\u201d IEEE Trans. Software\nEng., vol. 48, no. 2, pp. 1\u201336, 2022.\n[151] F. Tu, J. Zhu, Q. Zheng, and M. Zhou, \u201cBe careful of when:\nan empirical study on time-related misuse of issue tracking",
            "26\ndata,\u201d in Proceedings of the 2018 ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations\nof Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena\nVista, FL, USA, November 04-09, 2018 , G. T. Leavens, A. Garcia,\nand C. S. Pasareanu, Eds. ACM, 2018, pp. 307\u2013318. [Online].\nAvailable: https://doi.org/10.1145/3236024.3236054\n[152] Z. Sun, L. Li, Y. Liu, X. Du, and L. Li, \u201cOn the importance\nof building high-quality training datasets for neural code\nsearch,\u201d in 44th IEEE/ACM 44th International Conference on\nSoftware Engineering, ICSE 2022, Pittsburgh, P A, USA, May\n25-27, 2022 . ACM, 2022, pp. 1609\u20131620. [Online]. Available:\nhttps://doi.org/10.1145/3510003.3510160\n[153] L. Shi, Z. Jiang, Y. Yang, X. Chen, Y. Zhang, F. Mu, H. Jiang, and\nQ. Wang, \u201cISPY: automatic issue-solution pair extraction from\ncommunity live chats,\u201d in 36th IEEE/ACM International Conference\non Automated Software Engineering, ASE 2021, Melbourne, Australia,\nNovember 15-19, 2021 . IEEE, 2021, pp. 142\u2013154. [Online].\nAvailable: https://doi.org/10.1109/ASE51524.2021.9678894\n[154] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu,\nL. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano,\nS. K. Deng, C. B. Clement, D. Drain, N. Sundaresan, J. Yin,\nD. Jiang, and M. Zhou, \u201cGraphcodebert: Pre-training code\nrepresentations with data flow,\u201d in 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=jLoC4ez43PZ\n[155] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and\nJ. Xiao, \u201cLsun: Construction of a large-scale image dataset us-\ning deep learning with humans in the loop,\u201d arXiv preprint\narXiv:1506.03365, 2015.\n[156] LoadRunner, Inc., \u201cLoadrunner,\u201d 2023, microfocus.com.\n[157] LangChain, Inc., \u201cLangchain,\u201d 2023, https://docs.langchain.\ncom/docs/.\n[158] Prompt engineering, \u201cPrompt engineering guide,\u201d 2023, https:\n//github.com/dair-ai/Prompt-Engineering-Guide.\n[159] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola,\n\u201cMultimodal chain-of-thought reasoning in language models,\u201d\nCoRR, vol. abs/2302.00923, 2023.\n[160] Z. Liu, X. Yu, Y. Fang, and X. Zhang, \u201cGraphprompt: Unifying\npre-training and downstream tasks for graph neural networks,\u201d\nin Proceedings of the ACM Web Conference 2023, WWW 2023, Austin,\nTX, USA, 30 April 2023 - 4 May 2023, Y. Ding, J. Tang, J. F. Sequeda,\nL. Aroyo, C. Castillo, and G. Houben, Eds. ACM, 2023, pp. 417\u2013\n428.\n[161] Y. Charalambous, N. Tihanyi, R. Jain, Y. Sun, M. A. Ferrag, and\nL. C. Cordeiro, \u201cA new era in software security: Towards self-\nhealing software via large language models and formal verifica-\ntion,\u201d 2023.\n[162] S. Wang, L. Huang, A. Gao, J. Ge, T. Zhang, H. Feng, I. Satyarth,\nM. Li, H. Zhang, and V . Ng, \u201cMachine/deep learning for\nsoftware engineering: A systematic literature review,\u201d IEEE\nTrans. Software Eng., vol. 49, no. 3, pp. 1188\u20131231, 2023. [Online].\nAvailable: https://doi.org/10.1109/TSE.2022.3173346\n[163] Y. Yang, X. Xia, D. Lo, and J. C. Grundy, \u201cA survey on\ndeep learning for software engineering,\u201d ACM Comput. Surv. ,\nvol. 54, no. 10s, pp. 206:1\u2013206:73, 2022. [Online]. Available:\nhttps://doi.org/10.1145/3505243\n[164] C. Watson, N. Cooper, D. Nader-Palacio, K. Moran, and\nD. Poshyvanyk, \u201cA systematic literature review on the use of\ndeep learning in software engineering research,\u201d ACM Trans.\nSoftw. Eng. Methodol., vol. 31, no. 2, pp. 32:1\u201332:58, 2022. [Online].\nAvailable: https://doi.org/10.1145/3485275\n[165] M. Bajammal, A. Stocco, D. Mazinanian, and A. Mesbah,\n\u201cA survey on the use of computer vision to improve\nsoftware engineering tasks,\u201d IEEE Trans. Software Eng. ,\nvol. 48, no. 5, pp. 1722\u20131742, 2022. [Online]. Available:\nhttps://doi.org/10.1109/TSE.2020.3032986\n[166] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo,\nD. Lo, J. C. Grundy, and H. Wang, \u201cLarge language\nmodels for software engineering: A systematic literature\nreview,\u201d CoRR, vol. abs/2308.10620, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2308.10620\n[167] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta,\nS. Yoo, and J. M. Zhang, \u201cLarge language models\nfor software engineering: Survey and open problems,\u201d\nCoRR, vol. abs/2310.03533, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2310.03533\n[168] D. Zan, B. Chen, F. Zhang, D. Lu, B. Wu, B. Guan,\nY. Wang, and J. Lou, \u201cLarge language models meet nl2code:\nA survey,\u201d in Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), ACL 2023, Toronto, Canada, July 9-14, 2023 , A. Rogers,\nJ. L. Boyd-Graber, and N. Okazaki, Eds. Association for\nComputational Linguistics, 2023, pp. 7443\u20137464. [Online].\nAvailable: https://doi.org/10.18653/v1/2023.acl-long.411",
            "27\nTABLE 5: All details of the collected papers\nID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-\ngrated\nVenue Ref\n1 Unit Test Case Generation with Transform-\ners and Focal Context\n2021 Unit test case gener-\nation\nBART Pre-training and/or\nFine-tuning\nCode Pure LLM Arxiv\n[26]\n2 Codet: Code Generation with Generated\nTests\n2022 Unit test case gener-\nation\nCodex Zero-shot learning Code Pure LLM ICLR 2023\n[27]\n3 Interactive Code Generation via Test-Driven\nUser-Intent Formalization\n2022 Unit test case gener-\nation\nCodex Zero-shot learning Code Mutation testing;\nStatistic analysis\nArxiv\n[28]\n4 A3Test: Assertion-Augmented Automated\nTest Case Generation\n2023 Unit test case gener-\nation\nPLBART Pre-training and/or\nFine-tuning\nCode Syntactic repair Arxiv\n[29]\n5 An Empirical Evaluation of Using Large\nLanguage Models for Automated Unit Test\nGeneration\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code; Others Syntactic repair Arxiv\n[30]\n6 An Initial Investigation of ChatGPT Unit\nTest Generation Capability\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code Pure LLM SAST 2023\n[31]\n7 Automated Test Case Generation Using\nCode Models and Domain Adaptation\n2023 Unit test case gener-\nation\nCodeT5; LLaMA-2 Pre-training and/or\nFine-tuning\nCode Syntactic repair Arxiv\n[32]\n8 Automatic Generation of Test Cases based\non Bug Reports: a Feasibility Study with\nLarge Language Models\n2023 Unit test case gener-\nation\nCodeGPT; ChatGPT Pre-training and/or\nFine-tuning\nBug description Pure LLM Arxiv\n[33]\n9 Can Large Language Models Write Good\nProperty-Based Tests?\n2023 Unit test case gener-\nation\nGPT-4 Zero-shot learning Code; Others Pure LLM Arxiv\n[34]\n10 CAT-LM Training Language Models on\nAligned Code And Tests\n2023 Unit test case gener-\nation\nGPT-neox Pre-training and/or\nFine-tuning\nCode Pure LLM ASE 2023\n[35]\n11 ChatGPT vs SBST: A Comparative Assess-\nment of Unit Test Suite Generation\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code Pure LLM Arxiv [8]\n12 ChatUniTest: a ChatGPT-based Automated\nUnit Test Generation Tool\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code Syntactic repair Arxiv\n[36]\n13 CODAMOSA: Escaping Coverage Plateaus\nin Test Generation with Pre-trained Large\nLanguage Models\n2023 Unit test case gener-\nation\nCodex Zero-shot learning Code Mutation testing;\nProgram analysis\nICSE 2023\n[37]\n14 Effective Test Generation Using Pre-trained\nLarge Language Models and Mutation Test-\ning\n2023 Unit test case gener-\nation\nCodex Few-shot learning;\nZero-shot learning\nCode Mutation testing;\nSyntactic repair\nArxiv\n[38]\n15 Exploring the Effectiveness of Large Lan-\nguage Models in Generating Unit Tests\n2023 Unit test case gener-\nation\nCodeGen; Codex;\nChatGPT\nZero-shot learning Code Syntactic repair Arxiv\n[39]\n16 How Well does LLM Generate Security\nTests?\n2023 Unit test case gener-\nation\nChatGPT Few-shot learning Code Pure LLM Arxiv\n[40]\n17 No More Manual Tests? Evaluating and Im-\nproving ChatGPT for Unit Test Generation\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code; Error in-\nformation\nProgram analysis Arxiv [7]\n18 Prompting Code Interpreter to Write Better\nUnit Tests on Quixbugs Functions\n2023 Unit test case gener-\nation\nGPT-4 Few-shot learning Code Pure LLM Arxiv\n[41]\n19 Reinforcement Learning from Automatic\nFeedback for High-Quality Unit Test Gen-\neration\n2023 Unit test case gener-\nation\nCodex Pre-training and/or\nFine-tuning\nCode Program analysis,\nReinforcement\nlearning\nArxiv\n[42]\n20 Unit Test Generation using Generative AI: A\nComparative Performance Analysis of Au-\ntogeneration Tools\n2023 Unit test case gener-\nation\nChatGPT Zero-shot learning Code Pure LLM Arxiv\n[43]\n21 Generating Accurate Assert Statements for\nUnit Test Cases Using Pretrained Trans-\nformers\n2023 Test oracle genera-\ntion\nBART Pre-training and/or\nFine-tuning\nCode Pure LLM AST 2022\n[44]\n22 Learning Deep Semantics for Test Comple-\ntion\n2023 Test oracle genera-\ntion\nCodeT5 Pre-training and/or\nFine-tuning\nCode Statistic analysis ICSE 2023\n[45]\n23 Using Transfer Learning for Code-Related\nTasks\n2022 Test oracle gener-\nation; Program re-\npair\nT5 Pre-training and/or\nFine-tuning\nCode Pure LLM TSE 2022\n[46]\n24 Retrieval-Based Prompt Selection for Code-\nRelated Few-Shot Learning\n2023 Test oracle gener-\nation; Program re-\npair\nCodex Few-shot learning Code Pure LLM ICSE 2023\n[47]",
            "28\nID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-\ngrated\nVenue Ref\n25 Automated Conformance Testing for\nJavaScript Engines via Deep Compiler\nFuzzing\n2021 System test input\ngeneration\nGPT-2 Pre-training and/or\nFine-tuning\nCode Differential testing;\nProgram analysis\nPLDI 2021\n[48]\n26 Fill in the Blank: Context-aware Automated\nText Input Generation for Mobile GUI Test-\ning\n2022 System test input\ngeneration\nGPT-3 Pre-training and/or\nFine-tuning\nView hierarchy\nfile of UI\nPure LLM ICSE 2023\n[49]\n27 Large Language Models are Pretty Good\nZero-Shot Video Game Bug Detectors\n2022 System test input\ngeneration\nInstructGPT Chain-of-Thought;\nZero-shot learning\nOthers Pure LLM Arxiv\n[50]\n28 Slgpt: Using Transfer Learning to Directly\nGenerate Simulink Model Files and Find\nBugs in the Simulink Toolchain\n2022 System test input\ngeneration\nGPT-2 Pre-training and/or\nFine-tuning\nOthers Formal method EASE 2021\n[51]\n29 Augmenting Greybox Fuzzing with Gener-\native AI\n2023 System test input\ngeneration\nChatGPT Few-shot learning Code Pure LLM Arxiv\n[52]\n30 Automated Test Case Generation Using T5\nand GPT-3\n2023 System test input\ngeneration\nGPT-3; T5 Pre-training and/or\nFine-tuning; Zero-shot\nlearning\nNL specifica-\ntion\nPure LLM ICACCS\n2023 [53]\n31 Automating GUI-based Software Testing\nwith GPT-3\n2023 System test input\ngeneration\nGPT-3 Pre-training and/or\nFine-tuning\nView hierarchy\nfile of UI\nPure LLM ICSTW\n2023 [54]\n32 AXNav: Replaying Accessibility Tests from\nNatural Language\n2023 System test input\ngeneration\nGPT-4 Chain-of-Thought View hierarchy\nfile of UI\nPure LLM Arxiv\n[55]\n33 Can ChatGPT Advance Software Testing In-\ntelligence? An Experience Report on Meta-\nmorphic Testing\n2023 System test input\ngeneration\nChatGPT Zero-shot learning Others Pure LLM Arxiv\n[56]\n34 Efficient Mutation Testing via Pre-Trained\nLanguage Models\n2023 System test input\ngeneration\nCodeBert Zero-shot learning Code Mutation testing Arxiv\n[57]\n35 Large Language Models are Edge-Case\nGenerators:Crafting Unusual Programs for\nFuzzing Deep Learning Libraries\n2023 System test input\ngeneration\nCodex Chain-of-Thought; Pre-\ntraining and/or Fine-\ntuning; Zero-shot learn-\ning; Few-shot learning\nCode Differential testing ICSE 2024\n[58]\n36 Large Language Models are Zero Shot\nFuzzers: Fuzzing Deep Learning Libraries\nvia Large Language Models\n2023 System test input\ngeneration\nCodex; InCoder Zero-shot learning Code Mutation testing;\nDifferential testing\nISSTA 2023\n[59]\n37 Large Language Models for Fuzzing Parsers\n(Registered Report)\n2023 System test input\ngeneration\nGPT-4 Few-shot learning NL specifica-\ntion\nPure LLM FUZZING\n2023 [60]\n38 LLM for Test Script Generation and Migra-\ntion: Challenges, Capabilities, and Opportu-\nnities\n2023 System test input\ngeneration\nChatGPT Zero-shot learning View hierarchy\nfile of UI\nPure LLM Arxiv\n[61]\n39 Make LLM a Testing Expert: Bringing\nHuman-like Interaction to Mobile GUI Test-\ning via Functionality-aware Decisions\n2023 System test input\ngeneration\nGPT-3 Zero-shot learning View hierarchy\nfile of UI\nNatural language\nprocessing\nICSE 2024\n[14]\n40 PentestGPT: An LLM-empowered Auto-\nmatic Penetration Testing Tool\n2023 System test input\ngeneration\nChatGPT; GPT-4;\nLaMDA\nChain-of-Thought;\nFew-shot learning\nNL specifica-\ntion\nPure LLM Arxiv\n[62]\n41 SMT Solver Validation Empowered by\nLarge Pre-Trained Language Models\n2023 System test input\ngeneration\nGPT-2 Pre-training and/or\nFine-tuning\nCode Differential testing ASE 2023\n[63]\n42 TARGET: Automated Scenario Generation\nfrom Traffic Rules for Testing Autonomous\nVehicles\n2023 System test input\ngeneration\nGPT-3 Zero-shot learning Others Scenario testing Arxiv\n[64]\n43 Testing the Limits: Unusual Text Inputs\nGeneration for Mobile App Crash Detection\nwith Large Language Model\n2023 System test input\ngeneration\nChatGPT Few-shot learning View hierarchy\nfile of UI\nPure LLM ICSE 2024\n[65]\n44 Understanding Large Language Model\nBased Fuzz Driver Generation\n2023 System test input\ngeneration\nChatGPT; GPT-4 Few-shot learning;\nZero-shot learning\nCode; Others Pure LLM Arxiv\n[66]\n45 Universal Fuzzing via Large Language\nModels\n2023 System test input\ngeneration\nGPT-4; StarCoder Few-shot learning; Au-\ntomatic prompt\nCode Mutation testing ICSE 2024\n[67]\n46 Variable Discovery with Large Language\nModels for Metamorphic Testing of Scien-\ntific Software\n2023 System test input\ngeneration\nGPT-j Zero-shot learning Others Pure LLM SANER\n2023 [68]",
            "29\nID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-\ngrated\nVenue Ref\n47 White-box Compiler Fuzzing Empowered\nby Large Language Models\n2023 System test input\ngeneration\nGPT-4; StarCoder Few-shot learning Code Pure LLM Arxiv\n[69]\n48 Itiger: an Automatic Issue Title Generation\nTool\n2022 Bug analysis BART Pre-training and/or\nFine-tuning\nBug description Pure LLM FSE 2022\n[70]\n49 CrashTranslator: Automatically Reproduc-\ning Mobile Application Crashes Directly\nfrom Stack Trace\n2023 Bug analysis ChatGPT Pre-training and/or\nFine-tuning\nBug description Reinforcement\nlearning\nICSE 2024\n[71]\n50 Cupid: Leveraging ChatGPT for More Ac-\ncurate Duplicate Bug Report Detection\n2023 Bug analysis ChatGPT Zero-shot learning Bug description Statistic analysis Arxiv\n[72]\n51 Employing Deep Learning and Structured\nInformation Retrieval to Answer Clarifica-\ntion Questions on Bug Reports\n2023 Bug analysis CodeT5 Zero-shot learning Bug description Statistic analysis Arxiv\n[73]\n52 Explaining Software Bugs Leveraging Code\nStructures in Neural Machine Translation\n2023 Bug analysis CodeT5 Pre-training and/or\nFine-tuning\nCode Program analysis ICSE 2023\n[74]\n53 Prompting Is All Your Need: Automated\nAndroid Bug Replay with Large Language\nModels\n2023 Bug analysis ChatGPT Few-shot learning;\nChain-of-Thought\nBug description Pure LLM ICSE 2024\n[75]\n54 Still Confusing for Bug-Component Triag-\ning? Deep Feature Learning and Ensemble\nSetting to Rescue\n2023 Bug analysis CodeT5 Pre-training and/or\nFine-tuning\nBug description Statistic analysis ICPC 2023\n[76]\n55 Detect-Localize-Repair: A Unified Frame-\nwork for Learning to Debug with CodeT5\n2022 Debug CodeT5 Pre-training and/or\nFine-tuning\nCode Pure LLM EMNLP\n2022 [77]\n56 Large Language Models are Few-shot\nTesters: Exploring LLM-based General Bug\nReproduction\n2022 Debug Codex Few-shot learning Bug description Program analysis;\nStatistic analysis\nICSE 2023\n[78]\n57 A Preliminary Evaluation of LLM-Based\nFault Localization\n2023 Debug ChatGPT Few-shot learning Code Pure LLM Arxiv\n[79]\n58 Addressing Compiler Errors: Stack Over-\nflow or Large Language Models?\n2023 Debug ChatGPT; GPT-4 Zero-shot learning Error informa-\ntion\nPure LLM Arxiv\n[80]\n59 Can LLMs Demystify Bug Reports? 2023 Debug ChatGPT Zero-shot learning Bug description Pure LLM Arxiv\n[81]\n60 Dcc \u2013help: Generating Context-Aware Com-\npiler Error Explanations with Large Lan-\nguage Models\n2023 Debug ChatGPT Zero-shot learning Code; Error in-\nformation\nPure LLM SIGCSE\n2024 [82]\n61 Explainable Automated Debugging via\nLarge Language Model-driven Scientific De-\nbugging\n2023 Debug CodeGen; Codex;\nChatGPT\nSelf-consistency; Zero-\nshot learning\nCode Pure LLM Arxiv\n[83]\n62 Large Language Models for Test-Free Fault\nLocalization\n2023 Debug CodeGen Pre-training and/or\nFine-tuning\nCode Pure LLM ICSE 2024\n[84]\n63 Large Language Models in Fault Localisa-\ntion\n2023 Debug ChatGPT; GPT-4 Zero-shot learning Code; Error in-\nformation\nPure LLM Arxiv\n[85]\n64 LLM4CBI: Taming LLMs to Generate Effec-\ntive Test Programs for Compiler Bug Isola-\ntion\n2023 Debug ChatGPT Zero-shot learning Code Mutation testing;\nReinforcement\nlearning\nArxiv\n[86]\n65 Nuances are the Key: Unlocking ChatGPT\nto Find Failure-Inducing Tests with Differ-\nential Prompting\n2023 Debug ChatGPT Zero-shot learning Code Differential testing ASE 2023\n[87]\n66 Teaching Large Language Models to Self-\nDebug\n2023 Debug Codex; ChatGPT;\nGPT-4; StarCoder\nFew-shot learning Code Pure LLM Arxiv\n[88]\n67 A study on Prompt Design, Advantages and\nLimitations of ChatGPT for Deep Learning\nProgram Repair\n2023 Debug; Program re-\npair\nChatGPT Zero-shot learning Code Pure LLM Arxiv\n[89]\n68 Examining Zero-Shot Vulnerability Repair\nwith Large Language Models\n2021 Program repair Codex Zero-shot learning Code; Bug de-\nscription\nPure LLM SP 2023\n[90]\n69 Automated Repair of Programs from Large\nLanguage Models\n2022 Program repair Codex Zero-shot learning Code Pure LLM ICSE 2023\n[91]\n70 Fix Bugs with Transformer through a\nNeural-Symbolic Edit Grammar\n2022 Program repair CodeGPT Pre-training and/or\nFine-tuning\nCode Pure LLM Arxiv\n[92]\n71 Practical Program Repair in the Era of Large\nPre-trained Language Models\n2022 Program repair GPT-3; Codex;\nCodeT5; InCoder\nFew-shot learning;\nZero-shot learning\nCode Statistic analysis ICSE 2023\n[93]",
            "30\nID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-\ngrated\nVenue Ref\n72 Repairing Bugs in Python Assignments Us-\ning Large Language Models\n2022 Program repair Codex Few-shot learning;\nZero-shot learning\nCode; Error in-\nformation\nProgram analysis Arxiv\n[94]\n73 Towards JavaScript Program Repair with\nGenerative Pre-trained Transformer (GPT-2)\n2022 Program repair GPT-2 Pre-training and/or\nFine-tuning\nCode Pure LLM APR 2022\n[95]\n74 An Analysis of the Automatic Bug Fixing\nPerformance of ChatGPT\n2023 Program repair ChatGPT Zero-shot learning Code; Error in-\nformation\nPure LLM APR 2023\n[96]\n75 An Empirical Study on Fine-Tuning Large\nLanguage Models of Code for Automated\nProgram Repair\n2023 Program repair PLBART; CodeT5;\nUniXCoder\nPre-training and/or\nFine-tuning\nCode Pure LLM ASE 2023\n[97]\n76 An Evaluation of the Effectiveness of Ope-\nnAI\u2019s ChatGPT for Automated Python Pro-\ngram Bug Fixing using QuixBugs\n2023 Program repair ChatGPT Zero-shot learning Code Pure LLM iSemantic\n2023 [98]\n77 An Extensive Study on Model Architecture\nand Program Representation in the Domain\nof Learning-based Automated Program Re-\npair\n2023 Program repair T5; CodeT5 Pre-training and/or\nFine-tuning\nCode Pure LLM APR 2023\n[99]\n78 Can OpenAI\u2019s Codex Fix Bugs? An Evalua-\ntion on QuixBugs\n2023 Program repair Codex Few-shot learning;\nZero-shot learning\nCode Pure LLM APR 2022\n[100]\n79 CIRCLE: Continual Repair Across Program-\nming Languages\n2023 Program repair T5 Pre-training and/or\nFine-tuning\nCode Pure LLM ISSTA 2022\n[101]\n80 Coffee: Boost Your Code LLMs by Fixing\nBugs with Feedback\n2023 Program repair CodeLLAMA Pre-training and/or\nFine-tuning\nCode Pure LLM Arxiv\n[102]\n81 Copiloting the Copilots: Fusing Large Lan-\nguage Models with Completion Engines for\nAutomated Program Repair\n2023 Program repair CodeT5; InCoder Zero-shot learning Code Statistic analysis FSE 2023\n[103]\n82 Domain Knowledge Matters: Improving\nPrompts with Fix Templates for Repairing\nPython Type Errors\n2023 Program repair CodeT5 Pre-training and/or\nFine-tuning\nCode Program analysis ICSE 2024\n[104]\n83 Enhancing Genetic Improvement Mutations\nUsing Large Language Models\n2023 Program repair GPT-4 Zero-shot learning Code Pure LLM SSBSE 2023\n[105]\n84 FixEval: Execution-based Evaluation of Pro-\ngram Fixes for Programming Problems\n2023 Program repair CodeT5; PLBART Pre-training and/or\nFine-tuning\nCode Pure LLM APR 2023\n[106]\n85 Fixing Hardware Security Bugs with Large\nLanguage Models\n2023 Program repair Codex; CodeGen Few-shot learning;\nZero-shot learning\nCode; Bug de-\nscription\nPure LLM Arxiv\n[107]\n86 Fixing Rust Compilation Errors using LLMs 2023 Program repair ChatGPT; GPT-4 Zero-shot learning Code Pure LLM Arxiv\n[108]\n87 Framing Program Repair as Code Comple-\ntion\n2023 Program repair CodeGPT Zero-shot learning Code Pure LLM ICSE 2022\n[109]\n88 Frustrated with Code Quality Issues? LLMs\ncan Help!\n2023 Program repair ChatGPT; GPT-4 Zero-shot learning Code Pure LLM Arxiv\n[110]\n89 GPT-3-Powered Type Error Debugging: In-\nvestigating the Use of Large Language Mod-\nels for Code Repair\n2023 Program repair GPT-3 Zero-shot learning Code Program analysis SLE 2023\n[111]\n90 How Effective Are Neural Networks for Fix-\ning Security Vulnerabilities\n2023 Program repair Codex; CodeGen;\nCodeT5; PLBART;\nInCoder\nPre-training and/or\nFine-tuning; Zero-shot\nlearning\nCode Pure LLM ISSTA 2023\n[112]\n91 Impact of Code Language Models on Auto-\nmated Program Repair\n2023 Program repair PLBART; CodeT5;\nCodeGen; InCoder\nPre-training and/or\nFine-tuning; Zero-shot\nlearning\nCode Pure LLM ICSE 2023\n[113]\n92 Inferfix: End-to-end Program Repair with\nLLMs\n2023 Program repair Codex Few-shot learning; Pre-\ntraining and/or Fine-\ntuning\nCode Pure LLM FSE 2023\n[114]\n93 Keep the Conversation Going: Fixing 162\nout of 337 bugs for $0.42 each using Chat-\nGPT\n2023 Program repair ChatGPT Few-shot learning Code; Error in-\nformation\nPure LLM Arxiv\n[115]\n94 Neural Program Repair with Program De-\npendence Analysis and Effective Filter\nMechanism\n2023 Program repair CodeT5 Pre-training and/or\nFine-tuning\nCode Statistic analysis Arxiv\n[116]",
            "31\nID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-\ngrated\nVenue Ref\n95 Out of Context: How important is Local\nContext in Neural Program Repair?\n2023 Program repair CodeT5 Pre-training and/or\nFine-tuning\nCode Pure LLM ICSE 2024\n[117]\n96 Pre-trained Model-based Automated Soft-\nware Vulnerability Repair: How Far are We?\n2023 Program repair CodeT5; UniX-\nCoder; CodeGPT\nPre-training and/or\nFine-tuning\nCode Pure LLM IEEE TDSC\n[118]\n97 RAPGen: An Approach for Fixing Code In-\nefficiencies in Zero-Shot\n2023 Program repair Codex Few-shot learning;\nChain-of-Thought\nCode Pure LLM Arxiv\n[119]\n98 RAP-Gen: Retrieval-Augmented Patch Gen-\neration with CodeT5 for Automatic Pro-\ngram Repair\n2023 Program repair CodeT5 Pre-training and/or\nFine-tuning\nCode Statistic analysis FSE 2023\n[120]\n99 STEAM: Simulating the InTeractive BEhav-\nior of ProgrAMmers for Automatic Bug Fix-\ning\n2023 Program repair ChatGPT Zero-shot learning Code Pure LLM Arxiv\n[121]\n100 Towards Generating Functionally Correct\nCode Edits from Natural Language Issue\nDescriptions\n2023 Program repair Codex; ChatGPT Few-shot learning;\nZero-shot learning;\nChain-of-Thought\nCode; Bug de-\nscription\nPure LLM Arxiv\n[122]\n101 VulRepair: a T5-based Automated Software\nVulnerability Repair\n2023 Program repair T5 Pre-training and/or\nFine-tuning\nCode Pure LLM FSE 2022\n[123]\n102 What Makes Good In-Context Demonstra-\ntions for Code Intelligence Tasks with\nLLMs?\n2023 Program repair Codex; ChatGPT Few-shot learning Code Pure LLM ASE 2023\n[124]"
        ]
    }
}