{
    "contents": {
        "0": [
            "1Software Testing with Large Language Models:Survey, Landscape, and VisionJunjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing WangAbstractPre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural languageprocessing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a widerange of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliabilityof software products. As the scope and complexity of software systems continue to grow, the need for more effective software testingtechniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper providesa comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for softwaretesting, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks forwhich LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzesthe commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs.It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future researchin this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs insoftware testing.Index TermsPre-trained Large Language Model, Software Testing, LLM, GPT1 I NTRODUCTIONSoftware testing is a crucial undertaking that serves asa cornerstone for ensuring the quality and reliability ofsoftware products. Without the rigorous process of softwaretesting, software enterprises would be reluctant to releasetheir products into the market, knowing the potentialconsequences of delivering flawed software to end-users.By conducting thorough and meticulous testing procedures,software enterprises can minimize the occurrence of criticalsoftware failures, usability issues, or security breachesthat could potentially lead to financial losses or jeopardizeuser trust. Additionally, software testing helps to reducemaintenance costs by identifying and resolving issues earlyin the development lifecycle, preventing more significantcomplications down the line [1], [2].The significance of software testing has garnered sub-stantial attention within the research and industrial com-munities. In the field of software engineering, it stands asan immensely popular and vibrant research area. One canobserve the undeniable prominence of software testing bysimply examining the landscape of conferences and sym-posiums focused on software engineering. Amongst theseevents, topics related to software testing consistently domi-nate the submission numbers and are frequently selected forpublication. J. Wang,Y. Huang, Z. Liu, Q. Wang are with State Key Laboratory ofIntelligent Game, Institute of Software Chinese Academy of Sciences, andUniversity of Chinese Academy of Sciences, Beijing, China. J. Wang andQ. Wang are corresponding authors.E-mail: {junjie, yuchao2019, liuzhe2020, wq}@iscas.ac.cn C. Chen is with Monash University, Melbourne, AustraliaE-mail: chunyang.chen@monash.edu S. Wang is with York University, Toronto, Canada.E-mail: wangsong@yorku.caWhile the field of software testing has gained signifi-cant popularity, there remain dozens of challenges that havenot been effectively addressed. For example, one such chal-lenge is automated unit test case generation. Although var-ious approaches, including search-based [3], [4], constraint-based [5] or random-based [6] techniques to generate a suiteof unit tests, the coverage and the meaningfulness of thegenerated tests are still far from satisfactory [7], [8]. Simi-larly, when it comes to mobile GUI testing, existing studieswith random-/rule-based methods [9], [10], model-basedmethods [11], [12], and learning-based methods [13] are un-able to understand the semantic information of the GUIpage and often fall short in achieving comprehensive cov-erage [14], [15]. Considering these limitations, numerous re-search efforts are currently underway to explore innovativetechniques that can enhance the efficacy of software testingtasks, among which large language models are the mostpromising ones.Large language models (LLMs) such as T5 and GPT-3have revolutionized the field of natural language processing(NLP) and artificial intelligence (AI). These models, initiallypre-trained on extensive corpora, have exhibited remarkableperformance across a wide range of NLP tasks includingquestion-answering, machine translation, and text genera-tion [16][19]. In recent years, there has been a significantadvancement in LLMs with the emergence of models capa-ble of handling even larger-scale datasets. This expansionin model size has not only led to improved performancebut also opened up new possibilities for applying LLMsas Artificial General Intelligence. Among these advancedLLMs, models like ChatGPT 1 and LLaMA 2 boast billions1. https://openai.com/blog/chatgpt2. https://ai.meta.com/blog/large-language-model-llama-meta-ai/arXiv:2307.07221v3  [cs.SE]  4 Mar 2024",
            "2of parameters. Such models hold tremendous potential fortackling complex practical tasks in domains like code gener-ation and artistic creation. With their expanded capacity andenhanced capabilities, LLMs have become game-changers inNLP and AI, and are driving advancements in other fieldslike coding and software testing.LLMs have been used for various coding-related tasksincluding code generation and code recommendation [20][23]. On one hand, in software testing, there are many tasksrelated to code generation, such as unit test generation [7],where the utilization of LLMs is expected to yield goodperformance. On the other hand, software testing possessesunique characteristics that differentiate it from code gener-ation. For example, code generation primarily focuses onproducing a single, correct code snippet, whereas softwaretesting often requires generating diverse test inputs to en-sure better coverage of the software under test [1]. The ex-istence of these differences introduces new challenges andopportunities when employing LLMs for software testing.Moreover, people have benefited from the excellent perfor-mance of LLMs in generation and inference tasks, leadingto the emergence of dozens of new practices that use LLMsfor software testing.This article presents a comprehensive review of the uti-lization of LLMs in software testing. We collect 102 relevantpapers and conduct a thorough analysis from both softwaretesting and LLMs perspectives, as roughly summarized inFigure 1.From the viewpoint of software testing, our analysis in-volves an examination of the specific software testing tasksfor which LLMs are employed. Results show that LLMs arecommonly used for test case preparation (including unit testcase generation, test oracle generation, and system test inputgeneration), program debugging, and bug repair, while wedo not find the practices for applying LLMs in the tasks ofearly testing life-cycle (such as test requirement, test plan,etc). For each test task, we would provide detailed illustra-tions showcasing the utilization of LLMs in addressing thetask, highlighting commonly-used practices, tracking tech-nology evolution trends, and summarizing achieved per-formance, so as to facilitate readers in gaining a thoroughoverview of how LLMs are employed across various testingtasks.From the viewpoint of LLMs, our analysis includesthe commonly used LLMs in these studies, the types ofprompt engineering, the input of the LLMs, as well asthe accompanied techniques with these LLMs. Resultsshow that about one-third of the studies utilize the LLMsthrough pre-training or fine-tuning schema, while the othersemploy prompt engineering to communicate with LLMsto steer their behavior for desired outcomes. For promptengineering, the zero-shot learning and few-shot learningstrategies are most commonly used, while other advanceslike chain-of-thought promoting and self-consistency arerarely utilized. Results also show that traditional testingtechniques like differential testing and mutation testingare usually accompanied by LLMs to help generate morediversified tests.Furthermore, we summarize the key challenges and po-tential opportunities in this direction. Although softwaretesting with LLMs has undergone significant growth in theFig. 1: Structure of the contents in this paper (the numbersin bracket indicates the number of involved papers, and apaper might involve zero or multiple items)past two years, there are still challenges in achieving highcoverage of the testing, test oracle problem, rigorous evalu-ations, and real-world application of LLMs in software test-ing. Since it is a new emerging field, there are many researchopportunities, including exploring LLMs in an early stage oftesting, exploring LLMs for more types of software and non-functional testing, exploring advanced prompt engineering,as well as incorporating LLMs with traditional techniques.This paper makes the following contributions: We thoroughly analyze 102 relevant studies that usedLLMs for software testing, regarding publicationtrends, distribution of publication venues, etc. We conduct a comprehensive analysis from the perspec-tive of software testing to understand the distribution ofsoftware testing tasks with LLM and present a thoroughdiscussion about how these tasks are solved with LLM. We conduct a comprehensive analysis from the perspec-tive of LLMs, and uncover the commonly-used LLMs,the types of prompt engineering, input of the LLMs, aswell as the accompanied techniques with these LLMs. We highlight the challenges in existing studies andpresent potential opportunities for further studies.We believe that this work will be valuable to both re-searchers and practitioners in the field of software engineer-ing, as it provides a comprehensive overview of the currentstate and future vision of using LLMs for software testing.For researchers, this work can serve as a roadmap for futureresearch in this area, highlighting potential avenues for ex-ploration and identifying gaps in our current understandingof the use of LLMs in software testing. For practitioners, thiswork can provide insights into the potential benefits andlimitations of using LLMs for software testing, as well aspractical guidance on how to effectively integrate them into",
            "![](./images/llmpaper/image_1.png)",
            "3existing testing processes. By providing a detailed landscapeof the current state and future vision of using LLMs forsoftware testing, this work can help accelerate the adoptionof this technology in the software engineering communityand ultimately contribute to improving the quality and reli-ability of software systems.2 B ACKGROUND2.1 Large Language Model (LLM)Recently, pre-trained language models (PLMs) have beenproposed by pretraining Transformer-based models overlarge-scale corpora, showing strong capabilities in solvingvarious natural language processing (NLP) tasks [16][19].Studies have shown that model scaling can lead to improvedmodel capacity, prompting researchers to investigate thescaling effect through further parameter size increases.Interestingly, when the parameter scale exceeds a certainthreshold, these larger language models demonstrate notonly significant performance improvements but also specialabilities such as in-context learning, which are absent insmaller models such as BERT.To discriminate the language models in differentparameter scales, the research community has coinedthe term large language models (LLM) for the PLMs ofsignificant size. LLMs typically refer to language modelsthat have hundreds of billions (or more) of parameters andare trained on massive text data such as GPT-3, PaLM,Codex, and LLaMA. LLMs are built using the Transformerarchitecture, which stacks multi-head attention layersin a very deep neural network. Existing LLMs adoptsimilar model architectures (Transformer) and pre-trainingobjectives (language modeling) as small language models,but largely scale up the model size, pre-training data,and total compute power. This enables LLMs to betterunderstand natural language and generate high-quality textbased on given context or prompts.Note that, in existing literature, there is no formal con-sensus on the minimum parameter scale for LLMs, sincethe model capacity is also related to data size and totalcompute. In a recent survey of LLMs [17], the authors focuson discussing the language models with a model size largerthan 10B. Under their criteria, the first LLM is T5 releasedby Google in 2019, followed by GPT-3 released by OpenAIin 2020, and there are more than thirty LLMs released be-tween 2021 and 2023 indicating its popularity. In anothersurvey of unifying LLMs and knowledge graphs [24], theauthors categorize the LLMs into three types: encoder-only(e.g., BERT), encoder-decoder (e.g., T5), and decoder-onlynetwork architecture (e.g., GPT-3). In our review, we takeinto account the categorization criteria of the two surveysand only consider the encoder-decoder and decoder-onlynetwork architecture of pre-training language models, sincethey can both support generative tasks. We do not considerthe encoder-only network architecture because they cannothandle generative tasks, were proposed relatively early (e.g.,BERT in 2018), and there are almost no models using thisarchitecture after 2021. In other words, the LLMs discussedin this paper not only include models with parameters ofover 10B (as mentioned in [17]) but also include other mod-els that use the encoder-decoder and decoder-only networkarchitecture (as mentioned in [24]), such as BART with 140Mparameters and GPT-2 with parameter sizes ranging from117M to 1.5B. This is also to potentially include more studiesto demonstrate the landscape of this topic.2.2 Software TestingSoftware testing is a crucial process in software develop-ment that involves evaluating the quality of a software prod-uct. The primary goal of software testing is to identify de-fects or errors in the software system that could potentiallylead to incorrect or unexpected behavior. The whole lifecycle of software testing typically includes the followingtasks (demonstrated in Figure 4): Requirement Analysis: analyze the software require-ments and identify the testing objectives, scope, andcriteria. Test Plan: develop a test plan that outlines the testingstrategy, test objectives, and schedule. Test Design and Review: develop and review the testcases and test suites that align with the test plan andthe requirements of the software application. Test Case Preparation: the actual test cases are preparedbased on the designs created in the previous stage. Test Execution: execute the tests that were designed inthe previous stage. The software system is executedwith the test cases and the results are recorded. Test Reporting: analyze the results of the tests and gen-erate reports that summarize the testing process andidentify any defects or issues that were discovered. Bug Fixing and Regression Testing: defects or issuesidentified during testing are reported to the develop-ment team for fixing. Once the defects are fixed, regres-sion testing is performed to ensure that the changeshave not introduced new defects or issues. Software Release: once the software system has passedall of the testing stages and the defects have been fixed,the software can be released to the customer or enduser.The testing process is iterative and may involve multiplecycles of the above stages, depending on the complexity ofthe software system and the testing requirements.During the testing phase, various types of tests may beperformed, including unit tests, integration tests, systemtests, and acceptance tests. Unit Testing involves testing individual units or com-ponents of the software application to ensure that theyfunction correctly. Integration Testing involves testing different modulesor components of the software application together toensure that they work correctly as a system. System Testing involves testing the entire software sys-tem as a whole, including all the integrated componentsand external dependencies. Acceptance Testing involves testing the software appli-cation to ensure that it meets the business requirementsand is ready for deployment.In addition, there can be functional testing, performancetesting, unit testing, security testing, accessibility testing,etc, which explores various aspects of the software undertest [25].",
            "43.1.1 Automatic Search3.1.1 Automatic Filtering3.1.4 Quality Assessment3.1.5 Snowballing14,623 Papers102Papers1,239 Papers109 PapersSTART102 PapersMajor SE Venues& AI Venues3.1.2 Manual Search1,278 Papers3.1.3 Inclusion and Exclusion CriteriaFig. 2: Overview of the paper collection process3 P APER SELECTION AND REVIEW SCHEMA3.1 Paper Collection MethodologyFigure 2 shows our paper search and selection process. Tocollect as much relevant literature as possible, we use bothautomatic search (from paper repository database) and man-ual search (from major software engineering and artificialintelligence venues). We searched papers from Jan. 2019 toJun. 2023 and further conducted the second round of searchto include the papers from Jul. 2023 to Oct. 2023.3.1.1 Automatic SearchTo ensure that we collect papers from diverse research areas,we conduct an extensive search using four popular scientificdatabases: ACM digital library, IEEE Xplore digital library,arXiv, and DBLP .We search for papers whose title contains keywords re-lated to software testing tasks and testing techniques (as shownbelow) in the first three databases. In the case of DBLP , weuse additional keywords related to LLMs (as shown below)to filter out irrelevant studies, as relying solely on testing-related keywords would result in a large number of can-didate studies. While using two sets of keywords for DBLPmay result in overlooking certain related studies, we believeit is still a feasible strategy. This is due to the fact that asubstantial number of studies present in this database canalready be found in the first three databases, and the fourthdatabase only serves as a supplementary source for collect-ing additional papers. Keywords related with software testing tasks and tech-niques: test OR bug OR issue OR defect OR fault ORerror OR failure OR crash OR debug OR debugger ORrepair OR fix OR assert OR verification OR validationOR fuzz OR fuzzer OR mutation. Keywords related with LLMs: LLMOR language modelOR generative model OR large model OR GPT-3 ORChatGPT OR GPT-4 OR LLaMA OR PaLM2 OR CodeT5OR CodeX OR CodeGen OR Bard OR InstructGPT. Notethat, we only list the top ten most popular LLMs (basedon Google search), since they are the search keywordsfor matching paper titles, rather than matching the pa-per content.The above search strategy based on the paper title canrecall a large number of papers, and we further conduct theautomatic filtering based on the paper content. Specifically,we filter the paper whose content contains LLM or lan-guage model or generative model or large model orthe name of the LLMs (using the LLMs in [17], [24] exceptthose in our exclusion criteria). This can help eliminate thepapers that do not involve the neural models.3.1.2 Manual SearchTo compensate for the potential omissions that may resultfrom automated searches, we also conduct manual searches.In order to make sure we collect highly relevant papers,we conduct a manual search within the conference proceed-ings and journal articles from top-tier software engineeringvenues (listed in Table 2).In addition, given the interdisciplinary nature of thiswork, we also include the conference proceedings of theartificial intelligence field. We select the top ten venuesbased on the h5 index from Google Scholar, and excludethree computer vision venues, i.e., CVPR, ICCV , ECCV , aslisted in Table 2.3.1.3 Inclusion and Exclusion CriteriaThe search conducted on the databases and venue is, by de-sign, very inclusive. This allows us to collect as many papersas possible in our pool. However, this generous inclusivityresults in having papers that are not directly related to thescope of this survey. Accordingly, we define a set of specificinclusion and exclusion criteria and then we apply them toeach paper in the pool and remove papers not meeting thecriteria. This ensures that each collected paper aligns withour scope and research questions.Inclusion Criteria. We define the following criteria forincluding papers: The paper proposes or improves an approach, study, ortool/framework that targets testing specific software orsystems with LLMs. The paper applies LLMs to software testing practice,including all tasks within the software testing lifecycleas demonstrated in Section 2.2. The paper presents an empirical or experimental studyabout utilizing LLMs in software testing practice. The paper involves specific testing techniques (e.g.,fuzz testing) employing LLMs.If a paper satisfies any of the following criteria, we willinclude it.Exclusion Criteria. The following studies would be ex-cluded during study selection: The paper does not involve software testing tasks, e.g.,code comment generation. The paper does not utilize LLMs, e.g., using recurrentneural networks. The paper mentions LLMs only in future work or dis-cussions rather than using LLMs in the approach. The paper utilizes language models with encoder-onlyarchitecture, e.g., BERT, which can not directly be uti-lized for generation tasks (as demonstrated in Section2.1). The paper focuses on testing the performance of LLMs,such as fairness, stability, security, etc. [125][127]. The paper focuses on evaluating the performance ofLLM-enabled tools, e.g., evaluating the code quality ofthe code generation tool Copilot [128][130].For the papers collected through automatic search andmanual search, we conduct a manual inspection to checkwhether they satisfy our inclusion criteria and filter thosefollowing our exclusion criteria. Specifically, the first twoauthors read each paper to carefully determine whether it",
            "![](./images/llmpaper/image_2.png)",
            "![](./images/llmpaper/image_3.png)",
            "5TABLE 1: Details of the collected papersID Topic Paper title Year Reference1 Unit test case generation Unit Test Case Generation with Transformers and Focal Context 2020 [26]2 Unit test case generation Codet: Code Generation with Generated Tests 2022 [27]3 Unit test case generation Interactive Code Generation via Test-Driven User-Intent Formalization 2022 [28]4 Unit test case generation A3Test: Assertion-Augmented Automated Test Case Generation 2023 [29]5 Unit test case generation An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation 2023 [30]6 Unit test case generation An Initial Investigation of ChatGPT Unit Test Generation Capability 2023 [31]7 Unit test case generation Automated Test Case Generation Using Code Models and Domain Adaptation 2023 [32]8 Unit test case generation Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models 2023 [33]9 Unit test case generation Can Large Language Models Write Good Property-Based Tests? 2023 [34]10 Unit test case generation CAT-LM Training Language Models on Aligned Code And Tests 2023 [35]11 Unit test case generation ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation 2023 [8]12 Unit test case generation ChatUniTest: a ChatGPT-based Automated Unit Test Generation Tool 2023 [36]13 Unit test case generation CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models 2023 [37]14 Unit test case generation Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing 2023 [38]15 Unit test case generation Exploring the Effectiveness of Large Language Models in Generating Unit Tests 2023 [39]16 Unit test case generation How Well does LLM Generate Security Tests? 2023 [40]17 Unit test case generation No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation 2023 [7]18 Unit test case generation Prompting Code Interpreter to Write Better Unit Tests on Quixbugs Functions 2023 [41]19 Unit test case generation Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation 2023 [42]20 Unit test case generation Unit Test Generation using Generative AI: A Comparative Performance Analysis of Autogeneration Tools 2023 [43]21 Test oracle generation Generating Accurate Assert Statements for Unit Test Cases Using Pretrained Transformers 2022 [44]22 Test oracle generation Learning Deep Semantics for Test Completion 2023 [45]23 Test oracle generation; Program repairUsing Transfer Learning for Code-Related Tasks 2023 [46]24 Test oracle generation; Program repairRetrieval-Based Prompt Selection for Code-Related Few-Shot Learning 2023 [47]25 System test input generation Automated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing 2021 [48]26 System test input generation Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing 2022 [49]27 System test input generation Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors 2022 [50]28 System test input generation Slgpt: Using Transfer Learning to Directly Generate Simulink Model Files and Find Bugs in the Simulink Toolchain2021 [51]29 System test input generation Augmenting Greybox Fuzzing with Generative AI 2023 [52]30 System test input generation Automated Test Case Generation Using T5 and GPT-3 2023 [53]31 System test input generation Automating GUI-based Software Testing with GPT-3 2023 [54]32 System test input generation AXNav: Replaying Accessibility Tests from Natural Language 2023 [55]33 System test input generation Can ChatGPT Advance Software Testing Intelligence? An Experience Report on Metamorphic Testing 2023 [56]34 System test input generation Efficient Mutation Testing via Pre-Trained Language Models 2023 [57]35 System test input generation Large Language Models are Edge-Case Generators:Crafting Unusual Programs for Fuzzing Deep Learning Libraries2023 [58]36 System test input generation Large Language Models are Zero Shot Fuzzers: Fuzzing Deep Learning Libraries via Large Language Models2023 [59]37 System test input generation Large Language Models for Fuzzing Parsers (Registered Report) 2023 [60]38 System test input generation LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities 2023 [61]39 System test input generation Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions2023 [14]40 System test input generation PentestGPT: An LLM-empowered Automatic Penetration Testing Tool 2023 [62]41 System test input generation SMT Solver Validation Empowered by Large Pre-Trained Language Models 2023 [63]42 System test input generation TARGET: Automated Scenario Generation from Traffic Rules for Testing Autonomous Vehicles 2023 [64]43 System test input generation Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model2023 [65]44 System test input generation Understanding Large Language Model Based Fuzz Driver Generation 2023 [66]45 System test input generation Universal Fuzzing via Large Language Models 2023 [67]46 System test input generation Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software 2023 [68]47 System test input generation White-box Compiler Fuzzing Empowered by Large Language Models 2023 [69]48 Bug analysis Itiger: an Automatic Issue Title Generation Tool 2022 [70]49 Bug analysis CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace 2023 [71]50 Bug analysis Cupid: Leveraging ChatGPT for More Accurate Duplicate Bug Report Detection 2023 [72]51 Bug analysis Employing Deep Learning and Structured Information Retrieval to Answer Clarification Questions on Bug Reports2023 [73]52 Bug analysis Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation 2022 [74]53 Bug analysis Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models 2023 [75]54 Bug analysis Still Confusing for Bug-Component Triaging? Deep Feature Learning and Ensemble Setting to Rescue 2023 [76]55 Debug Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5 2022 [77]56 Debug Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction 2022 [78]57 Debug A Preliminary Evaluation of LLM-Based Fault Localization 2023 [79]58 Debug Addressing Compiler Errors: Stack Overflow or Large Language Models? 2023 [80]59 Debug Can LLMs Demystify Bug Reports? 2023 [81]60 Debug Dcc help: Generating Context-Aware Compiler Error Explanations with Large Language Models 2023 [82]61 Debug Explainable Automated Debugging via Large Language Model-driven Scientific Debugging 2023 [83]62 Debug Large Language Models for Test-Free Fault Localization 2023 [84]63 Debug Large Language Models in Fault Localisation 2023 [85]64 Debug LLM4CBI: Taming LLMs to Generate Effective Test Programs for Compiler Bug Isolation 2023 [86]65 Debug Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting 2023 [87]66 Debug Teaching Large Language Models to Self-Debug 2023 [88]67 Debug; Program repair A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair 2023 [89]68 Program repair Examining Zero-Shot Vulnerability Repair with Large Language Models 2022 [90]69 Program repair Automated Repair of Programs from Large Language Models 2022 [91]70 Program repair Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar 2022 [92]71 Program repair Practical Program Repair in the Era of Large Pre-trained Language Models 2022 [93]72 Program repair Repairing Bugs in Python Assignments Using Large Language Models 2022 [94]73 Program repair Towards JavaScript Program Repair with Generative Pre-trained Transformer (GPT-2) 2022 [95]74 Program repair An Analysis of the Automatic Bug Fixing Performance of ChatGPT 2023 [96]75 Program repair An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair 2023 [97]76 Program repair An Evaluation of the Effectiveness of OpenAIs ChatGPT for Automated Python Program Bug Fixing using QuixBugs2023 [98]77 Program repair An Extensive Study on Model Architecture and Program Representation in the Domain of Learning-based Automated Program Repair2023 [99]78 Program repair Can OpenAIs Codex Fix Bugs? An Evaluation on QuixBugs 2022 [100]79 Program repair CIRCLE: Continual Repair Across Programming Languages 2022 [101]80 Program repair Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback 2023 [102]81 Program repair Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair2023 [103]82 Program repair Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors 2023 [104]83 Program repair Enhancing Genetic Improvement Mutations Using Large Language Models 2023 [105]84 Program repair FixEval: Execution-based Evaluation of Program Fixes for Programming Problems 2023 [106]85 Program repair Fixing Hardware Security Bugs with Large Language Models 2023 [107]86 Program repair Fixing Rust Compilation Errors using LLMs 2023 [108]87 Program repair Framing Program Repair as Code Completion 2022 [109]88 Program repair Frustrated with Code Quality Issues? LLMs can Help! 2023 [110]89 Program repair GPT-3-Powered Type Error Debugging: Investigating the Use of Large Language Models for Code Repair 2023 [111]90 Program repair How Effective Are Neural Networks for Fixing Security Vulnerabilities 2023 [112]91 Program repair Impact of Code Language Models on Automated Program Repair 2023 [113]92 Program repair Inferfix: End-to-end Program Repair with LLMs 2023 [114]93 Program repair Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT 2023 [115]94 Program repair Neural Program Repair with Program Dependence Analysis and Effective Filter Mechanism 2023 [116]95 Program repair Out of Context: How important is Local Context in Neural Program Repair? 2023 [117]96 Program repair Pre-trained Model-based Automated Software Vulnerability Repair: How Far are We? 2023 [118]97 Program repair RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot 2023 [119]98 Program repair RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair 2023 [120]99 Program repair STEAM: Simulating the InTeractive BEhavior of ProgrAMmers for Automatic Bug Fixing 2023 [121]100 Program repair Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions 2023 [122]101 Program repair VulRepair: a T5-based Automated Software Vulnerability Repair 2022 [123]102 Program repair What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs? 2023 [124]",
            "6TABLE 2: Conference proceedings and journals consideredfor manual searchAcronym VenueSE ConferenceICSE International Conference on Software EngineeringESEC/FSE Joint European Software Engineering Conference and Symposium on theFoundations of Software EngineeringASE International Conference on Automated Software EngineeringISSTA International Symposium on Software Testing and AnalysisICST International Conference on Software Testing, Verification and ValidationESEM International Symposium on Empirical Software Engineering and Mea-surementMSR International Conference on Mining Software RepositoriesQRS International Conference on Software Quality, Reliability and SecurityICSME International Conference on Software Maintenance and EvolutionISSRE International Symposium on Software Reliability EngineeringSE JournalTSE Transactions on Software EngineeringTOSEM Transactions on Software Engineering and MethodologyEMSE Empirical Software EngineeringASE Automated Software EngineeringJSS Journal of Systems and SoftwareJSEP Journal of Software: Evolution and ProcessSTVR Software Testing, Verification and ReliabilityIEEE SOFTW. IEEE SoftwareIET SOFTW. IET SoftwareIST Information and Software TechnologySQJ Software Quality JournalAI VenuesICLR International Conference on Learning RepresentationsNeurIPS Conference on Neural Information Processing SystemsICML International Conference on Machine LearningAAAI AAAI Conference on Artificial IntelligenceEMNLP Conference on Empirical Methods in Natural Language ProcessingACL Annual Meeting of the Association for Computational LinguisticsIJCAI International Joint Conference on Artificial Intelligenceshould be included based on the inclusion criteria and exclu-sion criteria, and any paper with different decisions will behanded over to the third author to make the final decision.3.1.4 Quality AssessmentIn addition, we establish quality assessment criteria to ex-clude low-quality studies as shown below. For each ques-tion, the studys quality is rated as yes, partial or nowhich are assigned values of 1, 0.5, and 0, respectively. Pa-pers with a score of less than eight will be excluded fromour study. Is there a clearly stated research goal related to softwaretesting? Is there a defined and repeatable technique? Is there any explicit contribution to software testing? Is there an explicit description of which LLMs are uti-lized? Is there an explicit explanation about how the LLMs areutilized? Is there a clear methodology for validating the tech-nique? Are the subject projects selected for validation suitablefor the research goals? Are there control techniques or baselines to demon-strate the effectiveness of the proposed technique? Are the evaluation metrics relevant (e.g., evaluate theeffectiveness of the proposed technique) to the researchobjectives? Do the results presented in the study align with theresearch objectives and are they presented in a clearand relevant manner?3.1.5 SnowballingAt the end of searching database repositories and confer-ence proceedings and journals, and applying inclusion/ex-clusion criteria and quality assessment, we obtain the initialset of papers. Next, to mitigate the risk of omitting rele-vant literature from this survey, we also perform backward/uni00000015/uni00000013/uni00000015/uni00000013/uni00000015/uni00000013/uni00000015/uni00000014/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000013/uni00000015/uni00000016/uni00000003/uni00000033/uni00000058/uni00000045/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000003c/uni00000048/uni00000044/uni00000055/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000006/uni00000003/uni00000033/uni00000058/uni00000045/uni0000004f/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000014/uni00000015/uni00000014/uni0000001c/uni0000001b/uni00000015Fig. 3: Trend in the number of papers with yearsnowballing [131] by inspecting the references cited by thecollected papers so far. Note that, this procedure did not in-clude new studies, which might because the surveyed topicis quite new and the reference studies tend to published pre-viously, and we already include a relatively comprehensiveautomatic and manual search.3.2 Collection ResultsAs shown in Figure 2, the collection process startedwith a total of 14,623 papers retrieved from fouracademic databases employing keyword searching.Then after automated filtering, manual search, applyinginclusion/exclusion criteria, and quality assessment, wefinally collected a total of 102 papers involving softwaretesting with LLMs. Table 1 shows the details of the collectedpapers. Besides, we also use Table 5 (at the end of thepaper) to provide a more comprehensive overview of thesepapers regarding the specific characteristics which will beillustrated in Section 4 and Section 5.Note that, there are two studies which are respectivelythe extension of a previously published paper by the sameauthors ( [46] and [132], [68] and [133]), and we only keepthe extended version to avoid duplicate.3.3 General Overview of Collected PaperAmong the papers, 47% papers are published in softwareengineering venues, among which 19 papers are from ICSE,5 papers are from FSE, 5 papers are from ASE, and 3 pa-pers are from ISSTA. 2% papers are published in artificialintelligence venues such as EMNLP and ICLR, and 5% pa-pers are published in program analysis or security venueslike PLDI and S&P . Besides, 46% of the papers have notyet been published via peer-reviewed venues, i.e., they aredisclosed on arXiv. This is understandable because this fieldis emerging and many works are just completed and inthe process of submission. Although these papers did notundergo peer review, we have a quality assessment processthat eliminates papers with low quality, which potentiallyensures the quality of this survey.Figure 3 demonstrates the trend of our collected papersper year. We can see that as the years go by, the number ofpapers in this field is growing almost exponentially. In 2020and 2021, there were only 1 and 2 papers, respectively. In2022, there were 19 papers, and in 2023, there have been 82",
            "7Fig. 4: Distribution of testing tasks with LLMs (aligned with software testing life cycle [134][136], the number in bracketindicates the number of collected studies per task, and one paper might involve multiple tasks)papers. It is conceivable that there will be even more papersin the future, which indicates the popularity and attentionthat this field is receiving.4 A NALYSIS FROM SOFTWARE TESTING PER-SPECTIVEThis section presents our analysis from the viewpoint ofsoftware testing and organizes the collected studies in termsof testing tasks. Figure 4 lists the distribution of each in-volved testing task, aligned with the software testing lifecycle. We first provide a general overview of the distribu-tion, followed by further analysis for each task. Note that,for each following subsection, the cumulative total of sub-categories may not always match the total number of paperssince a paper might belong to more than one subcategory.We can see that LLMs have been effectively used in boththe mid to late stages of the software testing lifecycle. Inthe test case preparation phase, LLMs have been utilized fortasks such as generating unit test cases, test oracle genera-tion, and system test input generation. These tasks are cru-cial in the mid-phase of software testing to help catch issuesand prevent further development until issues are resolved.Furthermore, in later phases such as the test report/bug re-ports and bug fix phase, LLMs have been employed for taskssuch as bug analysis, debugging, and repair. These tasks arecritical towards the end of the testing phase when softwarebugs need to be resolved to prepare for the products release.4.1 Unit Test Case GenerationUnit test case generation involves writing unit test cases tocheck individual units/components of the software inde-pendently and ensure that they work correctly. For a methodunder test (i.e., often called the focal method), its corre-sponding unit test consists of a test prefix and a test oracle.In particular, the test prefix is typically a series of methodinvocation statements or assignment statements, which aimsat driving the focal method to a testable state; and then thetest oracle serves as the specification to check whether thecurrent behavior of the focal method satisfies the expectedone, e.g., the test assertion.To alleviate manual efforts in writing unit tests,researchers have proposed various techniques to facilitateautomated unit test generation. Traditional unit testgeneration techniques leverage search-based [3], [4],constraint-based [5] or random-based strategies [6] togenerate a suite of unit tests with the main goal ofmaximizing the coverage in the software under test.Nevertheless, the coverage and the meaningfulness of thegenerated tests are still far from satisfactory.Since LLMs have demonstrated promising results intasks such as code generation, and given that both codegeneration and unit test case generation involve generatingsource code, recent research has extended the domain ofcode generation to encompass unit test case generation.Despite initial success, there are nuances that set unittest case generation apart from general code generation,signaling the need for more tailored approaches.Pre-training or fine-tuning LLMs for unit test casegeneration. Due to the limitations of LLMs in their earlierstages, a majority of the earlier published studies adoptthis pre-training or fine-tuning schema. Moreover, in somerecent studies, this schema continues to be employed toincrease the LLMs familiarity with domain knowledge.Alagarsamy et al. [29] first pre-trained the LLM with thefocal method and asserted statements to enable the LLM tohave a stronger foundation knowledge of assertions, thenfine-tuned the LLM for the test case generation task wherethe objective is to learn the relationship between the focalmethod and the corresponding test case. Tufano et al. [26]utilized a similar schema by pre-training the LLM on alarge unsupervised Java corpus, and supervised fine-tuninga downstream translation task for generating unit tests.Hashtroudi et al. [32] leveraged the existing developer-written tests for each project to generate a project-specificdataset for domain adaptation when fine-tuning the LLM,which can facilitate generating human-readable unit tests.Rao et al. [35] trained a GPT-style language model byutilizing a pre-training signal that explicitly considers themapping between code and test files. Steenhoek et al.[42] utilizes reinforcement learning to optimize models byproviding rewards based on static quality metrics that canbe automatically computed for the generated unit test cases.Designing effective prompts for unit test case genera-tion. The advancement of LLMs has allowed them to excelat targeted tasks without pre-training or fine-tuning. There-fore most later studies typically focus on how to designthe prompt, to make the LLM better at understanding thecontext and nuances of this task. Xie et al. [36] generatedunit test cases by parsing the project, extracting essentialinformation, and creating an adaptive focal context that in-cludes a focal method and its dependencies within the pre-defined maximum prompt token limit of the LLM, and in-corporating these context into a prompt to query the LLM.",
            "![](./images/llmpaper/image_4.png)",
            "8TABLE 3: Performance of unit test case generationDataset Correctness Coverage LLM Paper5 Java projects from Defects4J 16.21% 5%-13% (line coverage) BART [26]10 Jave projects 40% 89% (line coverage), 90% (branch coverage) ChatGPT [36]CodeSearchNet 41% N/A ChatGPT [7]HumanEval 78% 87% (line coverage), 92% (branch coverage) Codex [39]SF110 2% 2% (line coverage), 1% (branch coverage) Codex [39]Note that, [39] experiments with Codex, CodeGen, and ChatGPT, and the best performance was achieved by Codex.Dakhel et al. [38] introduced MuTAP for improving the ef-fectiveness of test cases generated by LLMs in terms of re-vealing bugs by leveraging mutation testing. They augmentprompts with surviving mutants, as those mutants highlightthe limitations of test cases in detecting bugs. Zhang et al.[40] generated security tests with vulnerable dependencieswith LLMs.Yuan et al. [7] first performed an empirical study to eval-uate ChatGPTs capability of unit test generation with botha quantitative analysis and a user study in terms of cor-rectness, sufficiency, readability, and usability. And resultsshow that the generated tests still suffer from correctnessissues, including diverse compilation errors and executionfailures. They further propose an approach that leveragedthe ChatGPT itself to improve the quality of its generatedtests with an initial test generator and an iterative test re-finer. Specifically, the iterative test refiner iteratively fixedthe compilation errors in the tests generated by the initialtest generator, which follows a validate-and-fix paradigm toprompt the LLM based on the compilation error messagesand additional code context. Guilherme et al. [31] and Liet al. [41] respectively evaluated the quality of the gener-ated unit tests by LLM using different metrics and differentprompts.Test generation with additional documentation.Vikram et al. [34] went a step further by investigating thepotential of using LLMs to generate property-based testswhen provided API documentation. They believe that thedocumentation of an API method can assist the LLM inproducing logic to generate random inputs for that methodand deriving meaningful properties of the result to check.Instead of generating unit tests from the source code, Pleinet al. [33] generated the tests based on user-written bugreports.LLM and search-based method for unit test generation.The aforementioned studies utilize LLMs for the whole unittest case generation task, while Lemieux et al. [37] focus ona different direction, i.e., first letting the traditional search-based software testing techniques (e.g., Pynguin [137]) ingenerating unit test case until its coverage improvementsstall, then asking the LLM to provide the example test casesfor under-covered functions. These examples can help theoriginal test generation redirect its search to more usefulareas of the search space.Tang et al. [8] conducts a systematic comparison of testsuites generated by the LLM and the state-of-the-art search-based software testing tool EvoSuite, by considering the cor-rectness, readability, code coverage, and bug detection ca-pability. Similarly, Bhatia [43] experimentally investigatesthe quality of unit tests generated by LLM compared to acommonly-used test generator Pynguin.Performance of unit test case generation. Since theaforementioned studies of unit test case generation arebased on different datasets, one can hardly derive a faircomparison and we present the details in Table 3 to letthe readers obtain a general view. We can see that in theSF110 benchmark, all three evaluated LLMs have quite lowperformance, i.e., 2% coverage [39]. SF110 is an Evosuite(a search-based unit test case generation technique)benchmark consisting of 111 open-source Java projectsretrieved from SourceForge, containing 23,886 classes, over800,000 bytecode-level branches, and 6.6 million lines ofcode. The authors did not present detailed reasons for thelow performance which can be further explored in thefuture.4.2 Test Oracle GenerationA test oracle is a source of information about whether theoutput of a software system (or program or function ormethod) is correct or not [138]. Most of the collected studiesin this category target the test assertion generation, which isinside a unit test case. Nevertheless, we opted to treat thesestudies as separate sections to facilitate a more thoroughanalysis.Test assertion, which is to indicate the potential issuesin the tested code, is an important aspect that can distin-guish the unit test cases from the regular code. This is whysome studies specifically focus on the generation of effec-tive test assertions. Actually, before using LLMs, researchershave proposed RNN-based approaches that aim at learningfrom thousands of unit test methods to generate meaning-ful assert statements [139], yet only 17% of the generatedasserts can exactly match with the ground truth asserts. Sub-sequently, to improve the performance, several researchersutilized the LLMs for this task.Mastropaolo et al. [46], [132] pre-trained a T5 model ona dataset composed of natural language English text andsource code. Then, it fine-tuned such a model by reusingdatasets used in four previous works that used deep learn-ing techniques (such as RNN as mentioned before) includ-ing test assertion generation and program repair, etc. Resultsshowed that the extract match rate of the generated testassertion is 57%. Tufano et al. [44] proposed a similar ap-proach which separately pre-trained the LLM with Englishcorpus and code corpus, and then fine-tuned it on the assertsdataset (with test methods, focal methods, and asserts). Thisfurther improved the performance to 62% of the exact matchrate. Besides the syntax-level data as previous studies, Nie etal. [45] fine-tuned the LLMs with six kinds of code semanticsdata, including the execution result (e.g., types of the localvariables) and execution context (e.g., the last called methodin the test method), which enabled LLMs to learn to under-stand the code execution information. The exact match rate",
            "9is 17% (note that this paper is based on a different datasetfrom all other studies mentioned under this topic).The aforementioned studies utilized the pre-training andfine-tuning schema when using LLMs, and with the increas-ingly powerful capabilities of LLMs, they can perform wellon specific tasks without these specialized pre-training orfine-tuning datasets. Subsequently, Nashid et al. [47] uti-lized prompt engineering for this task, and proposed a tech-nique for prompt creation that automatically retrieves codedemonstrations similar to the task, based on embeddingor frequency analysis. They also present evaluations aboutthe few-shot learning with various numbers (e.g., zero-shot,one-shot, or n-shot) and forms (e.g., random vs. systematic,or with vs. without natural language descriptions) of theprompts, to investigate its feasibility on test assertion gen-eration. With only a few relevant code demonstrations, thisapproach can achieve an accuracy of 76% for exact matchesin test assertion generation, which is the state-of-the-art per-formance for this task.4.3 System Test Input GenerationThis category encompasses the studies related to creatingtest input of system testing for enabling the automation oftest execution. We employ three subsections to present theanalysis from three different orthogonal viewpoints, andeach of the collected studies may be analyzed in one ormore of these subsections.The first subsection is input generation in terms of softwaretypes. The generation of system-level test inputs for softwaretesting varies for specific types of software being tested. Forexample, for mobile applications, the test input generationrequires providing a diverse range of text inputs or oper-ation combinations (e.g., click a button, long press a list)[14], [49], which is the key to testing the applications func-tionality and user interface; while for Deep Learning (DL)libraries, the test input is a program which covers diversifiedDL APIs [58], [59]. This subsection will demonstrate how theLLMs are utilized to generate inputs for different types ofsoftware.The second subsection input generation in terms of testingtechniques. We have observed that certain approaches serveas specific types of testing techniques. For example, dozensof our collected studies specifically focus on using LLMsfor fuzz testing. Therefore, this subsection would providean analysis of the collected studies in terms of testing tech-niques, showcasing how the LLMs are employed to enhancetraditional testing techniques.The third subsection input generation in terms of input andoutput. While most of the collected studies take the sourcecode or the software itself as the input and directly outputthe softwares test input, there are studies that utilize alter-native forms of input and output. This subsection wouldprovide an analysis of such studies, highlighting differentapproaches and their input-output characteristics.4.3.1 Input Generation in Terms of Software TypesFigure 5 demonstrates the types of software under test inour collected studies. It is evident that the most prominentcategory is mobile apps, with five studies utilizing LLMsfor testing, possibly due to their prevalence and importance/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000030/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000003/uni00000044/uni00000053/uni00000053/uni00000027/uni00000048/uni00000048/uni00000053/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004f/uni0000004c/uni00000045/uni00000055/uni00000044/uni00000055/uni0000005c/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004c/uni0000004f/uni00000048/uni00000055/uni00000036/uni00000030/uni00000037/uni00000003/uni00000056/uni00000052/uni0000004f/uni00000059/uni00000048/uni00000055/uni00000024/uni00000058/uni00000057/uni00000052/uni00000051/uni00000052/uni00000050/uni00000052/uni00000058/uni00000056/uni00000003/uni00000047/uni00000055/uni0000004c/uni00000059/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni00000026/uni0000005c/uni00000045/uni00000048/uni00000055/uni00000003/uni00000053/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000044/uni0000004f/uni00000003/uni00000056/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni0000002a/uni00000032/uni00000003/uni00000057/uni00000052/uni00000052/uni0000004f/uni00000046/uni0000004b/uni00000044/uni0000004c/uni00000051/uni0000002d/uni00000044/uni00000059/uni00000044/uni00000036/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057/uni00000003/uni00000048/uni00000051/uni0000004a/uni0000004c/uni00000051/uni00000048/uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni00000058/uni00000050/uni00000003/uni00000046/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000053/uni0000004f/uni00000044/uni00000057/uni00000049/uni00000052/uni00000055/uni00000050/uni00000039/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000003/uni0000004a/uni00000044/uni00000050/uni00000048/uni00000036/uni00000052/uni00000049/uni00000057/uni0000005a/uni00000044/uni00000055/uni00000048/uni00000003/uni00000038/uni00000051/uni00000047/uni00000048/uni00000055/uni00000003/uni00000037/uni00000048/uni00000056/uni00000057/uni00000018/uni00000015/uni00000015/uni00000015/uni00000014/uni00000014/uni00000014/uni00000014/uni00000014/uni00000014Fig. 5: Distribution of software under testin todays business and daily life. Additionally, there arerespectively two studies focusing on testing deep learninglibraries, compilers, and SMT solvers. Moreover, LLM-basedtesting techniques have also been applied to domains suchas cyber-physical systems, quantum computing platforms,and more. This widespread adoption of LLMs demonstratestheir effectiveness in handling diverse test inputs and en-hancing testing activities across various software domains.A detailed analysis is provided below.Test input generation for mobile apps. For mobile apptesting, one difficulty is to generate the appropriate text in-puts to proceed to the next page, which remains a prominentobstacle for testing coverage. Considering the diversity andsemantic requirement of valid inputs (e.g., flight departure,movie name), traditional techniques with heuristic-based orconstraint-based techniques [10], [140] are far from generat-ing meaningful text input. Liu et al. [49] employ the LLMto intelligently generate the semantic input text accordingto the GUI context. In detail, their proposed QTypist auto-matically extracts the component information related to theEditText for generating the prompts, and then inputs theprompts into the LLM to generate the input text.Besides the text input, there are other forms of inputfor mobile apps, i.e., operations like click a button andselect a list. To fully test an app, it is required to covermore GUI pages and conduct more meaningful explorationtraces through the GUI operations, yet existing studies withrandom-/rule-based methods [9], [10], model-based meth-ods [11], [12], and learning-based methods [13] are unableto understand the semantic information of the GUI pagethus could not conduct the trace planning effectively. Liu etal. [14] formulates the test input generation of mobile GUItesting problem as a Q&A task, which asks LLM to chatwith the mobile apps by passing the GUI page informationto LLM to elicit testing scripts (i.e., GUI operation), andexecuting them to keep passing the app feedback to LLM, it-erating the whole process. The proposed GPTDroid extractsthe static context of the GUI page and the dynamic contextof the iterative testing process, and designs prompts for in-putting this information to LLM which enables the LLM tobetter understand the GUI page as well as the whole testingprocess. It also introduces a functionality-aware memoryprompting mechanism that equips the LLM with the abil-ity to retain testing knowledge of the whole process andconduct long-term, functionality-based reasoning to guideexploration. Similarly, Zimmermann et al. utilize the LLM to",
            "10interpret natural language test cases and programmaticallynavigate through the application under test [54].Yu et al. [61] investigate the LLMs capabilities in themobile app test script generation and migration task, in-cluding the scenario-based test generation, and the cross-platform/app test migration.Test input generation for DL libraries. The input fortesting DL libraries is DL programs, and the difficultyin generating the diversified input DL programs is thatthey need to satisfy both the input language (e.g., Python)syntax/semantics and the API input/shape constraints fortensor computations. Traditional techniques with API-levelfuzzing [141], [142] or model-level fuzzing [143], [144]suffer from the following limitations: 1) lack of diverse APIsequence thus cannot reveal bugs caused by chained APIsequences; 2) cannot generate arbitrary code thus cannotexplore the huge search space that exists when using the DLlibraries. Since LLMs can include numerous code snippetsinvoking DL library APIs in their training corpora, theycan implicitly learn both language syntax/semantics andintricate API constraints for valid DL program generation.Taken in this sense, Deng et al. [59] used both generativeand infilling LLMs to generate and mutate valid/diverseinput DL programs for fuzzing DL libraries. In detail, it firstuses a generative LLM (CodeX) to generate a set of seedprograms (i.e., code snippets that use the target DL APIs).Then it replaces part of the seed program with maskedtokens using different mutation operators and leverages theability of infilling LLM (InCoder) to perform code infillingto generate new code that replaces the masked tokens. Theirfollow-up study [58] goes a step further to prime LLMs tosynthesize unusual programs for the fuzzing DL libraries.It is built on the well-known hypothesis that historicalbug-triggering programs may include rare/valuable codeingredients important for bug finding and show improvedbug detection performance.Test input generation for other types of software.Thereare also dozens of studies that address testing tasks in vari-ous other domains, due to space limitations, we will presenta selection of representative studies in these domains.Finding bugs in a commercial cyber-physical system(CPS) development tool such as Simulink is even morechallenging. Given the complexity of the Simulink language,generating valid Simulink model files for testing is anambitious task for traditional machine learning or deeplearning techniques. Shrestha et al. [51] employs a small setof Simulink-specific training data to fine-tune the LLM forgenerating Simulink models. Results show that it can createSimulink models quite similar to the open-source models,and can find a super-set of the bugs traditional fuzzingapproaches found.Sun et al. [63] utilize LLM to generate test formulas forfuzzing SMT solvers. It retrains the LLMs on a large corpusof SMT formulas to enable them to acquire SMT-specificdomain knowledge. Then it further fine-tunes the LLMson historical bug-triggering formulas, which are knownto involve structures that are more likely to trigger bugsand solver-specific behaviors. The LLM-based compilerfuzzer proposed by Yang et al. [69] adopts a dual-modelframework: (1) an analysis LLM examines the low-leveloptimization source code and produces requirements on thehigh-level test programs that can trigger the optimization;(2) a generation LLM produces test programs based on thesummarized requirements. Ye et al. [48] utilize the LLMfor generating the JavaScript programs and then use thewell-structured ECMAScript specifications to automaticallygenerate test data along with the test programs, after thatthey apply differential testing to expose bugs.4.3.2 Input Generation in Terms of Testing TechniquesBy utilizing system test inputs generated by LLMs, the col-lected studies aim to enhance traditional testing techniquesand make them more effective. Among these techniques,fuzz testing is the most commonly involved one. Fuzz test-ing, as a general concept, revolves around generating in-valid, unexpected, or random data as inputs to evaluate thebehavior of software. LLMs play a crucial role in improv-ing traditional fuzz testing by facilitating the generation ofdiverse and realistic input data. This enables fuzz testing touncover potential bugs in the software by subjecting it to awide range of input scenarios. In addition to fuzz testing,LLMs also contribute to enhancing other testing techniques,which will be discussed in detail later.Universal fuzzing framework. Xia et al. [67] presentFuzz4All that can target many different input languagesand many different features of these languages. The keyidea behind it is to leverage LLMs as an input generationand mutation engine, which enables the approach toproduce diverse and realistic inputs for any practicallyrelevant language. To realize this potential, they presenta novel auto-prompting technique, which creates LLMprompts that are well-suited for fuzzing, and a novelLLM-powered fuzzing loop, which iteratively updates theprompt to create new fuzzing inputs. They experimentwith six different languages (C, C++, Go, SMT2, Java andPython) as inputs and demonstrate higher coverage thanexisting language-specific fuzzers. Hu et al. [52] propose agreybox fuzzer augmented by the LLM, which picks a seedin the fuzzers seed pool and prompts the LLM to producethe mutated seeds that might trigger a new code regionof the software. They experiment with three categories ofinput formats, i.e., formatted data files (e.g., json, xml),source code in different programming languages (e.g., JS,SQL, C), text with no explicit syntax rules (e.g., HTTPresponse, md5 checksum). In addition, effective fuzzingrelies on the effective fuzz driver, and Zhang et al. [66]utilize LLMs on the fuzz driver generation, in which fivequery strategies are designed and analyzed from basic toenhanced.Fuzzing techniques for specific software. There arestudies that focus on the fuzzing techniques tailored tospecific software, e.g., the deep learning library [58], [59],compiler [69], SMT solvers [63], input widget of mobile app[65], cyber-physical system [51], etc. One key focus of thesefuzzing techniques is to generate diverse test inputs so asto achieve higher coverage. This is commonly achievedby combining the mutation technique with LLM-basedgeneration, where the former produces various candidateswhile the latter is responsible for generating the executabletest inputs [59], [63]. Another focus of these fuzzingtechniques is to generate the risky test inputs that cantrigger bugs earlier. To achieve this, a common practice is to",
            "11collect the historical bug-triggering programs to fine-tunethe LLM [63] or treat them as the demonstrations whenquerying the LLM [58], [65].Other testing techniques. There are studies that utilizeLLMs for enhancing GUI testing for generating meaningfultext input [49] and functionality-oriented exploration traces[14], which has been introduced in Test input generation formobile apps part of Section 4.3.1.Besides, Deng et al. [62] leverage the LLMs to carry outpenetration testing tasks automatically. It involves setting apenetration testing goal for the LLM, soliciting it for theappropriate operation to execute, implementing it in thetesting environment, and feeding the test outputs back tothe LLM for next-step reasoning.4.3.3 Input Generation in Terms of Input and OutputOther output format of test generation. Although mostworks use LLM to generate test cases directly, there are alsosome works generating indirect inputs like testing code, testscenarios, metamorphic relations, etc. Liu et al. [65] pro-pose InputBlaster which leverages the LLM to automati-cally generate unusual text inputs for fuzzing the text inputwidgets in mobile apps. It formulates the unusual inputsgeneration problem as a task of producing a set of test gen-erators, each of which can yield a batch of unusual textinputs under the same mutation rule. In detail, InputBlasterleverages LLM to produce the test generators together withthe mutation rules serving as the reasoning chain and uti-lizes the in-context learning schema to demonstrate the LLMwith examples for boosting the performance. Deng et al.[64] use LLM to extract key information related to the testscenario from a traffic rule, and represent the extracted in-formation in a test scenario schema, then synthesize thecorresponding scenario scripts to construct the test scenario.Luu et al. [56] examine the effectiveness of LLM in generat-ing metamorphic relations (MRs) for metamorphic testing.Their results show that ChatGPT can be used to advancesoftware testing intelligence by proposing MRs candidatesthat can be later adapted for implementing tests, but humanintelligence should still inevitably be involved to justify andrectify their correctness.Other input format of test generation. The aforemen-tioned studies primarily take the source code or the softwareas the input of LLM, yet there are also studies that takenatural language description as the input for test generation.Mathur et al. [53] propose to generate test cases from thenatural language described requirements. Ackerman et al.[60] generate the instances from natural language describedrequirements recursively to serve as the seed examples for amutation fuzzer.4.4 Bug AnalysisThis category involves analyzing and categorizing the iden-tified software bugs to enhance understanding of the bug,and facilitate subsequent debug and bug repair. Mukher-jee et al. [73] generate relevant answers to follow-up ques-tions for deficient bug reports to facilitate bug triage. Su etal. [76] transform the bug-component triaging into a multi-classification task and a generation task with LLM, thenensemble the prediction results from them to improve theperformance of bug-component triaging further. Zhang etal. [72] first leverage the LLM under the zero-shot settingto get essential information on bug reports, then use theessential information as the input to detect duplicate bug re-ports. Mahbub et al. [74] proposes to explain software bugswith LLM, which generates natural language explanationsfor software bugs by learning from a large corpus of bug-fixcommits. Zhang et al. [70] target to automatically generatethe bug title from the descriptions of the bug, which aimsto help developers write issue titles and facilitate the bugtriaging and follow-up fixing process.4.5 DebugThis category refers to the process of identifying and locat-ing the cause of a software problem (i.e., bug). It involvesanalyzing the code, tracing the execution flow, collectingerror information to understand the root cause of the issue,and fixing the issue. Some studies concentrate on the com-prehensive debug process, while others delve into specificsub-activities within the process.Overall debug framework. Bui et al. [77] proposes a uni-fied Detect-Localize-Repair framework based on the LLMfor debugging, which first determines whether a given codesnippet is buggy or not, then identifies the buggy lines, andtranslates the buggy code to its fixed version. Kang et al.[83] proposes automated scientific debugging, a techniquethat given buggy code and a bug-revealing test, promptsLLMs to automatically generate hypotheses, uses debuggersto actively interact with buggy code, and thus automati-cally reaches conclusions prior to patch generation. Chenet al. [88] demonstrate that self-debugging can teach theLLM to perform rubber duck debugging; i.e., without anyhuman feedback on the code correctness or error messages,the model is able to identify its mistakes by investigating theexecution results and explaining the generated code in nat-ural language. Cao et al. [89] conducts a study of LLMs de-bugging ability for deep learning programs, including faultdetection, fault localization and program repair.Bug localization. Wu et al. [85] compare the two LLMs(ChatGPT and GPT-4) with the existing fault localizationtechniques, and investigate the consistency of LLMs in faultlocalization, as well as how prompt engineering and thelength of code context affect the results. Kang et al. [79]propose AutoFL, an automated fault localization techniquethat only requires a single failing test, and during its faultlocalization process, it also generates an explanation aboutwhy the given test fails. Yang et al. [84] propose LLMAO toovercome the left-to-right nature of LLMs by fine-tuning asmall set of bidirectional adapter layers on top of the rep-resentations learned by LLMs, which can locate buggy linesof code without any test coverage information. Tu et al. [86]propose LLM4CBI to tame LLMs to generate effective testprograms for finding suspicious files.Bug reproduction. There are also studies focusing on asub-phase of the debugging process. For example, Kang etal. [78] and Plein et al. [81] respectively propose the frame-work to harness the LLM to reproduce bugs, and suggestbug reproducing test cases to the developer for facilitatingdebugging. Li et al. [87] focus on a similar aspect of findingthe failure-inducing test cases whose test input can trigger",
            "12the softwares fault. It synergistically combines LLM anddifferential testing to do that.There are also studies focusing on the bug reproduc-tion of mobile apps to produce the replay script. Feng etal. [75] propose AdbGPT, a new lightweight approach toautomatically reproduce the bugs from bug reports throughprompt engineering, without any training and hard-codingeffort. It leverages few-shot learning and chain-of-thoughtreasoning to elicit human knowledge and logical reasoningfrom LLMs to accomplish the bug replay in a manner similarto a developer. Huang et al. [71] propose CrashTranslator toautomatically reproduce bugs directly from the stack trace.It accomplishes this by leveraging the LLM to predict theexploration steps for triggering the crash, and designing areinforcement learning based technique to mitigate the in-accurate prediction and guide the search holistically. Taeb etal. [55] convert the manual accessibility test instructions intoreplayable, navigable videos by using LLM and UI elementdetection models, which can also help reveal accessibilityissues.Error explanation. Taylor et al. [82] integrates the LLMinto the Debugging C Compiler to generate unique, novice-focused explanations tailored to each error. Widjojo et al.[80] study the effectiveness of Stack Overflow and LLMs atexplaining compiler errors.4.6 Program RepairThis category denotes the task of fixing the identifiedsoftware bugs. The high frequency of repair-related studiescan be attributed to the close relationship between thistask and the source code. With their advanced naturallanguage processing and understanding capabilities, LLMare well-equipped to process and analyze source code,making them an ideal tool for performing code-relatedtasks such as fixing bugs.There have been template-based [145], heuristic-based[146], and constraint-based [147], [148] automatic programrepair techniques. And with the development of deeplearning techniques in the past few years, there have beenseveral studies employing deep learning techniques forprogram repair. They typically adopt deep learning modelsto take a buggy software program as input and generate apatched program. Based on the training data, they wouldbuild a neural network model that learns the relationsbetween the buggy code and the corresponding fixed code.Nevertheless, these techniques still fail to fix a large portionof bugs, and they typically have to generate hundreds tothousands of candidate patches and take hours to validatethese patches to fix enough bugs. Furthermore, the deeplearning based program repair models need to be trainedwith huge amounts of labeled training data (typicallypairs of buggy and fixed code), which is time- and effort-consuming to collect the high-quality dataset. Subsequently,with the popularity and demonstrated capability of theLLMs, researchers begin to explore the LLMs for programrepair.Patch single-line bugs. In the early era of program re-pair, the focus was mainly on addressing defects related tosingle-line code errors, which are relatively simple and didnot require the repair of complex program logic. Lajk o etal. [95] propose to fine-tune the LLM with JavaScript codesnippets to serve as the purpose for the JavaScript programrepair. Zhang et al. [116] employs program slicing to extractcontextual information directly related to the given buggystatement as repair ingredients from the corresponding pro-gram dependence graph, which makes the fine-tuning morefocused on the buggy code. Zhang et al. [121] propose astage-wise framework STEAM for patching single-line bugs,which simulates the interactive behavior of multiple pro-grammers involved in bug management, e.g., bug reporting,bug diagnosis, patch generation, and patch verification.Since most real-world bugs would involve multiple linesof code, and later studies explore these more complex situa-tions (although some of them can also patch the single-linebugs).Patch multiple-lines bugs. The studies in this categorywould input a buggy function to the LLM, and the goal is tooutput the patched function, which might involve complexsemantic understanding, code hunk modification, as wellas program refactoring. Earlier studies typically employ thefine-tuning strategy to enable the LLM to better understandthe code semantics. Fu et al. [123] fine-tune the LLM byemploying BPE tokenization to handle Out-Of-Vocabulary(OOV) issues which makes the approach generate new to-kens that never appear in a training function but are newlyintroduced in the repair. Wang et. al. [120] train the LLMbased on both buggy input and retrieved bug-fix exampleswhich are retrieved in terms of the lexical and semanticalsimilarities. The aforementioned studies (including the onesin patching single-line bugs) would predict the fixed pro-grams directly, and Hu et al. [92] utilize a different setupthat predicts the scripts that can fix the bugs when executedwith the delete and insert grammar. For example, it predictswhether an original line of code should be deleted, and whatcontent should be inserted.Nevertheless, fine-tuning may face limitations in termsof its reliance on abundant high-quality labeled data,significant computational resources, and the possibility ofoverfitting. To approach the program repair problem moreeffectively, later studies focus on how to design an effectiveprompt for program repair. Several studies empiricallyinvestigate the effectiveness of prompt variants of the latestLLMs for program repair under different repair settingsand commonly-used benchmarks (which will be exploredin depth later), while other studies focus on proposingnew techniques. Ribeiro et al. [109] take advantage ofLLM to conduct the code completion in a buggy line forpatch generation, and elaborate on how to circumvent theopen-ended nature of code generation to appropriatelyfit the new code in the original program. Xia et al. [115]propose the conversation-driven program repair approachthat interleaves patch generation with instant feedbackto perform the repair in a conversational style. They firstfeed the LLM with relevant test failure information to startwith, and then learns from both failures and successesof earlier patching attempts of the same bug for morepowerful repair. For earlier patches that failed to passall tests, they combine the incorrect patches with theircorresponding relevant test failure information to constructa new prompt for the LLM to generate the next patch,in order to avoid making the same mistakes. For earlier",
            "13TABLE 4: Performance of program repairDataset % Correct patches LLM PaperDefects4J v1.2, Defects4Jv2.0, QuixBugs,HumanEval-Java22/40 Jave bugs (QuixBugs dataset, with InCoder-6B, correctcode infilling setting)PLBART, CodeT5, CodeGen, In-Coder (each with variant pa-rameters, 10 LLMs in total)[113]QuixBugs 23/40 Python bugs, 14/40 Java bugs (complete function genera-tion setting)Codex-12B [100]Defects4J v1.2, Defects4Jv2.0, QuixBugs, Many-Bugs39/40 Python bugs, 34/40 Java bugs (QuixBugs dataset, withCodex-12B, correct code infilling setting); 37/40 Python bugs,32/40 Java bugs (QuixBugs dataset, with Codex-12B, completefunction generation setting)Codex, GPT-Neo, CodeT5, In-Coder (each with variant pa-rameters, 9 LLMs in total)[93]QuixBugs 31/40 Python bugs (completion function generation setting) ChatGPT-175B [96]DL programs from Stack-Overflow16/72 Python bugs (complete function generation setting) ChatGPT-175B [89]Note that, for studies with multiple datasets or LLMs, we only present the best performance or in the most commonly utilized dataset.patches that passed all the tests (i.e., plausible patches),they further ask the LLM to generate alternative variationsof the original plausible patches. This can further build onand learn from earlier successes to generate more plausiblepatches to increase the chance of having correct patches.Zhang et al. [94] propose a similar approach design byleveraging multimodal prompts (e.g., natural languagedescription, error message, input-output-based test cases),iterative querying, test-case-based few-shot selection toproduce repairs. Moon et al. [102] propose for bug fixingwith feedback. It consists of a critic model to generatefeedback, an editor to edit codes based on the feedback,and a feedback selector to choose the best possible feedbackfrom the critic.Wei et. al. [103] propose Repilot to copilot the AI copi-lots (i.e., LLMs) by synthesizing more valid patches duringthe repair process. Its key insight is that many LLMs pro-duce outputs autoregressively (i.e., token by token), and byresembling human writing programs, the repair can be sig-nificantly boosted and guided through a completion engine.Brownlee et al. [105] propose to use the LLM as mutationoperators for the search-based techniques of program repair.Repair with static code analyzer. Most of the programrepair studies would suppose the bug has been detected,while Jin et al. [114] propose a program repair frameworkpaired with a static analyzer to first detect the bugs, andthen fix them. In detail, the static analyzer first detects anerror (e.g., null pointer dereference) and the context infor-mation provided by the static analyzer will be sent into theLLM for querying the patch for this specific error. Wadhwaet al. [110] focus on a similar task, and additionally employan LLM as the ranker to assess the likelihood of acceptanceof generated patches which can effectively catch plausiblebut incorrect fixes and reduce developer burden.Repair for specific bugs. The aforementioned studiesall consider the buggy code as the input for the automaticprogram repair, while other studies conduct program re-pairing in terms of other types of bug descriptions, specifictypes of bugs, etc. Fakhoury et al. [122] focus on programrepair from natural language issue descriptions, i.e., gen-erating the patch with the bug and fix-related informationdescribed in the issue reports. Garg et al. [119] aim at re-pairing performance issues, in which they first retrieve aprompt instruction from a pre-constructed knowledge-baseof previous performance bug fixes and then generate a re-pair prompt using the retrieved instruction. There are stud-ies focusing on the bug fixing of Rust programs [108] orOCaml programs (an industrial-strength programming lan-guage) [111].Empirical study about program repair.There are severalstudies related to the empirical or experimental evaluationof the various LLMs on program repair, and we summa-rize the performance in Table 4. Jiang et al. [113], Xia et al.[93], and Zhang et. al. [118] respectively conduct compre-hensive experimental evaluations with various LLMs andon different automated program repair benchmarks, whileother researchers [89], [96], [98], [100] focus on a specificLLM and on one dataset, e.g., QuixBugs. In addition, Gaoet al. [124] empirically investigate the impact of in-contextdemonstrations for bug fixing, including the selection, or-der, and number of demonstration examples. Prenner et al.[117] empirically study how the local context (i.e., code thatcomes before or after the bug location) affects the repair per-formance. Horv ath et al. [99] empirically study the impactof program representation and model architecture on therepair performance.There are two commonly-used repair settings when us-ing LLMs to generate patches: 1) complete function gen-eration (i.e., generating the entire patch function), 2) cor-rect code infilling (i.e., filling in a chunk of code given theprefix and suffix), and different studies might utilize differ-ent settings which are marked in Table 4. The commonly-used datasets are QuixBugs, Defects4J, etc. These datasetsonly involve the fundamental functionalities such as sortingalgorithms, each programs average number of lines rang-ing from 13 to 22, implementing one functionality, and in-volving few dependencies. To tackle this, Cao et al. [89]conducts an empirical study on a more complex datasetwith DL programs collected from StackOverflow. Every pro-gram contains about 46 lines of code on average, imple-menting several functionalities including data preprocess-ing, DL model construction, model training, and evaluation.And the dataset involves more than 6 dependencies for eachprogram, including TensorFlow, Keras, and Pytorch. Theirresults demonstrate a much lower rate of correct patchesthan in other datasets, which again reveals the potentialdifficulty of this task. Similarly, Haque et al. [106] introducea dataset comprising of buggy code submissions and theircorresponding fixes collected from online judge platforms,in which it offers an extensive collection of unit tests toenable the evaluations about the correctness of fixes and fur-ther information regarding time, memory constraints, andacceptance based on a verdict.",
            "14ChatGPT, 3625%Codex, 2316%CodeT5, 1813% GPT-4, 1410%GPT-3, 75%CodeGen, 64%InCoder, 54%PLBART, 54%T5, 54%CodeGPT, 43%GPT-2, 43%BART, 32%StarCoder, 32%UniXCoder, 21%Others, 75%Fig. 6: LLMs used in the collected papers5 A NALYSIS FROM LLM PERSPECTIVEThis section discusses the analysis based on the viewpointsof LLM, specifically, its unfolded from the viewpoints ofutilized LLMs, types of prompt engineering, input of theLLMs, as well as the accompanied techniques when utilizingLLM.5.1 LLM ModelsAs shown in Figure 6, the most commonly utilized LLMin software testing tasks is ChatGPT, which was releasedon Nov. 2022 by OpenAI. It is trained on a large corpusof natural language text data, and primarily designed fornatural language processing and conversation. ChatGPT isthe most widely recognized and popular LLM up until now,known for its exceptional performance across various tasks.Therefore, it comes as no surprise that it ranks in the topposition in terms of our collected studies.Codex, an LLM based on GPT-3, is the second most com-monly used LLM in our collected studies. It is trained on amassive code corpus containing examples from many pro-gramming languages such as JavaScript, Python, C/C++,and Java. Codex was released on Sep. 2021 by OpenAI andpowers GitHub Copilot an AI pair programmer that gener-ates whole code snippets, given a natural language descrip-tion as a prompt. Since a large portion of our collected stud-ies involve the source code (e.g., repair, unit test case gen-eration), it is not surprising that researchers choose Codexas the LLM in assisting them in accomplishing the coding-related tasks.The third-ranked LLM is CodeT5, which is an open-sourced LLM developed by salesforce 3. Thanks to its opensource, researchers can easily conduct the pre-training andfine-tuning with domain-specific data to achieve betterperformance. Similarly, CodeGen is also open-sourced andranked relatively higher. Besides, for CodeT5 and CodeGen,there are more than half of the related studies involve theempirical evaluations (which employ multiple LLMs), e.g.,program repair [112], [113], unit test case generation [39].3. https://blog.salesforceairesearch.com/codet5/There are already 14 studies that utilize GPT-4, rankingat the fourth place, which is launched on March 2023. Sev-eral studies directly utilize this state-of-the-art LLM of Ope-nAI, since it demonstrates excellent performance across awide range of generation and reasoning tasks. For example,Xie et al. utilize GPT-4 to generate fuzzing inputs [67], whileVikram et al. employ it to generate property-based tests withthe assistance of API documentation [34]. In addition, somestudies conduct experiments using both GPT-4 and Chat-GPT or other LLMs to provide a more comprehensive evalu-ation of these models performance. In their proposed LLM-empowered automatic penetration testing technique, Denget al. find that GPT-4 surpasses ChatGPT and LaMDA fromGoogle [62]. Similarly, Zhang et al. find that GPT-4 showsits performance superiority over ChatGPT when generat-ing the fuzz drivers with both the basic query strategiesand enhanced query strategies [66]. Furthermore, GPT-4, asa multi-modal LLM, sets itself apart from the other men-tioned LLMs by showcasing additional capabilities such asgenerating image narratives and answering questions basedon images [149]. Yet we have not come across any studiesthat explore the utilization of GPT-4s image-related features(e.g., UI screenshots, programming screencasts) in softwaretesting tasks.5.2 Types of Prompt EngineeringAs shown in Figure 7, among our collected studies, 38studies utilize the LLMs through pre-training or fine-tuning schema, while 64 studies employ the promptengineering to communicate with LLMs to steer itsbehavior for desired outcomes without updating the modelweights. When using the early LLMs, their performancesmight not be as impressive, so researchers often usepre-training or fine-tuning techniques to adjust the modelsfor specific domains and tasks in order to improve theirperformance. Then with the upgrading of LLM technology,especially with the introduction of GPT-3 and laterLLMs, the knowledge contained within the models andtheir understanding/inference capability has increasedsignificantly. Therefore, researchers will typically rely onprompt engineering to consider how to design appropriateprompts to stimulate the models knowledge.Among the 64 studies with prompt engineering, 51 stud-ies involve zero-shot learning, and 25 studies involve few-shot learning (a study may involve multiple types). Thereare also studies involving the chain-of-though (7 studies),self-consistency (1 study), and automatic prompt (1 study).Zero-shot learning is to simply feed the task text to themodel and ask for results. Many of the collected studies em-ploy the Codex, CodeT5, and CodeGen (as shown in Section5.1), which is already trained on source code. Hence, for thetasks dealing with source code like unit test case generationand program repair as demonstrated in previous sections,directly querying the LLM with prompts is the commonpractice. There are generally two types of manners of zero-shot learning, i.e., with and without instructions. For exam-ple, Xie et al. [36] would provide the LLMs with the instruc-tions as please help me generate a JUnit test for a specificJava method ... to facilitate the unit test case generation.In contrast, Siddiq et al. [39] only provide the code header",
            "15Fig. 7: Distribution about how LLM is used (Note that, a study can involve multiple types of prompt engineering)of the unit test case (e.g., class $ {className}${suffix}Test{), and the LLMs would carry out the unit test case gener-ation automatically. Generally speaking, prompts with clearinstructions will yield more accurate results, while promptswithout instructions are typically suitable for very specificsituations.Few-shot learning presents a set of high-quality demon-strations, each consisting of both input and desired output,on the target task. As the model first sees the examples,it can better understand human intention and criteria forwhat kinds of answers are wanted, which is especially im-portant for tasks that are not so straightforward or intuitiveto the LLM. For example, when conducting the automatictest generation from general bug reports, Kang et al. [78]provide examples of bug reports (questions) and the corre-sponding bug reproducing tests (answers) to the LLM, andtheir results show that two examples can achieve the highestperformance than no examples or other number of exam-ples. Another example of test assertion generation, Nashidet al. [47] provide demonstrations of the focal method, thetest method containing an <AssertPlaceholder>, and the ex-pected assertion, which enables the LLMs to better under-stand the task.Chain-of-thought (CoT) prompting generates asequence of short sentences to describe reasoning logicsstep by step (also known as reasoning chains or rationales)to the LLMs for generating the final answer. For example,for program repair from the natural language issuedescriptions [122], given the buggy code and issue report,the authors first ask the LLM to localize the bug, and thenthey ask it to explain why the localized lines are buggy,finally, they ask the LLM to fix the bug. Another example isfor generating unusual programs for fuzzing deep learninglibraries, Deng et al. [58] first generate a possible bug (bugdescription) before generating the actual bug-triggeringcode snippet that invokes the target API. The predictedbug description provides an additional hint to the LLM,indicating that the generated code should try to coverspecific potential buggy behavior.Self-consistency involves evaluating the coherence andconsistency of the LLMs responses on the same input indifferent contexts. There is one study with this prompttype, and it is about debugging. Kang et al. [83] employ ahypothesize-observe-conclude loop, which first generatesa hypothesis about what the bug is and constructs anexperiment to verify, using an LLM, then decide whetherthe hypothesis is correct based on the experiment result(with a debugger or code execution) using an LLM, afterthat, depending on the conclusion, it either starts with anew hypothesis or opts to terminate the debugging processand generate a fix.Automatic prompt aims to automatically generate andselect the appropriate instruction for the LLMs, instead ofrequiring the user to manually engineer a prompt. Xia etal. [67] introduce an auto-prompting step that automaticallydistils all user-provided inputs into a concise and effectiveprompt for fuzzing. Specifically, they first generate a list ofcandidate prompts by incorporating the user inputs andauto prompting instruction while setting the LLM at hightemperature, then a small-scale fuzzing experiment is con-ducted to evaluate each candidate prompt, and the best oneis selected.Note that there are fourteen studies that apply the it-erative prompt design when using zero-shot or few-shotlearning, in which the approach continuously refines theprompts with the running information of the testing task,e.g., the test failure information. For example, for programrepair, Xia et al. [115] interleave patch generation with testvalidation feedback to prompt future generation iteratively.In detail, they incorporate various information from a failingtest including its name, the relevant code line(s) triggeringthe test failure, and the error message produced in the nextround of prompting which can help the model understandthe failure reason and provide guidance towards generatingthe correct fix. Another example is for mobile GUI testing,Liu et al. [14] iteratively query the LLM about the operation(e.g., click a button, enter a text) to be conducted in themobile app, and at each iteration, they would provide theLLM with current context information like which GUI pagesand widgets have just explored.Mapping between testing tasks and how LLMs areused. Figure 8 demonstrates the mapping between the test-ing tasks (mentioned in Section 4) and how LLMs are used(as introduced in this subsection). The unit test case gen-eration and program repair share similar patterns of com-municating with the LLMs, since both tasks are closely re-lated to the source code. Typically, researchers utilize pre-training and/or fine-tuning and zero-shot learning methodsfor these two tasks. Zero-shot learning is suitable becausethese tasks are relatively straightforward and can be easilyunderstood by LLMs. Moreover, since the training data forthese two tasks can be automatically collected from sourcecode repositories, pre-training and/or fine-tuning methods",
            "![](./images/llmpaper/image_5.png)",
            "16Fig. 8: Mapping between testing tasks and how LLMs areusedCode, 7868%Bug description, 1210%Error information, 76%View hierarchy file of UI, 65%Others, 1210%Fig. 9: Input of LLMare widely employed for these two tasks, which can enhanceLLMs understanding of domain-specific knowledge.In comparison, for system test input generation, zero-shot learning and few-shot learning methods are commonlyused. This might be because this task often involves gener-ating specific types of inputs, and demonstrations in few-shot learning can assist the LLMs in better understandingwhat should be generated. Besides, for this task, the uti-lization of pre-training and/or fine-tuning methods are notas widespread as in unit test case generation and programrepair. This might be attributed to the fact that training datafor system testing varies across different software and isrelatively challenging to collect automatically.5.3 Input of LLMWe also find that different testing tasks or software undertest might involve diversified input when querying theLLM, as demonstrated in Figure 9.The most commonly utilized input is the source codesince a large portion of collected studies relate to programrepair or unit test case generation whose input are sourcecode. For unit test case generation, typical code-related in-formation would be (i) the complete focal method, includingthe signature and body; (ii) the name of the focal class (i.e.,the class that the focal method belongs to); (iii) the field inthe focal class; and (iv) the signatures of all methods definedin the focal class [7], [26]. For program repair, there can bedifferent setups and involve different inputs, including (i)inputting a buggy function with the goal of outputting thepatched function, (ii) inputting the buggy location with thegoal of generating the correct replacement code (can be asingle line change) given the prefix and suffix of the buggyfunction [93]. Besides, there can be variations for the buggylocation input, i.e., (i) does not contain the buggy lines (butthe bug location is still known), (ii) give the buggy lines aslines of comments.There are also 12 studies taking the bug description asinput for the LLM. For example, Kang et al. [78] take thebug description as input when querying LLM and let theLLM generate the bug-reproducing test cases. Fakhoury etal. [122] input the natural language descriptions of bugs tothe LLM, and generate the correct code fixes.There are 7 studies that would provide the intermedi-ate error information , e.g., test failure information, to theLLM, and would conduct the iterative prompt (as describedin Section 5.2) to enrich the context provided to the LLM.These studies are related to the unit test case generationand program repair, since in these scenarios, the runninginformation can be acquired easily.When testing mobile apps, since the utilized LLM couldnot understand the image of the GUI page, the view hierar-chy file which represents the details of the GUI page usuallyacts as the input to LLMs. Nevertheless, with the emergenceof GPT-4 which is a multimodal model and accepts bothimage and text inputs for model input, the GUI screenshotsmight be directly utilized for LLMs input.5.4 Incorporating Other Techniques with LLMThere are divided opinions on whether LLM has reachedan all-powerful status that requires no other techniques. Asshown in Figure 10, among our collected studies, 67 of themutilize LLMs to address the entire testing task, while 35 stud-ies incorporate additional techniques. These techniques in-clude mutation testing, differential testing, syntactic check-ing, program analysis, statistical analysis, etc. .The reason why researchers still choose to combineLLMs with other techniques might be because, despiteexhibiting enormous potential in various tasks, LLMs stillpossess limitations such as comprehending code semanticsand handling complex program structures. Therefore,combining LLMs with other techniques optimizes theirstrengths and weaknesses to achieve better outcomes inspecific scenarios. In addition, it is important to note thatwhile LLMs are capable of generating correct code, theymay not necessarily produce sufficient test cases to checkfor edge cases or rare scenarios. This is where mutationand other testing techniques come into play, as they allowfor the generation of more diverse and complex code thatcan better simulate real-world scenarios. Taken in thissense, a testing approach can incorporate a combinationof different techniques, including both LLMs and othertesting strategies, to ensure comprehensive coverage andeffectiveness.LLM + statistical analysis. As LLMs can often generatea multitude of outputs, manually sifting through and iden-tifying the correct output can be overwhelmingly laborious.As such, researchers have turned to statistical analysis tech-niques like ranking and clustering [28], [45], [78], [93], [116]",
            "![](./images/llmpaper/image_7.png)",
            "17Fig. 10: Distribution about other techniques incorporated with LLMs (Note that, a study can involve multiple types)to efficiently filter through LLMs outputs and ultimatelyobtain more accurate results.LLM + program analysis. When utilizing LLMs toaccomplish tasks such as generating unit test cases andrepairing software code, it is important to consider thatsoftware code inherently possesses structural information,which may not be fully understood by LLMs. Hence,researchers often utilize program analysis techniques,including code abstract syntax trees (ASTs) [74], torepresent the structure of code more effectively and increasethe LLMs ability to comprehend the code accurately.Researchers also perform the structure-based subsettingof code lines to narrow the focus for LLM [94], or extractadditional code context from other code files [7], to enablethe models to focus on the most task-relevant informationin the codebase and lead to more accurate predictions.LLM + mutation testing. It is mainly targeting at gener-ating more diversified test inputs. For example, Deng et al.[59] first use LLM to generate the seed programs (e.g., codesnippets using a target DL API) for fuzzing deep learninglibraries. To enrich the pool of these test programs, theyreplace parts of the seed program with masked tokens usingmutation operators (e.g., replaces the API call argumentswith the span token) to produce masked inputs, and againutilize the LLMs to perform code infilling to generate newcode that replaces the masked tokens.LLM + syntactic checking. Although LLMs have shownremarkable performance in various natural language pro-cessing tasks, the generated code from these models cansometimes be syntactically incorrect, leading to potential er-rors and reduced usability. Therefore, researchers have pro-posed to leverage syntax checking to identify and correcterrors in the generated code. For example, in their work forunit test case generation, Alagarsamy et al. [29] addition-ally introduce a verification method to check and repair thenaming consistency (i.e., revising the test method name tobe consistent with the focal method name) and the test sig-natures (i.e., adding missing keywords like public, void, or@test annotations). Xie et al. [36] also validates the generatedunit test case and employs rule-based repair to fix syntacticand simple compile errors.LLM + differential testing. Differential testing is well-suited to find semantic or logic bugs that do not exhibitexplicit erroneous behaviors like crashes or assertionfailures. In this category of our collected studies, the LLMis mainly responsible for generating valid and diversifiedinputs, while the differential testing helps to determinewhether there is a triggered bug based on the softwaresoutput. For example, Ye et al. [48] first uses LLM toproduce random JavaScript programs, and leverages thelanguage specification document to generate test data, thenconduct the differential testing on JavaScript engines suchas JavaScriptCore, ChakraCore, SpiderMonkey, QuickJS,etc. There are also studies utilizing the LLMs to generatetest inputs and then conduct differential testing for fuzzingDL libraries [58], [59] and SAT solvers [63]. Li et al. [87]employs the LLM in finding the failure-inducing test cases.In detail, given a program under test, they first request theLLM to infer the intention of the program, then request theLLM to generate programs that have the same intention,which are alternative implementations of the program, andare likely free of the programs bug. Then they performthe differential testing with the program under test and thegenerated programs to find the failure-inducing test cases.6 C HALLENGES AND OPPORTUNITIESBased on the above analysis from the viewpoints of soft-ware testing and LLM, we summarize the challenges andopportunities when conducting software testing with LLM.6.1 ChallengesAs indicated by this survey, software testing with LLMshas undergone significant growth in the past two years.However, it is still in its early stages of development, andnumerous challenges and open questions need to be ad-dressed.6.1.1 Challenges for Achieving High CoverageExploring the diverse behaviors of the software under testto achieve high coverage is always a significant concernin software testing. In this context, test generation differsfrom code generation, as code generation primarily focuseson producing a single, correct code snippet, whereas soft-ware testing requires generating diverse test inputs to en-sure better coverage of the software. Although setting a hightemperature can facilitate the LLMs in generating differentoutputs, it remains challenging for LLMs to directly achievethe required diversity. For example, for unit test case gen-eration, in SF110 dataset, the line coverage is merely 2%and the branch coverage is merely 1% [39]. For system testinput generation, in terms of fuzzing DL libraries, the APIcoverage for TensorFlow is reported to be 66% (2215/3316)[59].",
            "![](./images/llmpaper/image_6.png)",
            "18From our collected studies, we observe that theresearchers often utilize mutation testing together with theLLMs to generate more diversified outputs. For example,when fuzzing a DL library, instead of directly generatingthe code snippet with LLM, Deng et al. [59] replace partsof the selected seed (code generated by LLM) with maskedtokens using different mutation operators to producemasked inputs. They then leverage the LLM to performcode infilling to generate new code that replaces the maskedtokens, which can significantly increase the diversity of thegenerated tests. Liu et al. [65] leverage LLM to produce thetest generators (each of which can yield a batch of unusualtext inputs under the same mutation rule) together with themutation rules for text-oriented fuzzing, which reduces thehuman effort required for designing mutation rules.A potential research direction could involve utilizingtesting-specific data to train or fine-tune a specialized LLMthat is specifically designed to understand the nature oftesting. By doing so, the LLM can inherently acknowledgethe requirements of testing and autonomously generatediverse outputs.6.1.2 Challenges in Test Oracle ProblemThe oracle problem has been a longstanding challenge invarious testing applications, e.g., testing machine learningsystems [150] and testing deep learning libraries [59]. Toalleviate the oracle problem to the overall testing activities,a common practice in our collected studies is to transform itinto a more easily derived form, often by utilizing differen-tial testing [63] or focusing on only identifying crash bugs[14].There are successful applications of differential testingwith LLMs, as shown in Figure 10. For instance, whentesting the SMT solvers, Sun et al. adopt differential testingwhich involves comparing the results of multiple SMTsolvers (i.e., Z3, cvc5, and Bitwuzla) on the same generatedtest formulas by LLM [63]. However, this approach islimited to systems where counterpart software or runningenvironment can easily be found, potentially restrictingits applicability. Moreover, to mitigate the oracle problem,other studies only focus on the crash bugs which are easilyobserved automatically. This is particularly the case formobile applications testing, in which the LLMs guide thetesting in exploring more diversified pages, conductingmore complex operational actions, and covering moremeaningful operational sequences [14]. However, thissignificantly restricts the potential of utilizing the LLMs foruncovering various types of software bugs.Exploring the use of LLMs to derive other types oftest oracles represents an interesting and valuable researchdirection. Specifically, metamorphic testing is also widelyused in software testing practices to help mitigate the oracleproblem, yet in most cases, defining metamorphic relationsrelies on human ingenuity. Luu et al. [56] have examined theeffectiveness of LLM in generating metamorphic relations,yet they only experiment with straightforward prompts bydirectly querying ChatGPT. Further exploration, potentiallyincorporating human-computer interaction or domainknowledge, is highly encouraged. Another promisingavenue is exploring the capability of LLMs to automaticallygenerate test cases based on metamorphic relations,covering a wide range of inputs.The advancement of multi-model LLMs like GPT-4 mayopen up possibilities for exploring their ability to detectbugs in software user interfaces and assist in deriving testoracles. By leveraging the image understanding and reason-ing capabilities of these models, one can investigate theirpotential to automatically identify inconsistencies, errors, orusability issues in user interfaces.6.1.3 Challenges for Rigorous EvaluationsThe lack of benchmark datasets and the potential data leak-age issues associated with LLM-based techniques presentchallenges in conducting rigorous evaluations and compre-hensive comparisons of proposed methods.For program repair, there are only two well-known andcommonly-used benchmarks, i.e., Defect4J and QuixBugs,as demonstrated in Table 4. Furthermore, these datasets arenot specially designed for testing the LLMs. For example, asreported by Xia et al. [93], 39 out of 40 Python bugs in theQuixBugs dataset can be fixed by Codex, yet in real-worldpractice, the successful fix rate can be nowhere near as high.For unit test case generation, there are no widely recognizedbenchmarks, and different studies would utilize differentdatasets for performance evaluation, as demonstrated in Ta-ble 3. This indicates the need to build more specialized anddiversified benchmarks.Furthermore, the LLMs may have seen the widely-usedbenchmarks in their pre-training data, i.e., data leakageissues. Jiang et al. [113] check the CodeSearchNet andBigQuery, which are the data sources of common LLMs,and the results show that four repositories used by theDefect4J benchmark are also in CodeSearchNet, and thewhole Defects4J repository is included by BigQuery.Therefore, it is very likely that existing program repairbenchmarks are seen by the LLMs during pre-training. Thisdata leakage issue has also been investigated in machinelearning-related studies. For example, Tu et al. [151] focuson the data leakage in issue tracking data, and results showthat information leaked from the future makes predictionmodels misleadingly optimistic. This reminds us that theperformance of LLMs on software testing tasks may not beas good as reported in previous studies. It also suggeststhat we need more specialized datasets that are not seen byLLMs to serve as benchmarks. One way is to collect it fromspecialized sources, e.g., user-generated content from nicheonline communities.6.1.4 Challenges in Real-world Application of LLMs in Soft-ware TestingAs we mentioned in Section 5.2, in the early days of us-ing LLMs, pre-training and fine-tuning are commonly usedpractice, considering the model parameters are relativelyfew resulting in weaker model capabilities (e.g., T5). As timeprogressed, the number of model parameters increased sig-nificantly, leading to the emergence of models with greatercapabilities (e.g., ChatGPT). And in recent studies, promptengineering has become a common approach. However, dueto concerns regarding data privacy, when considering real-world practice, most software organizations tend to avoid",
            "19using commercial LLMs and would prefer to adopt open-source ones with training or fine-tuning using organization-specific data. Furthermore, some companies also considerthe current limitations in terms of computational power orpay close attention to energy consumption, they tend tofine-tune medium-sized models. It is quite challenging forthese models to achieve similar performance to what ourcollected papers have reported. For instance, in the widely-used QuixBugs dataset, it has been reported that 39 out of40 Python bugs and 34 out of 40 Java bugs can be automat-ically fixed [93]. However, when it comes to DL programscollected from Stack Overflow, which represent real-worldcoding practice, only 16 out of 72 Python bugs can be auto-matically fixed [89].Recent research has highlighted the importance of high-quality training data in improving the performance of mod-els for code-related tasks [152], yet manually building high-quality organization-specific datasets for training or fine-tuning is time-consuming and labor-intensive. To addressthis, one is encouraged to utilize the automated techniquesof mining software repositories to build the datasets, forexample, techniques like key information extraction tech-niques from Stack Overflow [153] offer potential solutionsfor automatically gathering relevant data.In addition, exploring the methodology for better fine-tuning the LLMs with software-specific data is worth con-sidering because software-specific data differs from naturallanguage data as it contains more structural information,such as data flow and control flow. Previous research oncode representations has shown the benefits of incorporat-ing data flow, which captures the semantic-level structureof code and represents the relationship between variables interms of whether-value-comes-from [154]. These insightscan provide valuable guidance for effectively fine-tuningLLMs with software-specific data.6.2 OpportunitiesThere are also many research opportunities in software test-ing with LLMs, which can greatly benefit developers, users,and the research community. While not necessarily chal-lenges, these opportunities contribute to advancements insoftware testing, benefiting practitioners and the wider re-search community.6.2.1 Exploring LLMs in the Early Stage of TestingAs shown in Figure 4, LLMs have not been used in the earlystage of testing, e.g., test requirements, and test planning.There might be two main reasons behind that. The first isthe subjectivity in early-stage testing tasks. Many tasks inthe early stages of testing, such as requirements gathering,test plan creation, and design reviews, may involve subjec-tive assessments that require significant input from humanexperts. This could make it less suitable for LLMs that relyheavily on data-driven approaches. The second might be thelack of open-sourced data in the early stages. Unlike in laterstages of testing, there may be limited data available onlineduring early-stage activities. This could mean that LLMsmay not have seen much of this type of data, and thereforemay not perform well on these tasks.Adopting a human-computer interaction schema fortackling early-stage testing tasks would harness the domain-specific knowledge of human developers and leverage thegeneral knowledge embedded in LLMs. Additionally, it ishighly encouraged for software development companiesto record and provide access to early-stage testing data,allowing for improved training and performance of LLMsin these critical testing activities.6.2.2 Exploring LLMs in Other Testing PhasesWe have analyzed the distribution of testing phases for thecollected studies. As shown in Fig 11, we can observe thatLLMs are most commonly used in unit testing, followed bysystem testing. However, there is still no research on the useof LLMs in integration testing and acceptance testing./uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000038/uni00000051/uni0000004c/uni00000057/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057/uni0000002c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057/uni00000036/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057/uni00000024/uni00000046/uni00000046/uni00000048/uni00000053/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000057/uni00000048/uni00000056/uni00000057/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000033/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000056/uni00000015/uni00000017/uni00000013/uni00000015/uni00000015/uni00000013Fig. 11: Distribution of testing phases (note that we omit thestudies which do not explicitly specify the testing phases,e.g., program repair)For integration testing, it involves testing the interfacesbetween different software modules. In some software or-ganizations, integration testing might be merged with unittesting, which can be a possible reason why LLM is rarelyutilized in integration testing. Another reason might be thatthe size and complexity of the input data in this circum-stance may exceed the capacity of the LLM to process andanalyze (e.g., the source code of all involved software mod-ules), which can lead to errors or unreliable results. To tacklethis, a potential reference can be found in Section 4.1, whereXie et al. [36] design a method to organize the necessaryinformation into the pre-defined maximum prompt tokenlimit of the LLM. Furthermore, integration testing requiresdiversified data to be generated to sufficiently test the in-terface among multiple modules. As mentioned in Section4.3, previous work has demonstrated the LLMs capabilityin generating diversified test input for system testing, inconjunction with mutation testing techniques [48], [59]. Andthese can provide insights about generating the diversifiedinterface data for integration testing.Acceptance testing is usually conducted by business an-alysts or end-users to validate the systems functionalityand usability, which requires more non-technical languageand domain-specific knowledge, thus making it challengingto apply LLM effectively. Since acceptance testing involveshumans, it is well-suited for the use of human-in-the-loopschema with LLMs. This has been studied in traditionalmachine learning [155], but has not yet been explored withLLMs. Specifically, the LLMs can be responsible for auto-matically generating test cases, evaluating test coverage, etc,while human testers are responsible for checking the pro-grams behavior and verifying test oracle.",
            "206.2.3 Exploring LLMs for More Types of SoftwareWe analyze what types of software have been explored inthe collected studies, as shown in Figure 5. Note that, sincea large portion of studies are focused on unit testing orprogram repair, they are conducted on publicly availabledatasets and do not involve specific software types.From the analysis in Section 4.3, the LLM can generatenot only the source code for testing DL libraries but alsothe textual input for testing mobile apps, even the modelsfor testing CPS. Overall, the LLM provides a flexible andpowerful framework for generating test inputs for a widerange of applications. Its versatility would make it usefulfor testing the software in other domains.From one point of view, some proposed techniques canbe applied to other types of software. For example, in thepaper proposed for testing deep learning libraries [58], sinceit proposes techniques for generating diversified, compli-cated, and human-like DL programs, the authors state thatthe approach can be easily extended to test software systemsfrom other application domains, e.g., interpreters, databasesystems, and other popular libraries. More than that, thereare already studies that focus on universal fuzzing tech-niques [52], [67] which are designed to be adaptable andapplicable to different types of test inputs and software.From another point of view, other types of software canalso benefit from the capabilities of LLMs to design the test-ing techniques that are better suited to their specific do-main and characteristics. For instance, the metaverse, withits immersive virtual environments and complex interac-tions, presents unique challenges for software testing. LLMscan be leveraged to generate diverse and realistic inputs thatmimic user behavior and interactions within the metaverse,which are never explored.6.2.4 Exploring LLMs for Non-functional TestingIn our collected studies, LLMs are primarily used for func-tional testing, and no practice in performance testing, usabil-ity testing or others. One possible reason for the prevalenceof LLM-based solutions in functional testing is that theycan convert functional testing problems into code gener-ation or natural language generation problems [14], [59],which LLMs are particularly adept at solving.On the other hand, performance testing and usabilitytesting may require more specialized models that are de-signed to detect and analyze specific types of data, handlecomplex statistical analyses, or determine the buggy criteria.Moreover, there have been dozens of performance testingtools (e.g., LoadRunner [156]) that can generate a workloadthat simulates real-world usage scenarios and achieve rela-tively satisfactory performance.The potential opportunities might let the LLM integratethe performance testing tools and acts like the LangChain[157], to better simulate different types of workloads basedon real user behavior. Furthermore, the LLMs can identifythe parameter combinations and values that have the high-est potential to trigger performance problems. It is essen-tially a way to rank and prioritize different parameter set-tings based on their impact on performance and improvethe efficiency of performance testing.6.2.5 Exploring Advanced Prompt EngineeringThere are a total of 11 commonly used prompt engineeringtechniques as listed in a popular prompt engineering guide[158], as shown in Figure 12. Currently, in our collectedstudies, only the first five techniques are being utilized. Themore advanced techniques have not been employed yet, andcan be explored in the future for prompt design.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. 12: List of advanced prompt engineering practices andthose utilized in the collected papersFor instance, multimodal chain of thought prompting in-volves using diverse sensory and cognitive cues to stimulatethinking and creativity in LLMs [159]. By providing images(e.g., GUI screenshots) or audio recordings related to thesoftware under test can help the LLM better understandthe softwares context and potential issues. Besides, try toprompt the LLM to imagine itself in different roles, suchas a developer, user, or quality assurance specialist. Thisperspective-shifting exercise enables the LLM to approachsoftware testing from multiple viewpoints and uncover dif-ferent aspects that might require attention or investigation.Graph prompting [160] involves the representation ofinformation using graphs or visual structures to facilitateunderstanding and problem-solving. Graph prompting canbe a natural match with software engineering, considerit involves various dependencies, control flow, data flow,state transitions, or other relevant graph structure. Graphprompting can be beneficial in analyzing this structuralinformation, and enabling the LLMs to comprehend thesoftware under test effectively. For instance, testers can usegraph prompts to visualize test coverage, identify untestedareas or paths, and ensure adequate test execution.6.2.6 Incorporating LLMs with Traditional TechniquesThere is currently no clear consensus on the extent to whichLLMs can solve software testing problems. From the analy-sis in Section 5.4, we have seen some promising results fromstudies that have combined LLMs with traditional softwaretesting techniques. This implies the LLMs are not the solesilver bullet for software testing. Considering the availabil-ity of many mature software testing techniques and tools,and the limited capabilities of LLMs, it is necessary to ex-plore other better ways to combine LLMs with traditionaltesting or program analysis techniques and tools for bettersoftware testing.",
            "21Based on the collected studies, the LLMs have been suc-cessfully utilized together with various techniques such asdifferential testing (e.g., [63]), mutation testing (e.g., [59]),program analysis (e.g., [104], as shown in Figure 10. Fromone perspective, future studies can explore improved in-tegration of these traditional techniques with LLMs. Takemutation testing as an example, current practices mainlyrely on the human-designed mutation rules to mutate thecandidate tests, and let the LLMs re-generate new tests [38],[59], [67], while Liu et al. directly utilize the LLMs for pro-ducing the mutation rules alongside the mutated tests [65].Further explorations in this direction are of great interest.From another point of view, more traditional techniquescan be incorporated in LLMs for software testing. For in-stance, besides the aforementioned traditional techniques,the LLMs have been combined with formal verification forself-healing software detection in the field of software se-curity [161]. More attempts are encouraged. Moreover, con-sidering the existence of numerous mature software testingtools, one can explore the integration of LLMs with thesetools, allowing them to act as a LangChain to better ex-plore the potential of these tools.7 R ELATED WORKThe systematic literature review is a crucial manner for gain-ing insights into the current trends and future directionswithin a particular field. It enables us to understand andstay updated on the developments in that domain.Wang et al. surveyed the machine learning and deeplearning techniques for software engineering [162]. Yang etal. and Watson et al. respectively carried out surveys aboutthe use of deep learning in software engineering domain[163], [164]. Bajammal et al. surveyed the utilization of com-puter vision techniques to improve software engineeringtasks [165]. Zhang et al. provided a survey of techniquesfor testing machine learning systems [150]With the advancements of artificial intelligence andLLMs, researchers also conduct systematic literaturereviews about LLMs, and their applications in variousfields (e.g., software engineering). Zhao et al. [17] reviewedrecent advances in LLMs by providing an overview of theirbackground, key findings, and mainstream techniques.They focused on four major aspects of LLMs, namelypre-training, adaptation tuning, utilization, and capacityevaluation. Additionally, they summarized the availableresources for developing LLMs and discuss the remainingissues for future directions. Hou et al. conducted asystematic literature review on using LLMs for softwareengineering, with a particular focus on understandinghow LLMs can be exploited to optimize processes andoutcomes [166]. Fan et al. conducted a survey of LLMs forsoftware engineering, and set out open research challengesfor the application of LLMs to technical problems faced bysoftware engineers [167]. Zan et al. conducted a survey ofexisting LLMs for NL2Code task (i.e., generating code froma natural language description), and reviewed benchmarksand metrics [168].While these studies either targeted the broader softwareengineering domain (with a limited focus on software test-ing tasks) or focused on other software development tasks(excluding software testing), this paper specifically focuseson the use of LLMs for software testing. It surveys relatedstudies, summarizes key challenges and potential opportu-nities, and serves as a roadmap for future research in thisarea.8 C ONCLUSIONThis paper provides a comprehensive review of the useof LLMs in software testing. We have analyzed relevantstudies that have utilized LLMs in software testing fromboth the software testing and LLMs perspectives. This paperalso highlights the challenges and potential opportunitiesin this direction. Results of this review demonstrate thatLLMs have been successfully applied in a wide rangeof testing tasks, including unit test case generation, testoracle generation, system test input generation, programdebugging, and program repair. However, challenges stillexist in achieving high testing coverage, addressing thetest oracle problem, conducting rigorous evaluations, andapplying LLMs in real-world scenarios. Additionally, it isobserved that LLMs are commonly used in only a subset ofthe entire testing lifecycle, for example, they are primarilyutilized in the middle and later stages of testing, onlyserving the unit and system testing phases, and only forfunctional testing. This highlights the research opportunitiesfor exploring the uncovered areas. Regarding how the LLMsare utilized, we find that various pre-training/fine-tuningand prompt engineering methods have been developedto enhance the capabilities of LLMs in addressing testingtasks. However, more advanced techniques in promptdesign have yet to be explored and can be an avenue forfuture research.It can serve as a roadmap for future research in this area,identifying gaps in our current understanding of the use ofLLMs in software testing and highlighting potential avenuesfor exploration. We believe that the insights provided in thispaper will be valuable to both researchers and practition-ers in the field of software engineering, assisting them inleveraging LLMs to improve software testing practices andultimately enhance the quality and reliability of softwaresystems.REFERENCES[1] G. J. Myers, The art of software testing (2. ed.) . Wiley,2004. [Online]. Available: http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471469122.html[2] M. Pezz `e and M. Young, Software testing and analysis - process,principles and techniques. Wiley, 2007.[3] M. Harman and P . McMinn, A theoretical and empirical studyof search-based testing: Local, global, and hybrid search, vol. 36,no. 2, 2010, pp. 226247.[4] P . Delgado-P erez, A. Ram rez, K. J. Valle-G omez, I. Medina-Bulo, and J. R. Romero, Interevo-tr: Interactive evolutionarytest generation with readability assessment, IEEE Trans. SoftwareEng., vol. 49, no. 4, pp. 25802596, 2023.[5] X. Xiao, S. Li, T. Xie, and N. Tillmann, Characteristic studiesof loop problems for structural test generation via symbolicexecution, in 2013 28th IEEE/ACM International Conference onAutomated Software Engineering, ASE 2013, Silicon Valley, CA, USA,November 11-15, 2013 , E. Denney, T. Bultan, and A. Zeller, Eds.IEEE, 2013, pp. 246256.[6] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball, Feedback-directed random test generation, in 29th International Conferenceon Software Engineering (ICSE 2007), Minneapolis, MN, USA, May20-26, 2007. IEEE Computer Society, 2007, pp. 7584.",
            "22[7] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng,No more manual tests? evaluating and improving chatgpt forunit test generation, arXiv preprint arXiv:2305.04207, 2023.[8] Y. Tang, Z. Liu, Z. Zhou, and X. Luo, Chatgpt vs SBST:A comparative assessment of unit test suite generation,CoRR, vol. abs/2307.00588, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.00588[9] A. Developers, Ui/application exerciser monkey, 2012.[10] Y. Li, Z. Yang, Y. Guo, and X. Chen, Droidbot: a lightweight ui-guided test input generator for android, in ICSE. IEEE, 2017.[11] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, andZ. Su, Guided, stochastic model-based gui testing of androidapps, in Proceedings of the 2017 11th Joint Meeting on Foundationsof Software Engineering, 2017, pp. 245256.[12] Z. Dong, M. B ohme, L. Cojocaru, and A. Roychoudhury, Time-travel testing of android apps, in ICSE. IEEE, 2020.[13] M. Pan, A. Huang, G. Wang, T. Zhang, and X. Li, Reinforcementlearning based curiosity-driven testing of android applications,in Proceedings of the 29th ACM SIGSOFT International Symposiumon Software Testing and Analysis, 2020, pp. 153164.[14] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che, D. Wang,and Q. Wang, Make LLM a testing expert: Bringing human-like interaction to mobile GUI testing via functionality-awaredecisions, CoRR, vol. abs/2310.15780, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.15780[15] T. Su, J. Wang, and Z. Su, Benchmarking automated GUI testingfor android against real-world bugs, in ESEC/FSE 21: 29th ACMJoint European Software Engineering Conference and Symposium onthe Foundations of Software Engineering, Athens, Greece, August 23-28, 2021. ACM, 2021, pp. 119130.[16] M. Shanahan, Talking about large language models,CoRR, vol. abs/2212.03551, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2212.03551[17] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou,Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang,Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu,P . Liu, J. Nie, and J. Wen, A survey of large languagemodels, CoRR, vol. abs/2303.18223, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2303.18223[18] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, andY. Iwasawa, Large language models are zero-shot reasoners, in NeurIPS, 2022. [Online]. Avail-able: http://papers.nips.cc/paper files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter,F. Xia, E. H. Chi, Q. V . Le, and D. Zhou,Chain-of-thought prompting elicits reasoning in largelanguage models, in NeurIPS, 2022. [Online]. Avail-able: http://papers.nips.cc/paper files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html[20] J. Li, G. Li, Y. Li, and Z. Jin, Structured chain-of-thoughtprompting for code generation, 2023. [Online]. Available:https://api.semanticscholar.org/CorpusID:258615421[21] J. Li, Y. Li, G. Li, Z. Jin, Y. Hao, and X. Hu, Skcoder: Asketch-based approach for automatic code generation, in 2023IEEE/ACM 45th International Conference on Software Engineering(ICSE), 2023, pp. 21242135.[22] J. Li, Y. Zhao, Y. Li, G. Li, and Z. Jin, Acecoder: Utilizing existingcode to enhance code generation, 2023. [Online]. Available:https://api.semanticscholar.org/CorpusID:257901190[23] Y. Dong, X. Jiang, Z. Jin, and G. Li, Self-collaborationcode generation via chatgpt, CoRR, vol. abs/2304.07590, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2304.07590[24] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu,Unifying large language models and knowledge graphs: Aroadmap, CoRR, vol. abs/2306.08302, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2306.08302[25] G. J. Myers, T. Badgett, T. M. Thomas, and C. Sandler, The art ofsoftware testing. Wiley Online Library, 2004, vol. 2.[26] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sun-daresan, Unit test case generation with transformers and focalcontext, arXiv preprint arXiv:2009.05617, 2020.[27] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, andW. Chen, Codet: Code generation with generated tests, arXivpreprint arXiv:2207.10397, 2022.[28] S. K. Lahiri, A. Naik, G. Sakkas, P . Choudhury, C. von Veh,M. Musuvathi, J. P . Inala, C. Wang, and J. Gao, Interactivecode generation via test-driven user-intent formalization, arXivpreprint arXiv:2208.05950, 2022.[29] S. Alagarsamy, C. Tantithamthavorn, and A. Aleti, A3test:Assertion-augmented automated test case generation, arXivpreprint arXiv:2302.10352, 2023.[30] M. Sch afer, S. Nadi, A. Eghbali, and F. Tip, An empirical eval-uation of using large language models for automated unit testgeneration, IEEE Transactions on Software Engineering , pp. 121,2023.[31] V . Guilherme and A. Vincenzi, An initial investigationof chatgpt unit test generation capability, in 8th BrazilianSymposium on Systematic and Automated Software Testing, SAST2023, Campo Grande, MS, Brazil, September 25-29, 2023 , A. L.Fontao, D. M. B. Paiva, H. Borges, M. I. Cagnin, P . G.Fernandes, V . Borges, S. M. Melo, V . H. S. Durelli, and E. D.Canedo, Eds. ACM, 2023, pp. 1524. [Online]. Available:https://doi.org/10.1145/3624032.3624035[32] S. Hashtroudi, J. Shin, H. Hemmati, and S. Wang,Automated test case generation using code models anddomain adaptation, CoRR, vol. abs/2308.08033, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2308.08033[33] L. Plein, W. C. Ou edraogo, J. Klein, and T. F. Bissyand e,Automatic generation of test cases based on bug reports:a feasibility study with large language models, CoRR, vol.abs/2310.06320, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.06320[34] V . Vikram, C. Lemieux, and R. Padhye, Can largelanguage models write good property-based tests? CoRR,vol. abs/2307.04346, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.04346[35] N. Rao, K. Jain, U. Alon, C. L. Goues, and V . J. Hellendoorn,CAT-LM training language models on aligned code andtests, in 38th IEEE/ACM International Conference on AutomatedSoftware Engineering, ASE 2023, Luxembourg, September 11-15, 2023 . IEEE, 2023, pp. 409420. [Online]. Available:https://doi.org/10.1109/ASE56229.2023.00193[36] Z. Xie, Y. Chen, C. Zhi, S. Deng, and J. Yin, Chatunitest: achatgpt-based automated unit test generation tool,arXiv preprintarXiv:2305.04764, 2023.[37] C. Lemieux, J. P . Inala, S. K. Lahiri, and S. Sen, Codamosa:Escaping coverage plateaus in test generation with pre-trainedlarge language models, in International conference on softwareengineering (ICSE), 2023.[38] A. M. Dakhel, A. Nikanjam, V . Majdinasab, F. Khomh,and M. C. Desmarais, Effective test generation usingpre-trained large language models and mutation testing,CoRR, vol. abs/2308.16557, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2308.16557[39] M. L. Siddiq, J. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and V . C.Lopes, Exploring the effectiveness of large language models ingenerating unit tests, arXiv preprint arXiv:2305.00418, 2023.[40] Y. Zhang, W. Song, Z. Ji, D. Yao, and N. Meng, How well doesLLM generate security tests? CoRR, vol. abs/2310.00710, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2310.00710[41] V . Li and N. Doiron, Prompting code interpreter to write betterunit tests on quixbugs functions, CoRR, vol. abs/2310.00483,2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.00483[42] B. Steenhoek, M. Tufano, N. Sundaresan, and A. Svyatkovskiy,Reinforcement learning from automatic feedback for high-quality unit test generation, 2023.[43] S. Bhatia, T. Gandhi, D. Kumar, and P . Jalote, Unit test generationusing generative ai : A comparative performance analysis ofautogeneration tools, 2023.[44] M. Tufano, D. Drain, A. Svyatkovskiy, and N. Sundaresan,Generating accurate assert statements for unit test cases usingpretrained transformers, in Proceedings of the 3rd ACM/IEEEInternational Conference on Automation of Software Test , 2022, pp.5464.[45] P . Nie, R. Banerjee, J. J. Li, R. J. Mooney, and M. Gligoric,Learning deep semantics for test completion, arXiv preprintarXiv:2302.10166, 2023.[46] A. Mastropaolo, N. Cooper, D. Nader-Palacio, S. Scalabrino,D. Poshyvanyk, R. Oliveto, and G. Bavota, Using transferlearning for code-related tasks, IEEE Trans. Software Eng. ,vol. 49, no. 4, pp. 15801598, 2023. [Online]. Available:https://doi.org/10.1109/TSE.2022.3183297",
            "23[47] N. Nashid, M. Sintaha, and A. Mesbah, Retrieval-based promptselection for code-related few-shot learning, in Proceedings ofthe 45th International Conference on Software Engineering (ICSE23) ,2023.[48] G. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian,H. Wang, and Z. Wang, Automated conformance testing forjavascript engines via deep compiler fuzzing, in Proceedings ofthe 42nd ACM SIGPLAN international conference on programminglanguage design and implementation, 2021, pp. 435450.[49] Z. Liu, C. Chen, J. Wang, X. Che, Y. Huang, J. Hu, and Q. Wang,Fill in the blank: Context-aware automated text input generationfor mobile gui testing, arXiv preprint arXiv:2212.04732, 2022.[50] M. R. Taesiri, F. Macklon, Y. Wang, H. Shen, and C.-P . Bezemer,Large language models are pretty good zero-shot video gamebug detectors, arXiv preprint arXiv:2210.02506, 2022.[51] S. L. Shrestha and C. Csallner, Slgpt: using transfer learningto directly generate simulink model files and find bugs in thesimulink toolchain, in Evaluation and Assessment in SoftwareEngineering, 2021, pp. 260265.[52] J. Hu, Q. Zhang, and H. Yin, Augmenting greybox fuzzingwith generative AI, CoRR, vol. abs/2306.06782, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2306.06782[53] A. Mathur, S. Pradhan, P . Soni, D. Patel, and R. Regunathan,Automated test case generation using t5 and gpt-3, in 2023 9thInternational Conference on Advanced Computing and CommunicationSystems (ICACCS), vol. 1, 2023, pp. 19861992.[54] D. Zimmermann and A. Koziolek, Automating gui-based soft-ware testing with gpt-3, in 2023 IEEE International Conferenceon Software Testing, Verification and Validation Workshops (ICSTW),2023, pp. 6265.[55] M. Taeb, A. Swearngin, E. Schoop, R. Cheng, Y. Jiang, andJ. Nichols, Axnav: Replaying accessibility tests from naturallanguage, CoRR, vol. abs/2310.02424, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.02424[56] Q. Luu, H. Liu, and T. Y. Chen, Can chatgpt advance softwaretesting intelligence? an experience report on metamorphictesting, CoRR, vol. abs/2310.19204, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.19204[57] A. Khanfir, R. Degiovanni, M. Papadakis, and Y. L. Traon, Ef-ficient mutation testing via pre-trained language models, arXivpreprint arXiv:2301.03543, 2023.[58] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,Large language models are edge-case fuzzers: Testing deeplearning libraries via fuzzgpt, arXiv preprint arXiv:2304.02014 ,2023.[59] , Large language models are zero shot fuzzers: Fuzzingdeep learning libraries via large language models, arXiv preprintarXiv:2209.11515, 2023.[60] J. Ackerman and G. Cybenko, Large language models forfuzzing parsers (registered report), in Proceedings of the2nd International Fuzzing Workshop, FUZZING 2023, Seattle,WA, USA, 17 July 2023 , M. B ohme, Y. Noller, B. Ray, andL. Szekeres, Eds. ACM, 2023, pp. 3138. [Online]. Available:https://doi.org/10.1145/3605157.3605173[61] S. Yu, C. Fang, Y. Ling, C. Wu, and Z. Chen, LLM fortest script generation and migration: Challenges, capabilities,and opportunities, CoRR, vol. abs/2309.13574, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2309.13574[62] G. Deng, Y. Liu, V . M. Vilches, P . Liu, Y. Li, Y. Xu,T. Zhang, Y. Liu, M. Pinzger, and S. Rass, Pentestgpt:An llm-empowered automatic penetration testing tool,CoRR, vol. abs/2308.06782, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2308.06782[63] M. Sun, Y. Yang, Y. Wang, M. Wen, H. Jia, and Y. Zhou,SMT solver validation empowered by large pre-trainedlanguage models, in 38th IEEE/ACM International Conference onAutomated Software Engineering, ASE 2023, Luxembourg, September11-15, 2023 . IEEE, 2023, pp. 12881300. [Online]. Available:https://doi.org/10.1109/ASE56229.2023.00180[64] Y. Deng, J. Yao, Z. Tu, X. Zheng, M. Zhang, and T. Zhang,Target: Automated scenario generation from traffic rulesfor testing autonomous vehicles, 2023. [Online]. Available:https://api.semanticscholar.org/CorpusID:258588387[65] Z. Liu, C. Chen, J. Wang, M. Chen, B. Wu, X. Che,D. Wang, and Q. Wang, Testing the limits: Unusual text inputsgeneration for mobile app crash detection with large languagemodel, CoRR, vol. abs/2310.15657, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.15657[66] C. Zhang, M. Bai, Y. Zheng, Y. Li, X. Xie, Y. Li, W. Ma, L. Sun,and Y. Liu, Understanding large language model based fuzzdriver generation, CoRR, vol. abs/2307.12469, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2307.12469[67] C. Xia, M. Paltenghi, J. Tian, M. Pradel, and L. Zhang,Universal fuzzing via large language models, ArXiv,vol. abs/2308.04748, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:260735598[68] C. Tsigkanos, P . Rani, S. M uller, and T. Kehrer, Variablediscovery with large language models for metamorphic testingof scientific software, in Computational Science - ICCS 2023 -23rd International Conference, Prague, Czech Republic, July 3-5,2023, Proceedings, Part I , ser. Lecture Notes in ComputerScience, J. Mikyska, C. de Mulatier, M. Paszynski, V . V .Krzhizhanovskaya, J. J. Dongarra, and P . M. A. Sloot, Eds.,vol. 14073. Springer, 2023, pp. 321335. [Online]. Available:https://doi.org/10.1007/978-3-031-35995-8 23[69] C. Yang, Y. Deng, R. Lu, J. Yao, J. Liu, R. Jabbarvand, andL. Zhang, White-box compiler fuzzing empowered by largelanguage models, CoRR, vol. abs/2310.15991, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2310.15991[70] T. Zhang, I. C. Irsan, F. Thung, D. Han, D. Lo, and L. Jiang,itiger: an automatic issue title generation tool, in Proceedingsof the 30th ACM Joint European Software Engineering Conference andSymposium on the Foundations of Software Engineering , 2022, pp.16371641.[71] Y. Huang, J. Wang, Z. Liu, Y. Wang, S. Wang, C. Chen,Y. Hu, and Q. Wang, Crashtranslator: Automaticallyreproducing mobile application crashes directly from stacktrace, CoRR, vol. abs/2310.07128, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.07128[72] T. Zhang, I. C. Irsan, F. Thung, and D. Lo, Cupid:Leveraging chatgpt for more accurate duplicate bug reportdetection, CoRR, vol. abs/2308.10022, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2308.10022[73] U. Mukherjee and M. M. Rahman, Employing deeplearning and structured information retrieval to answerclarification questions on bug reports, 2023. [Online]. Available:https://api.semanticscholar.org/CorpusID:259501524[74] P . Mahbub, O. Shuvo, and M. M. Rahman, Explaining softwarebugs leveraging code structures in neural machine translation,arXiv preprint arXiv:2212.04584, 2022.[75] S. Feng and C. Chen, Prompting is all your need:Automated android bug replay with large language models,CoRR, vol. abs/2306.01987, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2306.01987[76] Y. Su, Z. Han, Z. Gao, Z. Xing, Q. Lu, and X. Xu, Stillconfusing for bug-component triaging? deep feature learningand ensemble setting to rescue, in 31st IEEE/ACM InternationalConference on Program Comprehension, ICPC 2023, Melbourne,Australia, May 15-16, 2023 . IEEE, 2023, pp. 316327. [Online].Available: https://doi.org/10.1109/ICPC58990.2023.00046[77] N. D. Bui, Y. Wang, and S. Hoi, Detect-localize-repair: A unifiedframework for learning to debug with codet5, arXiv preprintarXiv:2211.14875, 2022.[78] S. Kang, J. Yoon, and S. Yoo, Large language models are few-shottesters: Exploring llm-based general bug reproduction, arXivpreprint arXiv:2209.11515, 2022.[79] S. Kang, G. An, and S. Yoo, A preliminary evaluation ofllm-based fault localization, CoRR, vol. abs/2308.05487, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2308.05487[80] P . Widjojo and C. Treude, Addressing compiler errors: Stackoverflow or large language models? CoRR, vol. abs/2307.10793,2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.10793[81] L. Plein and T. F. Bissyand e, Can llms demystify bugreports? CoRR, vol. abs/2310.06310, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2310.06310[82] A. Taylor, A. Vassar, J. Renzella, and H. A. Pearce, Dcchelp: Generating context-aware compiler error explanationswith large language models, 2023. [Online]. Available:https://api.semanticscholar.org/CorpusID:261076439[83] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, Explainable automateddebugging via large language model-driven scientific debug-ging, arXiv preprint arXiv:2304.02195, 2023.",
            "24[84] A. Z. H. Yang, R. Martins, C. L. Goues, and V . J.Hellendoorn, Large language models for test-free faultlocalization, CoRR, vol. abs/2310.01726, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2310.01726[85] Y. Wu, Z. Li, J. M. Zhang, M. Papadakis, M. Harman,and Y. Liu, Large language models in fault localisation,CoRR, vol. abs/2308.15276, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2308.15276[86] H. Tu, Z. Zhou, H. Jiang, I. N. B. Yusuf, Y. Li, and L. Jiang,LLM4CBI: taming llms to generate effective test programsfor compiler bug isolation, CoRR, vol. abs/2307.00593, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2307.00593[87] T.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, S.-C. Cheung,and J. Kramer, Nuances are the key: Unlocking chatgpt tofind failure-inducing tests with differential prompting, in 202338th IEEE/ACM International Conference on Automated SoftwareEngineering (ASE), 2023, pp. 1426.[88] X. Chen, M. Lin, N. Sch arli, and D. Zhou, Teaching largelanguage models to self-debug,CoRR, vol. abs/2304.05128, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2304.05128[89] J. Cao, M. Li, M. Wen, and S.-c. Cheung, A study on promptdesign, advantages and limitations of chatgpt for deep learningprogram repair, arXiv preprint arXiv:2304.08191, 2023.[90] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt,Examining zero-shot vulnerability repair with large languagemodels, in 2023 IEEE Symposium on Security and Privacy (SP) .IEEE Computer Society, 2022, pp. 118.[91] Z. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, Automatedrepair of programs from large language models, arXiv preprintarXiv:2205.10583, 2022.[92] Y. Hu, X. Shi, Q. Zhou, and L. Pike, Fix bugs with trans-former through a neural-symbolic edit grammar, arXiv preprintarXiv:2204.06643, 2022.[93] C. S. Xia, Y. Wei, and L. Zhang, Practical program repair inthe era of large pre-trained language models, arXiv preprintarXiv:2210.14179, 2022.[94] J. Zhang, J. Cambronero, S. Gulwani, V . Le, R. Piskac, G. Soares,and G. Verbruggen, Repairing bugs in python assignmentsusing large language models, arXiv preprint arXiv:2209.14876 ,2022.[95] M. Lajk o, V . Csuvik, and L. Vidacs, Towards javascript programrepair with generative pre-trained transformer (gpt-2), in Pro-ceedings of the Third International Workshop on Automated ProgramRepair, 2022, pp. 6168.[96] D. Sobania, M. Briesch, C. Hanna, and J. Petke, An analysis ofthe automatic bug fixing performance of chatgpt, arXiv preprintarXiv:2301.08653, 2023.[97] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li,and Y. Zhang, An empirical study on fine-tuning largelanguage models of code for automated program repair,in 38th IEEE/ACM International Conference on AutomatedSoftware Engineering, ASE 2023, Luxembourg, September 11-15, 2023 . IEEE, 2023, pp. 11621174. [Online]. Available:https://doi.org/10.1109/ASE56229.2023.00181[98] M. C. Wuisang, M. Kurniawan, K. A. Wira Santosa, A. AgungSantoso Gunawan, and K. E. Saputra, An evaluation of theeffectiveness of openais chatgpt for automated python programbug fixing using quixbugs, in2023 International Seminar on Appli-cation for Technology of Information and Communication (iSemantic) ,2023, pp. 295300.[99] D. Horv ath, V . Csuvik, T. Gyim othy, and L. Vid acs,An extensive study on model architecture and programrepresentation in the domain of learning-based automatedprogram repair, in IEEE/ACM International Workshop onAutomated Program Repair, APR@ICSE 2023, Melbourne, Australia,May 16, 2023 . IEEE, 2023, pp. 3138. [Online]. Available:https://doi.org/10.1109/APR59189.2023.00013[100] J. A. Prenner, H. Babii, and R. Robbes, Can openais codex fixbugs? an evaluation on quixbugs, in Proceedings of the ThirdInternational Workshop on Automated Program Repair, 2022, pp. 6975.[101] W. Yuan, Q. Zhang, T. He, C. Fang, N. Q. V . Hung, X. Hao, andH. Yin, Circle: continual repair across programming languages,in Proceedings of the 31st ACM SIGSOFT International Symposiumon Software Testing and Analysis, 2022, pp. 678690.[102] S. Moon, Y. Song, H. Chae, D. Kang, T. Kwon, K. T. iunn Ong,S. won Hwang, and J. Yeo, Coffee: Boost your code llms byfixing bugs with feedback, 2023.[103] Y. Wei, C. S. Xia, and L. Zhang, Copiloting the copilots:Fusing large language models with completion engines forautomated program repair, in Proceedings of the 31st ACM JointEuropean Software Engineering Conference and Symposium on theFoundations of Software Engineering, ESEC/FSE 2023, San Francisco,CA, USA, December 3-9, 2023 , S. Chandra, K. Blincoe, andP . Tonella, Eds. ACM, 2023, pp. 172184. [Online]. Available:https://doi.org/10.1145/3611643.3616271[104] Y. Peng, S. Gao, C. Gao, Y. Huo, and M. R. Lyu, Domainknowledge matters: Improving prompts with fix templates forrepairing python type errors, CoRR, vol. abs/2306.01394, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2306.01394[105] A. E. I. Brownlee, J. Callan, K. Even-Mendoza, A. Geiger,C. Hanna, J. Petke, F. Sarro, and D. Sobania, Enhancinggenetic improvement mutations using large language models,in Search-Based Software Engineering - 15th InternationalSymposium, SSBSE 2023, San Francisco, CA, USA, December8, 2023, Proceedings , ser. Lecture Notes in ComputerScience, P . Arcaini, T. Yue, and E. M. Fredericks, Eds.,vol. 14415. Springer, 2023, pp. 153159. [Online]. Available:https://doi.org/10.1007/978-3-031-48796-5 13[106] M. M. A. Haque, W. U. Ahmad, I. Lourentzou, and C. Brown,Fixeval: Execution-based evaluation of program fixes forprogramming problems, in IEEE/ACM International Workshop onAutomated Program Repair, APR@ICSE 2023, Melbourne, Australia,May 16, 2023 . IEEE, 2023, pp. 1118. [Online]. Available:https://doi.org/10.1109/APR59189.2023.00009[107] B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce, Fixinghardware security bugs with large language models, arXivpreprint arXiv:2302.01215, 2023.[108] P . Deligiannis, A. Lal, N. Mehrotra, and A. Rastogi, Fixing rustcompilation errors using llms, CoRR, vol. abs/2308.05177, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2308.05177[109] F. Ribeiro, R. Abreu, and J. Saraiva, Framing program repairas code completion, in Proceedings of the Third InternationalWorkshop on Automated Program Repair, 2022, pp. 3845.[110] N. Wadhwa, J. Pradhan, A. Sonwane, S. P . Sahu, N. Natarajan,A. Kanade, S. Parthasarathy, and S. K. Rajamani, Frustrated withcode quality issues? llms can help! CoRR, vol. abs/2309.12938,2023. [Online]. Available: https://doi.org/10.48550/arXiv.2309.12938[111] F. Ribeiro, J. N. C. de Macedo, K. Tsushima, R. Abreu,and J. Saraiva, Gpt-3-powered type error debugging:Investigating the use of large language models for coderepair, in Proceedings of the 16th ACM SIGPLAN InternationalConference on Software Language Engineering, SLE 2023, Cascais,Portugal, October 23-24, 2023 , J. Saraiva, T. Degueule, andE. Scott, Eds. ACM, 2023, pp. 111124. [Online]. Available:https://doi.org/10.1145/3623476.3623522[112] Y. Wu, N. Jiang, H. V . Pham, T. Lutellier, J. Davis, L. Tan,P . Babkin, and S. Shah, How effective are neural networks forfixing security vulnerabilities, arXiv preprint arXiv:2305.18607 ,2023.[113] N. Jiang, K. Liu, T. Lutellier, and L. Tan, Impact of codelanguage models on automated program repair, arXiv preprintarXiv:2302.05020, 2023.[114] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan,and A. Svyatkovskiy, Inferfix: End-to-end program repair withllms, arXiv preprint arXiv:2303.07263, 2023.[115] C. S. Xia and L. Zhang, Keep the conversation going: Fixing162 out of 337 bugs for $0.42 each using chatgpt, arXiv preprintarXiv:2304.00385, 2023.[116] Y. Zhang, G. Li, Z. Jin, and Y. Xing, Neural program repair withprogram dependence analysis and effective filter mechanism,arXiv preprint arXiv:2305.09315, 2023.[117] J. A. Prenner and R. Robbes, Out of context: How important islocal context in neural program repair? 2023.[118] Q. Zhang, C. Fang, B. Yu, W. Sun, T. Zhang, and Z. Chen,Pre-trained model-based automated software vulnerabilityrepair: How far are we? CoRR, vol. abs/2308.12533, 2023.[Online]. Available: https://doi.org/10.48550/arXiv.2308.12533[119] S. Garg, R. Z. Moghaddam, and N. Sundaresan, Rapgen:An approach for fixing code inefficiencies in zero-shot,",
            "25CoRR, vol. abs/2306.17077, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2306.17077[120] W. Wang, Y. Wang, S. Joty, and S. C. H. Hoi, Rap-gen: Retrieval-augmented patch generation with codet5 forautomatic program repair, in Proceedings of the 31st ACM JointEuropean Software Engineering Conference and Symposium on theFoundations of Software Engineering, ESEC/FSE 2023, San Francisco,CA, USA, December 3-9, 2023 , S. Chandra, K. Blincoe, andP . Tonella, Eds. ACM, 2023, pp. 146158. [Online]. Available:https://doi.org/10.1145/3611643.3616256[121] Y. Zhang, Z. Jin, Y. Xing, and G. Li, STEAM: simulatingthe interactive behavior of programmers for automatic bugfixing, CoRR, vol. abs/2308.14460, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2308.14460[122] S. Fakhoury, S. Chakraborty, M. Musuvathi, and S. K. Lahiri,Towards generating functionally correct code edits from natu-ral language issue descriptions, arXiv preprint arXiv:2304.03816,2023.[123] M. Fu, C. Tantithamthavorn, T. Le, V . Nguyen, and D. Phung,Vulrepair: a t5-based automated software vulnerability repair,in Proceedings of the 30th ACM Joint European Software EngineeringConference and Symposium on the Foundations of Software Engineer-ing, 2022, pp. 935947.[124] S. Gao, X. Wen, C. Gao, W. Wang, H. Zhang, andM. R. Lyu, What makes good in-context demonstrationsfor code intelligence tasks with llms? in 38th IEEE/ACMInternational Conference on Automated Software Engineering, ASE2023, Luxembourg, September 11-15, 2023 . IEEE, 2023, pp. 761773. [Online]. Available: https://doi.org/10.1109/ASE56229.2023.00109[125] C. Treude and H. Hata, She elicits requirements and hetests: Software engineering gender bias in large languagemodels, CoRR, vol. abs/2303.10131, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2303.10131[126] R. Kocielnik, S. Prabhumoye, V . Zhang, R. M. Alvarez, andA. Anandkumar, Autobiastest: Controllable sentence generationfor automated and open-ended social bias testing in languagemodels, CoRR, vol. abs/2302.07371, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2302.07371[127] M. Ciniselli, L. Pascarella, and G. Bavota, To what extent dodeep learning-based code recommenders generate predictionsby cloning code from the training set? in 19th IEEE/ACMInternational Conference on Mining Software Repositories, MSR 2022,Pittsburgh, P A, USA, May 23-24, 2022. ACM, 2022, pp. 167178.[Online]. Available: https://doi.org/10.1145/3524842.3528440[128] D. Erhabor, S. Udayashankar, M. Nagappan, and S. Al-Kiswany,Measuring the runtime performance of code produced withgithub copilot, CoRR, vol. abs/2305.06439, 2023. [Online].Available: https://doi.org/10.48550/arXiv.2305.06439[129] R. Wang, R. Cheng, D. Ford, and T. Zimmermann, Investigatingand designing for trust in ai-powered code generationtools, CoRR, vol. abs/2305.11248, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2305.11248[130] B. Yetistiren, I. Ozsoy, M. Ayerdem, and E. T uz un, Evaluatingthe code quality of ai-assisted code generation tools: Anempirical study on github copilot, amazon codewhisperer, andchatgpt, CoRR, vol. abs/2304.10778, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2304.10778[131] C. Wohlin, Guidelines for snowballing in systematic literaturestudies and a replication in software engineering, in18th International Conference on Evaluation and Assessmentin Software Engineering, EASE 14, London, England, UnitedKingdom, May 13-14, 2014 , M. J. Shepperd, T. Hall, andI. Myrtveit, Eds. ACM, 2014, pp. 38:138:10. [Online]. Available:https://doi.org/10.1145/2601248.2601268[132] A. Mastropaolo, S. Scalabrino, N. Cooper, D. Nader-Palacio,D. Poshyvanyk, R. Oliveto, and G. Bavota, Studying the usageof text-to-text transfer transformer to support code-related tasks,in 43rd IEEE/ACM International Conference on Software Engineering,ICSE 2021, Madrid, Spain, 22-30 May 2021 . IEEE, 2021, pp. 336347.[133] C. Tsigkanos, P . Rani, S. M uller, and T. Kehrer, Largelanguage models: The next frontier for variable discoverywithin metamorphic testing? in IEEE International Conferenceon Software Analysis, Evolution and Reengineering, SANER 2023,Taipa, Macao, March 21-24, 2023 , T. Zhang, X. Xia, andN. Novielli, Eds. IEEE, 2023, pp. 678682. [Online]. Available:https://doi.org/10.1109/SANER56733.2023.00070[134] G. J. Myers, The art of software testing (2. ed.) . Wiley,2004. [Online]. Available: http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471469122.html[135] P . Farrell-Vinay,Manage software testing. Auerbach Publ., 2008.[136] A. Mili and F. Tchier, Software testing: Concepts and operations .John Wiley & Sons, 2015.[137] S. Lukasczyk and G. Fraser, Pynguin: Automated unittest generation for python, in 44th IEEE/ACM InternationalConference on Software Engineering: Companion Proceedings,ICSE Companion 2022, Pittsburgh, P A, USA, May 22-24,2022. ACM/IEEE, 2022, pp. 168172. [Online]. Available:https://doi.org/10.1145/3510454.3516829[138] E. T. Barr, M. Harman, P . McMinn, M. Shahbaz, and S. Yoo, Theoracle problem in software testing: A survey, IEEE transactionson software engineering, vol. 41, no. 5, pp. 507525, 2014.[139] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk,On learning meaningful assert statements for unit test cases,in ICSE 20: 42nd International Conference on Software Engineering,Seoul, South Korea, 27 June - 19 July, 2020, G. Rothermel and D. Bae,Eds. ACM, 2020, pp. 13981409.[140] Y. He, L. Zhang, Z. Yang, Y. Cao, K. Lian, S. Li, W. Yang, Z. Zhang,M. Yang, Y. Zhang, and H. Duan, Textexerciser: Feedback-driventext input exercising for android applications, in 2020 IEEESymposium on Security and Privacy, SP 2020, San Francisco, CA,USA, May 18-21, 2020. IEEE, 2020, pp. 10711087.[141] A. Wei, Y. Deng, C. Yang, and L. Zhang, Free lunch for test-ing: Fuzzing deep-learning libraries from open source, in 44thIEEE/ACM 44th International Conference on Software Engineering,ICSE 2022, Pittsburgh, P A, USA, May 25-27, 2022 . ACM, 2022,pp. 9951007.[142] D. Xie, Y. Li, M. Kim, H. V . Pham, L. Tan, X. Zhang, and M. W.Godfrey, Docter: documentation-guided fuzzing for testingdeep learning API functions, in ISSTA 22: 31st ACM SIGSOFTInternational Symposium on Software Testing and Analysis, VirtualEvent, South Korea, July 18 - 22, 2022 , S. Ryu and Y. Smaragdakis,Eds. ACM, 2022, pp. 176188.[143] Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, X. Li, and C. Shen,Audee: Automated testing for deep learning frameworks, in35th IEEE/ACM International Conference on Automated SoftwareEngineering, ASE 2020, Melbourne, Australia, September 21-25, 2020.IEEE, 2020, pp. 486498.[144] Z. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang, Deep learninglibrary testing via effective model generation, in ESEC/FSE20: 28th ACM Joint European Software Engineering Conferenceand Symposium on the Foundations of Software Engineering, VirtualEvent, USA, November 8-13, 2020 , P . Devanbu, M. B. Cohen, andT. Zimmermann, Eds. ACM, 2020, pp. 788799.[145] J. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, Shapingprogram repair space with existing patches and similar code, inProceedings of the 27th ACM SIGSOFT International Symposium onSoftware Testing and Analysis , ser. ISSTA 2018. New York, NY,USA: Association for Computing Machinery, 2018, p. 298309.[Online]. Available: https://doi.org/10.1145/3213846.3213871[146] M. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, Context-aware patch generation for better automated program repair,in Proceedings of the 40th International Conference on SoftwareEngineering, ser. ICSE 18. New York, NY, USA: Associationfor Computing Machinery, 2018, p. 111. [Online]. Available:https://doi.org/10.1145/3180155.3180233[147] Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, andL. Zhang, Precise condition synthesis for program repair, in2017 IEEE/ACM 39th International Conference on Software Engineer-ing (ICSE), 2017, pp. 416426.[148] J. Xuan, M. Martinez, F. DeMarco, M. Cl ement, S. L. Marcote,T. Durieux, D. Le Berre, and M. Monperrus, Nopol: Automaticrepair of conditional statement bugs in java programs, IEEETransactions on Software Engineering, vol. 43, no. 1, pp. 3455, 2017.[149] S. Song, X. Li, and S. Li, How to bridge the gap between modal-ities: A comprehensive survey on multimodal large languagemodel, CoRR, vol. abs/2311.07594, 2023.[150] J. M. Zhang, M. Harman, L. Ma, and Y. Liu, Machine learningtesting: Survey, landscapes and horizons, IEEE Trans. SoftwareEng., vol. 48, no. 2, pp. 136, 2022.[151] F. Tu, J. Zhu, Q. Zheng, and M. Zhou, Be careful of when:an empirical study on time-related misuse of issue tracking",
            "26data, in Proceedings of the 2018 ACM Joint Meeting on EuropeanSoftware Engineering Conference and Symposium on the Foundationsof Software Engineering, ESEC/SIGSOFT FSE 2018, Lake BuenaVista, FL, USA, November 04-09, 2018 , G. T. Leavens, A. Garcia,and C. S. Pasareanu, Eds. ACM, 2018, pp. 307318. [Online].Available: https://doi.org/10.1145/3236024.3236054[152] Z. Sun, L. Li, Y. Liu, X. Du, and L. Li, On the importanceof building high-quality training datasets for neural codesearch, in 44th IEEE/ACM 44th International Conference onSoftware Engineering, ICSE 2022, Pittsburgh, P A, USA, May25-27, 2022 . ACM, 2022, pp. 16091620. [Online]. Available:https://doi.org/10.1145/3510003.3510160[153] L. Shi, Z. Jiang, Y. Yang, X. Chen, Y. Zhang, F. Mu, H. Jiang, andQ. Wang, ISPY: automatic issue-solution pair extraction fromcommunity live chats, in 36th IEEE/ACM International Conferenceon Automated Software Engineering, ASE 2021, Melbourne, Australia,November 15-19, 2021 . IEEE, 2021, pp. 142154. [Online].Available: https://doi.org/10.1109/ASE51524.2021.9678894[154] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu,L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano,S. K. Deng, C. B. Clement, D. Drain, N. Sundaresan, J. Yin,D. Jiang, and M. Zhou, Graphcodebert: Pre-training coderepresentations with data flow, in 9th International Conferenceon Learning Representations, ICLR 2021, Virtual Event, Austria,May 3-7, 2021 . OpenReview.net, 2021. [Online]. Available:https://openreview.net/forum?id=jLoC4ez43PZ[155] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, andJ. Xiao, Lsun: Construction of a large-scale image dataset us-ing deep learning with humans in the loop, arXiv preprintarXiv:1506.03365, 2015.[156] LoadRunner, Inc., Loadrunner, 2023, microfocus.com.[157] LangChain, Inc., Langchain, 2023, https://docs.langchain.com/docs/.[158] Prompt engineering, Prompt engineering guide, 2023, https://github.com/dair-ai/Prompt-Engineering-Guide.[159] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola,Multimodal chain-of-thought reasoning in language models,CoRR, vol. abs/2302.00923, 2023.[160] Z. Liu, X. Yu, Y. Fang, and X. Zhang, Graphprompt: Unifyingpre-training and downstream tasks for graph neural networks,in Proceedings of the ACM Web Conference 2023, WWW 2023, Austin,TX, USA, 30 April 2023 - 4 May 2023, Y. Ding, J. Tang, J. F. Sequeda,L. Aroyo, C. Castillo, and G. Houben, Eds. ACM, 2023, pp. 417428.[161] Y. Charalambous, N. Tihanyi, R. Jain, Y. Sun, M. A. Ferrag, andL. C. Cordeiro, A new era in software security: Towards self-healing software via large language models and formal verifica-tion, 2023.[162] S. Wang, L. Huang, A. Gao, J. Ge, T. Zhang, H. Feng, I. Satyarth,M. Li, H. Zhang, and V . Ng, Machine/deep learning forsoftware engineering: A systematic literature review, IEEETrans. Software Eng., vol. 49, no. 3, pp. 11881231, 2023. [Online].Available: https://doi.org/10.1109/TSE.2022.3173346[163] Y. Yang, X. Xia, D. Lo, and J. C. Grundy, A survey ondeep learning for software engineering, ACM Comput. Surv. ,vol. 54, no. 10s, pp. 206:1206:73, 2022. [Online]. Available:https://doi.org/10.1145/3505243[164] C. Watson, N. Cooper, D. Nader-Palacio, K. Moran, andD. Poshyvanyk, A systematic literature review on the use ofdeep learning in software engineering research, ACM Trans.Softw. Eng. Methodol., vol. 31, no. 2, pp. 32:132:58, 2022. [Online].Available: https://doi.org/10.1145/3485275[165] M. Bajammal, A. Stocco, D. Mazinanian, and A. Mesbah,A survey on the use of computer vision to improvesoftware engineering tasks, IEEE Trans. Software Eng. ,vol. 48, no. 5, pp. 17221742, 2022. [Online]. Available:https://doi.org/10.1109/TSE.2020.3032986[166] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo,D. Lo, J. C. Grundy, and H. Wang, Large languagemodels for software engineering: A systematic literaturereview, CoRR, vol. abs/2308.10620, 2023. [Online]. Available:https://doi.org/10.48550/arXiv.2308.10620[167] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta,S. Yoo, and J. M. Zhang, Large language modelsfor software engineering: Survey and open problems,CoRR, vol. abs/2310.03533, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.03533[168] D. Zan, B. Chen, F. Zhang, D. Lu, B. Wu, B. Guan,Y. Wang, and J. Lou, Large language models meet nl2code:A survey, in Proceedings of the 61st Annual Meeting ofthe Association for Computational Linguistics (Volume 1: LongPapers), ACL 2023, Toronto, Canada, July 9-14, 2023 , A. Rogers,J. L. Boyd-Graber, and N. Okazaki, Eds. Association forComputational Linguistics, 2023, pp. 74437464. [Online].Available: https://doi.org/10.18653/v1/2023.acl-long.411",
            "27TABLE 5: All details of the collected papersID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-gratedVenue Ref1 Unit Test Case Generation with Transform-ers and Focal Context2021 Unit test case gener-ationBART Pre-training and/orFine-tuningCode Pure LLM Arxiv[26]2 Codet: Code Generation with GeneratedTests2022 Unit test case gener-ationCodex Zero-shot learning Code Pure LLM ICLR 2023[27]3 Interactive Code Generation via Test-DrivenUser-Intent Formalization2022 Unit test case gener-ationCodex Zero-shot learning Code Mutation testing;Statistic analysisArxiv[28]4 A3Test: Assertion-Augmented AutomatedTest Case Generation2023 Unit test case gener-ationPLBART Pre-training and/orFine-tuningCode Syntactic repair Arxiv[29]5 An Empirical Evaluation of Using LargeLanguage Models for Automated Unit TestGeneration2023 Unit test case gener-ationChatGPT Zero-shot learning Code; Others Syntactic repair Arxiv[30]6 An Initial Investigation of ChatGPT UnitTest Generation Capability2023 Unit test case gener-ationChatGPT Zero-shot learning Code Pure LLM SAST 2023[31]7 Automated Test Case Generation UsingCode Models and Domain Adaptation2023 Unit test case gener-ationCodeT5; LLaMA-2 Pre-training and/orFine-tuningCode Syntactic repair Arxiv[32]8 Automatic Generation of Test Cases basedon Bug Reports: a Feasibility Study withLarge Language Models2023 Unit test case gener-ationCodeGPT; ChatGPT Pre-training and/orFine-tuningBug description Pure LLM Arxiv[33]9 Can Large Language Models Write GoodProperty-Based Tests?2023 Unit test case gener-ationGPT-4 Zero-shot learning Code; Others Pure LLM Arxiv[34]10 CAT-LM Training Language Models onAligned Code And Tests2023 Unit test case gener-ationGPT-neox Pre-training and/orFine-tuningCode Pure LLM ASE 2023[35]11 ChatGPT vs SBST: A Comparative Assess-ment of Unit Test Suite Generation2023 Unit test case gener-ationChatGPT Zero-shot learning Code Pure LLM Arxiv [8]12 ChatUniTest: a ChatGPT-based AutomatedUnit Test Generation Tool2023 Unit test case gener-ationChatGPT Zero-shot learning Code Syntactic repair Arxiv[36]13 CODAMOSA: Escaping Coverage Plateausin Test Generation with Pre-trained LargeLanguage Models2023 Unit test case gener-ationCodex Zero-shot learning Code Mutation testing;Program analysisICSE 2023[37]14 Effective Test Generation Using Pre-trainedLarge Language Models and Mutation Test-ing2023 Unit test case gener-ationCodex Few-shot learning;Zero-shot learningCode Mutation testing;Syntactic repairArxiv[38]15 Exploring the Effectiveness of Large Lan-guage Models in Generating Unit Tests2023 Unit test case gener-ationCodeGen; Codex;ChatGPTZero-shot learning Code Syntactic repair Arxiv[39]16 How Well does LLM Generate SecurityTests?2023 Unit test case gener-ationChatGPT Few-shot learning Code Pure LLM Arxiv[40]17 No More Manual Tests? Evaluating and Im-proving ChatGPT for Unit Test Generation2023 Unit test case gener-ationChatGPT Zero-shot learning Code; Error in-formationProgram analysis Arxiv [7]18 Prompting Code Interpreter to Write BetterUnit Tests on Quixbugs Functions2023 Unit test case gener-ationGPT-4 Few-shot learning Code Pure LLM Arxiv[41]19 Reinforcement Learning from AutomaticFeedback for High-Quality Unit Test Gen-eration2023 Unit test case gener-ationCodex Pre-training and/orFine-tuningCode Program analysis,ReinforcementlearningArxiv[42]20 Unit Test Generation using Generative AI: AComparative Performance Analysis of Au-togeneration Tools2023 Unit test case gener-ationChatGPT Zero-shot learning Code Pure LLM Arxiv[43]21 Generating Accurate Assert Statements forUnit Test Cases Using Pretrained Trans-formers2023 Test oracle genera-tionBART Pre-training and/orFine-tuningCode Pure LLM AST 2022[44]22 Learning Deep Semantics for Test Comple-tion2023 Test oracle genera-tionCodeT5 Pre-training and/orFine-tuningCode Statistic analysis ICSE 2023[45]23 Using Transfer Learning for Code-RelatedTasks2022 Test oracle gener-ation; Program re-pairT5 Pre-training and/orFine-tuningCode Pure LLM TSE 2022[46]24 Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning2023 Test oracle gener-ation; Program re-pairCodex Few-shot learning Code Pure LLM ICSE 2023[47]",
            "28ID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-gratedVenue Ref25 Automated Conformance Testing forJavaScript Engines via Deep CompilerFuzzing2021 System test inputgenerationGPT-2 Pre-training and/orFine-tuningCode Differential testing;Program analysisPLDI 2021[48]26 Fill in the Blank: Context-aware AutomatedText Input Generation for Mobile GUI Test-ing2022 System test inputgenerationGPT-3 Pre-training and/orFine-tuningView hierarchyfile of UIPure LLM ICSE 2023[49]27 Large Language Models are Pretty GoodZero-Shot Video Game Bug Detectors2022 System test inputgenerationInstructGPT Chain-of-Thought;Zero-shot learningOthers Pure LLM Arxiv[50]28 Slgpt: Using Transfer Learning to DirectlyGenerate Simulink Model Files and FindBugs in the Simulink Toolchain2022 System test inputgenerationGPT-2 Pre-training and/orFine-tuningOthers Formal method EASE 2021[51]29 Augmenting Greybox Fuzzing with Gener-ative AI2023 System test inputgenerationChatGPT Few-shot learning Code Pure LLM Arxiv[52]30 Automated Test Case Generation Using T5and GPT-32023 System test inputgenerationGPT-3; T5 Pre-training and/orFine-tuning; Zero-shotlearningNL specifica-tionPure LLM ICACCS2023 [53]31 Automating GUI-based Software Testingwith GPT-32023 System test inputgenerationGPT-3 Pre-training and/orFine-tuningView hierarchyfile of UIPure LLM ICSTW2023 [54]32 AXNav: Replaying Accessibility Tests fromNatural Language2023 System test inputgenerationGPT-4 Chain-of-Thought View hierarchyfile of UIPure LLM Arxiv[55]33 Can ChatGPT Advance Software Testing In-telligence? An Experience Report on Meta-morphic Testing2023 System test inputgenerationChatGPT Zero-shot learning Others Pure LLM Arxiv[56]34 Efficient Mutation Testing via Pre-TrainedLanguage Models2023 System test inputgenerationCodeBert Zero-shot learning Code Mutation testing Arxiv[57]35 Large Language Models are Edge-CaseGenerators:Crafting Unusual Programs forFuzzing Deep Learning Libraries2023 System test inputgenerationCodex Chain-of-Thought; Pre-training and/or Fine-tuning; Zero-shot learn-ing; Few-shot learningCode Differential testing ICSE 2024[58]36 Large Language Models are Zero ShotFuzzers: Fuzzing Deep Learning Librariesvia Large Language Models2023 System test inputgenerationCodex; InCoder Zero-shot learning Code Mutation testing;Differential testingISSTA 2023[59]37 Large Language Models for Fuzzing Parsers(Registered Report)2023 System test inputgenerationGPT-4 Few-shot learning NL specifica-tionPure LLM FUZZING2023 [60]38 LLM for Test Script Generation and Migra-tion: Challenges, Capabilities, and Opportu-nities2023 System test inputgenerationChatGPT Zero-shot learning View hierarchyfile of UIPure LLM Arxiv[61]39 Make LLM a Testing Expert: BringingHuman-like Interaction to Mobile GUI Test-ing via Functionality-aware Decisions2023 System test inputgenerationGPT-3 Zero-shot learning View hierarchyfile of UINatural languageprocessingICSE 2024[14]40 PentestGPT: An LLM-empowered Auto-matic Penetration Testing Tool2023 System test inputgenerationChatGPT; GPT-4;LaMDAChain-of-Thought;Few-shot learningNL specifica-tionPure LLM Arxiv[62]41 SMT Solver Validation Empowered byLarge Pre-Trained Language Models2023 System test inputgenerationGPT-2 Pre-training and/orFine-tuningCode Differential testing ASE 2023[63]42 TARGET: Automated Scenario Generationfrom Traffic Rules for Testing AutonomousVehicles2023 System test inputgenerationGPT-3 Zero-shot learning Others Scenario testing Arxiv[64]43 Testing the Limits: Unusual Text InputsGeneration for Mobile App Crash Detectionwith Large Language Model2023 System test inputgenerationChatGPT Few-shot learning View hierarchyfile of UIPure LLM ICSE 2024[65]44 Understanding Large Language ModelBased Fuzz Driver Generation2023 System test inputgenerationChatGPT; GPT-4 Few-shot learning;Zero-shot learningCode; Others Pure LLM Arxiv[66]45 Universal Fuzzing via Large LanguageModels2023 System test inputgenerationGPT-4; StarCoder Few-shot learning; Au-tomatic promptCode Mutation testing ICSE 2024[67]46 Variable Discovery with Large LanguageModels for Metamorphic Testing of Scien-tific Software2023 System test inputgenerationGPT-j Zero-shot learning Others Pure LLM SANER2023 [68]",
            "29ID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-gratedVenue Ref47 White-box Compiler Fuzzing Empoweredby Large Language Models2023 System test inputgenerationGPT-4; StarCoder Few-shot learning Code Pure LLM Arxiv[69]48 Itiger: an Automatic Issue Title GenerationTool2022 Bug analysis BART Pre-training and/orFine-tuningBug description Pure LLM FSE 2022[70]49 CrashTranslator: Automatically Reproduc-ing Mobile Application Crashes Directlyfrom Stack Trace2023 Bug analysis ChatGPT Pre-training and/orFine-tuningBug description ReinforcementlearningICSE 2024[71]50 Cupid: Leveraging ChatGPT for More Ac-curate Duplicate Bug Report Detection2023 Bug analysis ChatGPT Zero-shot learning Bug description Statistic analysis Arxiv[72]51 Employing Deep Learning and StructuredInformation Retrieval to Answer Clarifica-tion Questions on Bug Reports2023 Bug analysis CodeT5 Zero-shot learning Bug description Statistic analysis Arxiv[73]52 Explaining Software Bugs Leveraging CodeStructures in Neural Machine Translation2023 Bug analysis CodeT5 Pre-training and/orFine-tuningCode Program analysis ICSE 2023[74]53 Prompting Is All Your Need: AutomatedAndroid Bug Replay with Large LanguageModels2023 Bug analysis ChatGPT Few-shot learning;Chain-of-ThoughtBug description Pure LLM ICSE 2024[75]54 Still Confusing for Bug-Component Triag-ing? Deep Feature Learning and EnsembleSetting to Rescue2023 Bug analysis CodeT5 Pre-training and/orFine-tuningBug description Statistic analysis ICPC 2023[76]55 Detect-Localize-Repair: A Unified Frame-work for Learning to Debug with CodeT52022 Debug CodeT5 Pre-training and/orFine-tuningCode Pure LLM EMNLP2022 [77]56 Large Language Models are Few-shotTesters: Exploring LLM-based General BugReproduction2022 Debug Codex Few-shot learning Bug description Program analysis;Statistic analysisICSE 2023[78]57 A Preliminary Evaluation of LLM-BasedFault Localization2023 Debug ChatGPT Few-shot learning Code Pure LLM Arxiv[79]58 Addressing Compiler Errors: Stack Over-flow or Large Language Models?2023 Debug ChatGPT; GPT-4 Zero-shot learning Error informa-tionPure LLM Arxiv[80]59 Can LLMs Demystify Bug Reports? 2023 Debug ChatGPT Zero-shot learning Bug description Pure LLM Arxiv[81]60 Dcc help: Generating Context-Aware Com-piler Error Explanations with Large Lan-guage Models2023 Debug ChatGPT Zero-shot learning Code; Error in-formationPure LLM SIGCSE2024 [82]61 Explainable Automated Debugging viaLarge Language Model-driven Scientific De-bugging2023 Debug CodeGen; Codex;ChatGPTSelf-consistency; Zero-shot learningCode Pure LLM Arxiv[83]62 Large Language Models for Test-Free FaultLocalization2023 Debug CodeGen Pre-training and/orFine-tuningCode Pure LLM ICSE 2024[84]63 Large Language Models in Fault Localisa-tion2023 Debug ChatGPT; GPT-4 Zero-shot learning Code; Error in-formationPure LLM Arxiv[85]64 LLM4CBI: Taming LLMs to Generate Effec-tive Test Programs for Compiler Bug Isola-tion2023 Debug ChatGPT Zero-shot learning Code Mutation testing;ReinforcementlearningArxiv[86]65 Nuances are the Key: Unlocking ChatGPTto Find Failure-Inducing Tests with Differ-ential Prompting2023 Debug ChatGPT Zero-shot learning Code Differential testing ASE 2023[87]66 Teaching Large Language Models to Self-Debug2023 Debug Codex; ChatGPT;GPT-4; StarCoderFew-shot learning Code Pure LLM Arxiv[88]67 A study on Prompt Design, Advantages andLimitations of ChatGPT for Deep LearningProgram Repair2023 Debug; Program re-pairChatGPT Zero-shot learning Code Pure LLM Arxiv[89]68 Examining Zero-Shot Vulnerability Repairwith Large Language Models2021 Program repair Codex Zero-shot learning Code; Bug de-scriptionPure LLM SP 2023[90]69 Automated Repair of Programs from LargeLanguage Models2022 Program repair Codex Zero-shot learning Code Pure LLM ICSE 2023[91]70 Fix Bugs with Transformer through aNeural-Symbolic Edit Grammar2022 Program repair CodeGPT Pre-training and/orFine-tuningCode Pure LLM Arxiv[92]71 Practical Program Repair in the Era of LargePre-trained Language Models2022 Program repair GPT-3; Codex;CodeT5; InCoderFew-shot learning;Zero-shot learningCode Statistic analysis ICSE 2023[93]",
            "30ID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-gratedVenue Ref72 Repairing Bugs in Python Assignments Us-ing Large Language Models2022 Program repair Codex Few-shot learning;Zero-shot learningCode; Error in-formationProgram analysis Arxiv[94]73 Towards JavaScript Program Repair withGenerative Pre-trained Transformer (GPT-2)2022 Program repair GPT-2 Pre-training and/orFine-tuningCode Pure LLM APR 2022[95]74 An Analysis of the Automatic Bug FixingPerformance of ChatGPT2023 Program repair ChatGPT Zero-shot learning Code; Error in-formationPure LLM APR 2023[96]75 An Empirical Study on Fine-Tuning LargeLanguage Models of Code for AutomatedProgram Repair2023 Program repair PLBART; CodeT5;UniXCoderPre-training and/orFine-tuningCode Pure LLM ASE 2023[97]76 An Evaluation of the Effectiveness of Ope-nAIs ChatGPT for Automated Python Pro-gram Bug Fixing using QuixBugs2023 Program repair ChatGPT Zero-shot learning Code Pure LLM iSemantic2023 [98]77 An Extensive Study on Model Architectureand Program Representation in the Domainof Learning-based Automated Program Re-pair2023 Program repair T5; CodeT5 Pre-training and/orFine-tuningCode Pure LLM APR 2023[99]78 Can OpenAIs Codex Fix Bugs? An Evalua-tion on QuixBugs2023 Program repair Codex Few-shot learning;Zero-shot learningCode Pure LLM APR 2022[100]79 CIRCLE: Continual Repair Across Program-ming Languages2023 Program repair T5 Pre-training and/orFine-tuningCode Pure LLM ISSTA 2022[101]80 Coffee: Boost Your Code LLMs by FixingBugs with Feedback2023 Program repair CodeLLAMA Pre-training and/orFine-tuningCode Pure LLM Arxiv[102]81 Copiloting the Copilots: Fusing Large Lan-guage Models with Completion Engines forAutomated Program Repair2023 Program repair CodeT5; InCoder Zero-shot learning Code Statistic analysis FSE 2023[103]82 Domain Knowledge Matters: ImprovingPrompts with Fix Templates for RepairingPython Type Errors2023 Program repair CodeT5 Pre-training and/orFine-tuningCode Program analysis ICSE 2024[104]83 Enhancing Genetic Improvement MutationsUsing Large Language Models2023 Program repair GPT-4 Zero-shot learning Code Pure LLM SSBSE 2023[105]84 FixEval: Execution-based Evaluation of Pro-gram Fixes for Programming Problems2023 Program repair CodeT5; PLBART Pre-training and/orFine-tuningCode Pure LLM APR 2023[106]85 Fixing Hardware Security Bugs with LargeLanguage Models2023 Program repair Codex; CodeGen Few-shot learning;Zero-shot learningCode; Bug de-scriptionPure LLM Arxiv[107]86 Fixing Rust Compilation Errors using LLMs 2023 Program repair ChatGPT; GPT-4 Zero-shot learning Code Pure LLM Arxiv[108]87 Framing Program Repair as Code Comple-tion2023 Program repair CodeGPT Zero-shot learning Code Pure LLM ICSE 2022[109]88 Frustrated with Code Quality Issues? LLMscan Help!2023 Program repair ChatGPT; GPT-4 Zero-shot learning Code Pure LLM Arxiv[110]89 GPT-3-Powered Type Error Debugging: In-vestigating the Use of Large Language Mod-els for Code Repair2023 Program repair GPT-3 Zero-shot learning Code Program analysis SLE 2023[111]90 How Effective Are Neural Networks for Fix-ing Security Vulnerabilities2023 Program repair Codex; CodeGen;CodeT5; PLBART;InCoderPre-training and/orFine-tuning; Zero-shotlearningCode Pure LLM ISSTA 2023[112]91 Impact of Code Language Models on Auto-mated Program Repair2023 Program repair PLBART; CodeT5;CodeGen; InCoderPre-training and/orFine-tuning; Zero-shotlearningCode Pure LLM ICSE 2023[113]92 Inferfix: End-to-end Program Repair withLLMs2023 Program repair Codex Few-shot learning; Pre-training and/or Fine-tuningCode Pure LLM FSE 2023[114]93 Keep the Conversation Going: Fixing 162out of 337 bugs for $0.42 each using Chat-GPT2023 Program repair ChatGPT Few-shot learning Code; Error in-formationPure LLM Arxiv[115]94 Neural Program Repair with Program De-pendence Analysis and Effective FilterMechanism2023 Program repair CodeT5 Pre-training and/orFine-tuningCode Statistic analysis Arxiv[116]",
            "31ID Paper title Year Topic Involved LLM How LLM is used Input to LLM How LLM inte-gratedVenue Ref95 Out of Context: How important is LocalContext in Neural Program Repair?2023 Program repair CodeT5 Pre-training and/orFine-tuningCode Pure LLM ICSE 2024[117]96 Pre-trained Model-based Automated Soft-ware Vulnerability Repair: How Far are We?2023 Program repair CodeT5; UniX-Coder; CodeGPTPre-training and/orFine-tuningCode Pure LLM IEEE TDSC[118]97 RAPGen: An Approach for Fixing Code In-efficiencies in Zero-Shot2023 Program repair Codex Few-shot learning;Chain-of-ThoughtCode Pure LLM Arxiv[119]98 RAP-Gen: Retrieval-Augmented Patch Gen-eration with CodeT5 for Automatic Pro-gram Repair2023 Program repair CodeT5 Pre-training and/orFine-tuningCode Statistic analysis FSE 2023[120]99 STEAM: Simulating the InTeractive BEhav-ior of ProgrAMmers for Automatic Bug Fix-ing2023 Program repair ChatGPT Zero-shot learning Code Pure LLM Arxiv[121]100 Towards Generating Functionally CorrectCode Edits from Natural Language IssueDescriptions2023 Program repair Codex; ChatGPT Few-shot learning;Zero-shot learning;Chain-of-ThoughtCode; Bug de-scriptionPure LLM Arxiv[122]101 VulRepair: a T5-based Automated SoftwareVulnerability Repair2023 Program repair T5 Pre-training and/orFine-tuningCode Pure LLM FSE 2022[123]102 What Makes Good In-Context Demonstra-tions for Code Intelligence Tasks withLLMs?2023 Program repair Codex; ChatGPT Few-shot learning Code Pure LLM ASE 2023[124]"
        ]
    }
}